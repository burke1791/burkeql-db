{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BurkeQL Relational Database BurkeQL is a relational database we are going to write from scratch in C. The current source code can be found here . Why? The dumb name? Well, my name is Chris Burke and I like to incorporate my last name into a lot of personal projects - call it vanity if you must. Plus, a single syllable is easy to squeeze into made up names. This project? I love databases. I am a database engineer by day and love teaching my app dev coworkers how to make their queries go fast. And I often find an excuse to dive into database internals as part of query tuning exercises. Learning C has been on my list for a while. As I was getting started, I came across cstack's Let's Build a Simple Database tutorial and loved it. He builds a simple sqlite clone that stores data in a B+tree structure and persists it to disk. However, at the time of writing, his sqlite clone only supports a hard-coded table definition. Going through his tutorial inspired me to start this project with the ultimate goal of having a DBMS that can support any number of arbitrarily defined tables. Though after writing a few sections, I realize my ambitious project is not nearly as approachable as cstack's. If you find your eyes glazing over as you work through my tutorial, I strongly suggest you go through his (currently) 14-part series. Disclaimers I am not an expert at C. I was able to slap together some working code, which is what I present here. I did not go out of my way to make sure the code is cross-platform - I wrote it using an Ubuntu-22.04 image running in WSL. Therefore, please be aware that directly copying the code to your system may not work.. and I am not nearly qualified enough to help you get it working. What You Will Build This tutorial will walk you through my process of building a relational database from scratch. I will provide diffs of every single line of code I add/change/delete, as well as keep an organized Github repository for easy reference. For each of the major sections in this tutorial, there will be a corresponding branch in my Github repo that contains the code as it was at the completion of that section. This RDBMS will be a command-line program that accepts SQL-like input and behaves as you'd expect a database client to. The final result will be a fully functional, persistent, relational database that's capable of storing all of the basic data types in any number of tables you want. Check out the Project Plan to see what I currently have planned.","title":"Home"},{"location":"#burkeql-relational-database","text":"BurkeQL is a relational database we are going to write from scratch in C. The current source code can be found here .","title":"BurkeQL Relational Database"},{"location":"#why","text":"The dumb name? Well, my name is Chris Burke and I like to incorporate my last name into a lot of personal projects - call it vanity if you must. Plus, a single syllable is easy to squeeze into made up names. This project? I love databases. I am a database engineer by day and love teaching my app dev coworkers how to make their queries go fast. And I often find an excuse to dive into database internals as part of query tuning exercises. Learning C has been on my list for a while. As I was getting started, I came across cstack's Let's Build a Simple Database tutorial and loved it. He builds a simple sqlite clone that stores data in a B+tree structure and persists it to disk. However, at the time of writing, his sqlite clone only supports a hard-coded table definition. Going through his tutorial inspired me to start this project with the ultimate goal of having a DBMS that can support any number of arbitrarily defined tables. Though after writing a few sections, I realize my ambitious project is not nearly as approachable as cstack's. If you find your eyes glazing over as you work through my tutorial, I strongly suggest you go through his (currently) 14-part series.","title":"Why?"},{"location":"#disclaimers","text":"I am not an expert at C. I was able to slap together some working code, which is what I present here. I did not go out of my way to make sure the code is cross-platform - I wrote it using an Ubuntu-22.04 image running in WSL. Therefore, please be aware that directly copying the code to your system may not work.. and I am not nearly qualified enough to help you get it working.","title":"Disclaimers"},{"location":"#what-you-will-build","text":"This tutorial will walk you through my process of building a relational database from scratch. I will provide diffs of every single line of code I add/change/delete, as well as keep an organized Github repository for easy reference. For each of the major sections in this tutorial, there will be a corresponding branch in my Github repo that contains the code as it was at the completion of that section. This RDBMS will be a command-line program that accepts SQL-like input and behaves as you'd expect a database client to. The final result will be a fully functional, persistent, relational database that's capable of storing all of the basic data types in any number of tables you want. Check out the Project Plan to see what I currently have planned.","title":"What You Will Build"},{"location":"00-intro/project-plan/","text":"BurkeQL - Project Roadmap This page contains an outline of my plan for this project. It's really meant to serve as a scratchpad for me to organize my thoughts. Please ignore any notes that don't make sense, I probably just forgot to delete/move them. Milestones Milestone Status Basic SQL Parser Done Data Persistence In Progress System Catalogs Planned BQL DML Planned The Plan What follows is basically a table of contents for this tutorial/blog series. I list all sections I've completed, as well as future sections I have planned, and any associated notes. Basic CLI (COMPLETE) Writing an extremely basic cli with an extremely basic input parser. Github branch: basic-cli Intro Flex Bison Putting It Together Abstract Syntax Trees (COMPLETE) Introduce ASTs to the parser. Github branch: ast Intro AST Interface AST Implementation Parser Refactor Parser Interface Putting It Together The Database Page (COMPLETE) This section is entirely conceptual - there will be zero code written. Discussion about how data are organized on disk. Also touch on how different DBMS's do things slightly different from one another. Page Structure Data Persistence (COMPLETE) Start storing data on disk. Using a hard-coded table definition: Create Table person ( person_id Int Not Null, name Char(20) Not Null ); Every insert is immediately persisted to disk Talk about the buffer pool? Probably not Only a single data page allowed - throw error if page is full Page splits implemented in a later section Select statement also deferred - will be inspecting data pages using xxd Intro Config File Loading Config DB File Interface DB Page Interface DB Page Implementation Parser Refactor - Insert Serializing and Inserting Data Selecting Data (COMPLETE) Updating the parser to support selecting data from the table MAYBE introduce the analyzer Implement basic table scan RecordDescriptor Datum construct Parser Refactor - Select The SQL Analyzer Buffer Pool Table Scan Displaying Results Data Type - Ints (COMPLETE) Write functions to support all of the Int class data types: TinyInt SmallInt Int BigInt Use a new hard-coded table: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Hard-Coded Table Refactor Parser Refactor Fill and Defill Select Updates Data Type - Bool (COMPLETE) 1-byte Essentially a TinyInt, but can only be 1 or 0 so requires extra code Hard-coded table: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); Hard-Coded Table Refactor Parser Refactor Fill and Defill Select Updates Data Type - Varchar (COMPLETE) 2-byte variable length overhead talk about how column order in the create statement can differ from how the engine stores columns on disk Hard-coded table: Create Table person ( person_id Int Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null, age Int Not Null ); Intro Storage and Fill Temporary Code Refactor Inserting Data Defill and Display Data Constraint - NULLs (in progress) Introduce the Null bitmap and explain why it's important Implement the Null bitmap Hard-coded table: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); Intro Parser Refactor Writing Data Reading Data Page Splits (planned) Create a new page when an insert won't fit on an existing page Page metadata functionality - linked list in the header fields Because tables are heaps, we will loop through all existing pages until we find one that has enough space Introduce buffer tags and fully flesh out the buffer pool (except for locks) System Catalog - _tables (planned) Introduce the concept of the system catalog Global values object_id page_id ? Implement the _tables system catalog Check for the existence of the system catalogs on boot Populate the _tables table on startup of a new database System Catalog - _columns (planned) Same as above, but for columns Refactor the retrieval code to use system catalogs for grabbing RecordDescriptors Analyzer (planned) BQL - Select From (planned) BQL - Create Table (planned) BQL - Insert Into (planned) BQL - Update (planned) BQL - Delete (planned)","title":"Project Roadmap"},{"location":"00-intro/project-plan/#burkeql-project-roadmap","text":"This page contains an outline of my plan for this project. It's really meant to serve as a scratchpad for me to organize my thoughts. Please ignore any notes that don't make sense, I probably just forgot to delete/move them.","title":"BurkeQL - Project Roadmap"},{"location":"00-intro/project-plan/#milestones","text":"Milestone Status Basic SQL Parser Done Data Persistence In Progress System Catalogs Planned BQL DML Planned","title":"Milestones"},{"location":"00-intro/project-plan/#the-plan","text":"What follows is basically a table of contents for this tutorial/blog series. I list all sections I've completed, as well as future sections I have planned, and any associated notes.","title":"The Plan"},{"location":"00-intro/project-plan/#basic-cli-complete","text":"Writing an extremely basic cli with an extremely basic input parser. Github branch: basic-cli Intro Flex Bison Putting It Together","title":"Basic CLI (COMPLETE)"},{"location":"00-intro/project-plan/#abstract-syntax-trees-complete","text":"Introduce ASTs to the parser. Github branch: ast Intro AST Interface AST Implementation Parser Refactor Parser Interface Putting It Together","title":"Abstract Syntax Trees (COMPLETE)"},{"location":"00-intro/project-plan/#the-database-page-complete","text":"This section is entirely conceptual - there will be zero code written. Discussion about how data are organized on disk. Also touch on how different DBMS's do things slightly different from one another. Page Structure","title":"The Database Page (COMPLETE)"},{"location":"00-intro/project-plan/#data-persistence-complete","text":"Start storing data on disk. Using a hard-coded table definition: Create Table person ( person_id Int Not Null, name Char(20) Not Null ); Every insert is immediately persisted to disk Talk about the buffer pool? Probably not Only a single data page allowed - throw error if page is full Page splits implemented in a later section Select statement also deferred - will be inspecting data pages using xxd Intro Config File Loading Config DB File Interface DB Page Interface DB Page Implementation Parser Refactor - Insert Serializing and Inserting Data","title":"Data Persistence (COMPLETE)"},{"location":"00-intro/project-plan/#selecting-data-complete","text":"Updating the parser to support selecting data from the table MAYBE introduce the analyzer Implement basic table scan RecordDescriptor Datum construct Parser Refactor - Select The SQL Analyzer Buffer Pool Table Scan Displaying Results","title":"Selecting Data (COMPLETE)"},{"location":"00-intro/project-plan/#data-type-ints-complete","text":"Write functions to support all of the Int class data types: TinyInt SmallInt Int BigInt Use a new hard-coded table: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Hard-Coded Table Refactor Parser Refactor Fill and Defill Select Updates","title":"Data Type - Ints (COMPLETE)"},{"location":"00-intro/project-plan/#data-type-bool-complete","text":"1-byte Essentially a TinyInt, but can only be 1 or 0 so requires extra code Hard-coded table: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); Hard-Coded Table Refactor Parser Refactor Fill and Defill Select Updates","title":"Data Type - Bool (COMPLETE)"},{"location":"00-intro/project-plan/#data-type-varchar-complete","text":"2-byte variable length overhead talk about how column order in the create statement can differ from how the engine stores columns on disk Hard-coded table: Create Table person ( person_id Int Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null, age Int Not Null ); Intro Storage and Fill Temporary Code Refactor Inserting Data Defill and Display","title":"Data Type - Varchar (COMPLETE)"},{"location":"00-intro/project-plan/#data-constraint-nulls-in-progress","text":"Introduce the Null bitmap and explain why it's important Implement the Null bitmap Hard-coded table: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); Intro Parser Refactor Writing Data Reading Data","title":"Data Constraint - NULLs (in progress)"},{"location":"00-intro/project-plan/#page-splits-planned","text":"Create a new page when an insert won't fit on an existing page Page metadata functionality - linked list in the header fields Because tables are heaps, we will loop through all existing pages until we find one that has enough space Introduce buffer tags and fully flesh out the buffer pool (except for locks)","title":"Page Splits (planned)"},{"location":"00-intro/project-plan/#system-catalog-_tables-planned","text":"Introduce the concept of the system catalog Global values object_id page_id ? Implement the _tables system catalog Check for the existence of the system catalogs on boot Populate the _tables table on startup of a new database","title":"System Catalog - _tables (planned)"},{"location":"00-intro/project-plan/#system-catalog-_columns-planned","text":"Same as above, but for columns Refactor the retrieval code to use system catalogs for grabbing RecordDescriptors","title":"System Catalog - _columns (planned)"},{"location":"00-intro/project-plan/#analyzer-planned","text":"","title":"Analyzer (planned)"},{"location":"00-intro/project-plan/#bql-select-from-planned","text":"","title":"BQL - Select From (planned)"},{"location":"00-intro/project-plan/#bql-create-table-planned","text":"","title":"BQL - Create Table (planned)"},{"location":"00-intro/project-plan/#bql-insert-into-planned","text":"","title":"BQL - Insert Into (planned)"},{"location":"00-intro/project-plan/#bql-update-planned","text":"","title":"BQL - Update (planned)"},{"location":"00-intro/project-plan/#bql-delete-planned","text":"","title":"BQL - Delete (planned)"},{"location":"01-basic-cli/bison/","text":"Bison Grammar Similar to flex, bison files are also divided into three sections with a %% symbol. /* bison config options */ %{ /* code block for #includes or external interfaces */ }% /* grammar types, tokens, etc. */ %% /* first divider */ /* grammar definitions */ %% /* second divider */ /* c code */ Also similar to flex, the third section is optional. However, this time we will actually put something there. Another note, comments in a bison file DO NOT need to be indented like they do in flex. Config and Types %{ #include <stdio.h> #include <stdarg.h> %} %token INSERT %token QUIT %token SELECT %start cmd At the top we're including some standard libraries we make use of, as well as an interface to the lexer's line number tracker. In the next section, we define our tokens with the %token symbol. When bison builds this file, it takes all %tokens and stuffs them in an enum called yytokentype , which lives in bison's header file - the same file we #include d in our scanner. You can define multiple tokens on a single line by separating them with a space, e.g. %token INSERT QUIT SELECT . However, our grammar will eventually have a lot of tokens and I've found separating them alphabetically keeps things much more organized. The %start symbol is not entirely necessary for our simple grammar, but I like to include it to be explicit about the root of our grammar definition. Grammar We define a grammar by defining a number of rules. These rules can be arbitrarily complex, but in our case we just have three simple tokens to handle. cmd: stmt { return 0; } ; stmt: select_stmt | insert_stmt | quit_stmt ; /* test comment */ select_stmt: SELECT { printf(\"SELECT command received\\n\"); } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); } ; quit_stmt: QUIT { printf(\"QUIT command received\\n\"); } ; Starting at the top, we have a rule named cmd . A cmd consists of a stmt rule, and when bison is able to satisfy the cmd rule, it returns 0 indicating success. Drilling deeper, a stmt rule can be one of three things: select_stmt , insert_stmt , or quit_stmt . Each of these has their own definition and associated code to run when encountered. Code Section Unlike the scanner, we are actually going to write some code for the third section. We are simply implementing our own error function that prints out a provided message. void yyerror(char* s, ...) { va_list ap; va_start(ap, s); fprintf(stderr, \"error: \"); vfprintf(stderr, s, ap); fprintf(stderr, \"\\n\"); } Full File src/parser/gram.y %{ #include <stdio.h> #include <stdarg.h> %} %token INSERT %token QUIT %token SELECT %start cmd %% cmd: stmt { return 0; } ; stmt: select_stmt | insert_stmt | quit_stmt ; select_stmt: SELECT { printf(\"SELECT command received\\n\"); } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); } ; quit_stmt: QUIT { printf(\"QUIT command received\\n\"); } ; %% void yyerror(char* s, ...) { va_list ap; va_start(ap, s); fprintf(stderr, \"error: \"); vfprintf(stderr, s, ap); fprintf(stderr, \"\\n\"); }","title":"Bison"},{"location":"01-basic-cli/bison/#bison-grammar","text":"Similar to flex, bison files are also divided into three sections with a %% symbol. /* bison config options */ %{ /* code block for #includes or external interfaces */ }% /* grammar types, tokens, etc. */ %% /* first divider */ /* grammar definitions */ %% /* second divider */ /* c code */ Also similar to flex, the third section is optional. However, this time we will actually put something there. Another note, comments in a bison file DO NOT need to be indented like they do in flex.","title":"Bison Grammar"},{"location":"01-basic-cli/bison/#config-and-types","text":"%{ #include <stdio.h> #include <stdarg.h> %} %token INSERT %token QUIT %token SELECT %start cmd At the top we're including some standard libraries we make use of, as well as an interface to the lexer's line number tracker. In the next section, we define our tokens with the %token symbol. When bison builds this file, it takes all %tokens and stuffs them in an enum called yytokentype , which lives in bison's header file - the same file we #include d in our scanner. You can define multiple tokens on a single line by separating them with a space, e.g. %token INSERT QUIT SELECT . However, our grammar will eventually have a lot of tokens and I've found separating them alphabetically keeps things much more organized. The %start symbol is not entirely necessary for our simple grammar, but I like to include it to be explicit about the root of our grammar definition.","title":"Config and Types"},{"location":"01-basic-cli/bison/#grammar","text":"We define a grammar by defining a number of rules. These rules can be arbitrarily complex, but in our case we just have three simple tokens to handle. cmd: stmt { return 0; } ; stmt: select_stmt | insert_stmt | quit_stmt ; /* test comment */ select_stmt: SELECT { printf(\"SELECT command received\\n\"); } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); } ; quit_stmt: QUIT { printf(\"QUIT command received\\n\"); } ; Starting at the top, we have a rule named cmd . A cmd consists of a stmt rule, and when bison is able to satisfy the cmd rule, it returns 0 indicating success. Drilling deeper, a stmt rule can be one of three things: select_stmt , insert_stmt , or quit_stmt . Each of these has their own definition and associated code to run when encountered.","title":"Grammar"},{"location":"01-basic-cli/bison/#code-section","text":"Unlike the scanner, we are actually going to write some code for the third section. We are simply implementing our own error function that prints out a provided message. void yyerror(char* s, ...) { va_list ap; va_start(ap, s); fprintf(stderr, \"error: \"); vfprintf(stderr, s, ap); fprintf(stderr, \"\\n\"); }","title":"Code Section"},{"location":"01-basic-cli/bison/#full-file","text":"src/parser/gram.y %{ #include <stdio.h> #include <stdarg.h> %} %token INSERT %token QUIT %token SELECT %start cmd %% cmd: stmt { return 0; } ; stmt: select_stmt | insert_stmt | quit_stmt ; select_stmt: SELECT { printf(\"SELECT command received\\n\"); } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); } ; quit_stmt: QUIT { printf(\"QUIT command received\\n\"); } ; %% void yyerror(char* s, ...) { va_list ap; va_start(ap, s); fprintf(stderr, \"error: \"); vfprintf(stderr, s, ap); fprintf(stderr, \"\\n\"); }","title":"Full File"},{"location":"01-basic-cli/flex/","text":"A Flex Scanner Flex files have three sections separated by a %% symbol: /* flex config options, e.g. */ %option case-insensitive %{ /* code block for #includes or providing an interface to external functions */ }% %% /* 1st divider */ /* This second section is where we define regex patters for matching tokens from the input, e.g. */ /* matches any alphanumeric pattern and returns the matched token */ [a-zA-Z0-9]+ { return yytext; } %% /* 2nd divider */ /* The third section is where we can define C functions for use within the lexer */ The third section is entirely optional. If you want to use functions that are external to the flex file, you simply need to #include the appropriate header in the first section's code block. Enough with the intro, let's get started. Config %option noyywrap nodefault case-insensitive %{ #include \"gram.tab.h\" void yyerror(char* s, ...); }% Again, I'm not deep-diving flex here so I won't be going into the %options , but just know we need all three of them for the scanner to behave how we want. In the code block we include the header file that bison produces (explained in the next section), as well as an interface to bison's error function. Regex The second section is pretty straightforward. We're simply defining regex patters for our scanner to match against its input. Currently, we're implementing only three commands: insert , quit , and select . Therefore, we need a match pattern for each one. /* keywords (alphabetical order) */ INSERT { return INSERT; } QUIT { return QUIT; } SELECT { return SELECT; } /* everything else */ [ \\t\\n] /* whitespace - ignore */ . { yyerror(\"unknown character '%c'\", *yytext); } A couple things are going on here. At the beginning of the line, we specify our regex pattern. Since we want to match our three commands exactly, the pattern is just the command itself. After the regex pattern, we see code in curly brackets - this is what flex runs when it finds a match. If the word \"insert\" comes along, flex will match it and return the INSERT token to bison. How does flex know that INSERT is a valid token? Bison defines all tokens in its gram.tab.h header file that we included at the top. Note: comments in this section MUST be indented. Flex assumes anything at the start of the line is a regex pattern and will throw a compilation error. Code Section Empty. Full File src/parser/scan.l %option noyywrap nodefault yylineno case-insensitive %{ #include \"gram.tab.h\" void yyerror(char* s, ...); %} %% /* keywords */ INSERT { return INSERT; } QUIT { return QUIT; } SELECT { return SELECT; } /* everything else */ [ \\t\\n] /* whitespace */ . { yyerror(\"unknown character '%c'\", *yytext); } %%","title":"Flex"},{"location":"01-basic-cli/flex/#a-flex-scanner","text":"Flex files have three sections separated by a %% symbol: /* flex config options, e.g. */ %option case-insensitive %{ /* code block for #includes or providing an interface to external functions */ }% %% /* 1st divider */ /* This second section is where we define regex patters for matching tokens from the input, e.g. */ /* matches any alphanumeric pattern and returns the matched token */ [a-zA-Z0-9]+ { return yytext; } %% /* 2nd divider */ /* The third section is where we can define C functions for use within the lexer */ The third section is entirely optional. If you want to use functions that are external to the flex file, you simply need to #include the appropriate header in the first section's code block. Enough with the intro, let's get started.","title":"A Flex Scanner"},{"location":"01-basic-cli/flex/#config","text":"%option noyywrap nodefault case-insensitive %{ #include \"gram.tab.h\" void yyerror(char* s, ...); }% Again, I'm not deep-diving flex here so I won't be going into the %options , but just know we need all three of them for the scanner to behave how we want. In the code block we include the header file that bison produces (explained in the next section), as well as an interface to bison's error function.","title":"Config"},{"location":"01-basic-cli/flex/#regex","text":"The second section is pretty straightforward. We're simply defining regex patters for our scanner to match against its input. Currently, we're implementing only three commands: insert , quit , and select . Therefore, we need a match pattern for each one. /* keywords (alphabetical order) */ INSERT { return INSERT; } QUIT { return QUIT; } SELECT { return SELECT; } /* everything else */ [ \\t\\n] /* whitespace - ignore */ . { yyerror(\"unknown character '%c'\", *yytext); } A couple things are going on here. At the beginning of the line, we specify our regex pattern. Since we want to match our three commands exactly, the pattern is just the command itself. After the regex pattern, we see code in curly brackets - this is what flex runs when it finds a match. If the word \"insert\" comes along, flex will match it and return the INSERT token to bison. How does flex know that INSERT is a valid token? Bison defines all tokens in its gram.tab.h header file that we included at the top. Note: comments in this section MUST be indented. Flex assumes anything at the start of the line is a regex pattern and will throw a compilation error.","title":"Regex"},{"location":"01-basic-cli/flex/#code-section","text":"Empty.","title":"Code Section"},{"location":"01-basic-cli/flex/#full-file","text":"src/parser/scan.l %option noyywrap nodefault yylineno case-insensitive %{ #include \"gram.tab.h\" void yyerror(char* s, ...); %} %% /* keywords */ INSERT { return INSERT; } QUIT { return QUIT; } SELECT { return SELECT; } /* everything else */ [ \\t\\n] /* whitespace */ . { yyerror(\"unknown character '%c'\", *yytext); } %%","title":"Full File"},{"location":"01-basic-cli/intro/","text":"Basic CLI Before we begin writing the database internals, we need to create the scaffolding that will allow a user to interact with the database. Our implementation will be extremely basic to start, in fact it will only support three commands: quit , insert , and select . When we start the CLI program, it will immediately enter an infinite loop that waits for user input. When it receives a command from the user, it will evaluate the command to check its validity then execute it if valid. Although our CLI is basic in functionality, we are going to seemingly over-engineer the input parsing by using two purpose-built tools: flex and bison. Flex and Bison Installation on Ubuntu: sudo apt update sudo apt install flex sudo apt install bison These two tools are used all over the place to build compilers. In fact, postgres uses them to implement its SQL parser. With flex, you build what's called a lexer - a program that scans input from a buffer and looks for matching patterns. Those matching patterns are chunked together into tokens and passed on to bison. Bison takes those tokens and attempts to fit them into its grammar - the syntax rules we define. For example, say we pass the following query to a flex/bison SQL parser: Select colname From tablename; Flex would find five tokens: 'Select', 'colname', 'From', 'tablename', and ';'. It sends those tokens one at a time to bison for syntax analysis. Since this is a valid SQL query, bison will not throw an error. But what if we tried to send this through a SQL parser: Select From tablename; Flex would find four tokens this time and send them to bison one at a time. Bison receives the 'Select' token and starts down the rules path for a select statement. Bison is now expecting a list of column names (or a '*'), but it receives the 'From' token, which is a reserved keyword in SQL and not valid in a select column list. Because the syntax is invalid, bison throws an error stating invalid syntax. I won't be doing a deep dive on flex and bison, as they are very flexible tools that can do a lot more than what we need. Instead, I'm just going to give a high level explanation of our implementation. If you don't care about the lexing and parsing part of the code, then you can skip those sections. BQL As we build out the database system, we'll naturally need a more robust query language than the three basic commands we're starting with. That's where BQL comes in. BQL stands for Burke Query Language, or if you're offput by my vanity, you can call it Basic Query Language. BQL is going to look a lot like SQL, but it won't be nearly as complex as any of the existing SQL dialects out there. However, we will make it robust enough to do everything we need to do.","title":"Intro"},{"location":"01-basic-cli/intro/#basic-cli","text":"Before we begin writing the database internals, we need to create the scaffolding that will allow a user to interact with the database. Our implementation will be extremely basic to start, in fact it will only support three commands: quit , insert , and select . When we start the CLI program, it will immediately enter an infinite loop that waits for user input. When it receives a command from the user, it will evaluate the command to check its validity then execute it if valid. Although our CLI is basic in functionality, we are going to seemingly over-engineer the input parsing by using two purpose-built tools: flex and bison.","title":"Basic CLI"},{"location":"01-basic-cli/intro/#flex-and-bison","text":"Installation on Ubuntu: sudo apt update sudo apt install flex sudo apt install bison These two tools are used all over the place to build compilers. In fact, postgres uses them to implement its SQL parser. With flex, you build what's called a lexer - a program that scans input from a buffer and looks for matching patterns. Those matching patterns are chunked together into tokens and passed on to bison. Bison takes those tokens and attempts to fit them into its grammar - the syntax rules we define. For example, say we pass the following query to a flex/bison SQL parser: Select colname From tablename; Flex would find five tokens: 'Select', 'colname', 'From', 'tablename', and ';'. It sends those tokens one at a time to bison for syntax analysis. Since this is a valid SQL query, bison will not throw an error. But what if we tried to send this through a SQL parser: Select From tablename; Flex would find four tokens this time and send them to bison one at a time. Bison receives the 'Select' token and starts down the rules path for a select statement. Bison is now expecting a list of column names (or a '*'), but it receives the 'From' token, which is a reserved keyword in SQL and not valid in a select column list. Because the syntax is invalid, bison throws an error stating invalid syntax. I won't be doing a deep dive on flex and bison, as they are very flexible tools that can do a lot more than what we need. Instead, I'm just going to give a high level explanation of our implementation. If you don't care about the lexing and parsing part of the code, then you can skip those sections.","title":"Flex and Bison"},{"location":"01-basic-cli/intro/#bql","text":"As we build out the database system, we'll naturally need a more robust query language than the three basic commands we're starting with. That's where BQL comes in. BQL stands for Burke Query Language, or if you're offput by my vanity, you can call it Basic Query Language. BQL is going to look a lot like SQL, but it won't be nearly as complex as any of the existing SQL dialects out there. However, we will make it robust enough to do everything we need to do.","title":"BQL"},{"location":"01-basic-cli/putting-it-together/","text":"The main.c The last piece we need is an outer program that can call the parser. We define our main.c as follows: src/main.c #include <stdio.h> #include <stdlib.h> #include \"gram.tab.h\" static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { print_prompt(); while (!yyparse()) { print_prompt(); } return EXIT_SUCCESS; } Our program prints out a prompt and waits for user input. Upon a successful parse, it prints the prompt again and waits for more input. Note: before you compile everything, gram.tab.h won't exist, so your intellisense will probably yell at you. This is fine and you can ignore it. Makefiles Just like I didn't dive into flex and bison, I'm also not going to dive into makefiles. Mostly because I'm still a beginner at them myself. But, here are the makefiles that will build a working program: Inner Makefile src/Makefile CC = gcc LEX = flex YACC = bison CFLAGS = -fsanitize=address -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. SRC_FILES = main.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c rm -f $(BUILD_DIR)/$(TARGET_EXEC) Outer Makefile The entire purpose of this inner Makefile is because I am too lazy to type cd src && make . I just want to be able to call all make functionality from the same directory I can call my git commands from. Makefile all: cd src && $(MAKE) clean: rm -f ./bql cd src && $(MAKE) clean Current Directory Tree At this point, our project should look like this: \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 main.c \u2514\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u2514\u2500\u2500 scan.l Running the Program $ make *** There will be several warnings, but you can safely ignore them *** make[1]: Leaving directory '***' $ ./burkeql bql > select SELECT command received bql > insert INSERT command received bql > quit QUIT command received bql > foo error: unknown character 'f' error: unknown character 'o' error: unknown character 'o' As you can see the parser is able to recognize our three commands, and even gracefully ignore whitespace. However, the error handling is quite pedestrian. Don't worry, we'll fix it later.","title":"Putting It Together"},{"location":"01-basic-cli/putting-it-together/#the-mainc","text":"The last piece we need is an outer program that can call the parser. We define our main.c as follows: src/main.c #include <stdio.h> #include <stdlib.h> #include \"gram.tab.h\" static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { print_prompt(); while (!yyparse()) { print_prompt(); } return EXIT_SUCCESS; } Our program prints out a prompt and waits for user input. Upon a successful parse, it prints the prompt again and waits for more input. Note: before you compile everything, gram.tab.h won't exist, so your intellisense will probably yell at you. This is fine and you can ignore it.","title":"The main.c"},{"location":"01-basic-cli/putting-it-together/#makefiles","text":"Just like I didn't dive into flex and bison, I'm also not going to dive into makefiles. Mostly because I'm still a beginner at them myself. But, here are the makefiles that will build a working program:","title":"Makefiles"},{"location":"01-basic-cli/putting-it-together/#inner-makefile","text":"src/Makefile CC = gcc LEX = flex YACC = bison CFLAGS = -fsanitize=address -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. SRC_FILES = main.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c rm -f $(BUILD_DIR)/$(TARGET_EXEC)","title":"Inner Makefile"},{"location":"01-basic-cli/putting-it-together/#outer-makefile","text":"The entire purpose of this inner Makefile is because I am too lazy to type cd src && make . I just want to be able to call all make functionality from the same directory I can call my git commands from. Makefile all: cd src && $(MAKE) clean: rm -f ./bql cd src && $(MAKE) clean","title":"Outer Makefile"},{"location":"01-basic-cli/putting-it-together/#current-directory-tree","text":"At this point, our project should look like this: \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 main.c \u2514\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u2514\u2500\u2500 scan.l","title":"Current Directory Tree"},{"location":"01-basic-cli/putting-it-together/#running-the-program","text":"$ make *** There will be several warnings, but you can safely ignore them *** make[1]: Leaving directory '***' $ ./burkeql bql > select SELECT command received bql > insert INSERT command received bql > quit QUIT command received bql > foo error: unknown character 'f' error: unknown character 'o' error: unknown character 'o' As you can see the parser is able to recognize our three commands, and even gracefully ignore whitespace. However, the error handling is quite pedestrian. Don't worry, we'll fix it later.","title":"Running the Program"},{"location":"02-ast/ast-implementation/","text":"AST Implementation With our header file defined, we can start implementing the AST functionality. First we'll define our free_node function. Since it needs to handle all types of nodes, we'll use a switch statement on the NodeTag property. void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; default: printf(\"Unknown node type\\n\"); } free(n); } A forward thinker will see that although we only need to expose a single free function, we still need to write specific functions for all different node types. Fortunately, those only need to exist in this file. static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } We define a static function for the SysCmd node that frees memory for all properties within the SysCmd node. Note that we do not free the SysCmd itself because the outer free_node function will do it for us. Finally, we write the print function: void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; default: printf(\"print_node() | unknown node type\\n\"); } } This function has a similar structure to the free_node function. Later down the road, we will write corresponding static print_ functions for each node type, but for now the SysCmd node is simple enough to live in the switch branch. Full File src/parser/parsetree.c #include <stdlib.h> #include <stdio.h> #include \"parser/parsetree.h\" static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; default: printf(\"Unknown node type\\n\"); } free(n); } void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; default: printf(\"print_node() | unknown node type\\n\"); } }","title":"AST Implementation"},{"location":"02-ast/ast-implementation/#ast-implementation","text":"With our header file defined, we can start implementing the AST functionality. First we'll define our free_node function. Since it needs to handle all types of nodes, we'll use a switch statement on the NodeTag property. void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; default: printf(\"Unknown node type\\n\"); } free(n); } A forward thinker will see that although we only need to expose a single free function, we still need to write specific functions for all different node types. Fortunately, those only need to exist in this file. static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } We define a static function for the SysCmd node that frees memory for all properties within the SysCmd node. Note that we do not free the SysCmd itself because the outer free_node function will do it for us. Finally, we write the print function: void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; default: printf(\"print_node() | unknown node type\\n\"); } } This function has a similar structure to the free_node function. Later down the road, we will write corresponding static print_ functions for each node type, but for now the SysCmd node is simple enough to live in the switch branch.","title":"AST Implementation"},{"location":"02-ast/ast-implementation/#full-file","text":"src/parser/parsetree.c #include <stdlib.h> #include <stdio.h> #include \"parser/parsetree.h\" static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; default: printf(\"Unknown node type\\n\"); } free(n); } void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; default: printf(\"print_node() | unknown node type\\n\"); } }","title":"Full File"},{"location":"02-ast/ast-interface/","text":"AST Interface The first thing we need to do is begin writing the header file for our AST. In it we'll define all structs available to the parser, as well as some helper functions for creating and destroying nodes. The Node The most basic unit of our AST is the Node struct - every single node in our tree is a Node . A Node can be one of many different types that we'll define later, but they're all just the same base unit. The primary benefit of doing it this way is minimizing the number of tree-walker functions we need to expose to external processes. We only need to expose a single void process_node(Node* n) function that is capable of handling everything. If we had separate node types, e.g. SelectNode , FromNode , SortNode , etc. We would need to expose a different function for each of them. Moreover, there's a clever trick we can do with macros that makes creating different node types in the parser very easy. Header File Structs First we define our structs: typedef enum NodeTag { T_SysCmd } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; The NodeTag enum is how we signify the type of node we're dealing with. Next up is the Node , our basic AST building block. Containing only a single property, the Node 's only purpose is a common junction for jumping between node types. And finally, we introduce the SysCmd struct. All SysCmd structs will have type = T_SysCmd , and a string representing the command the user passed to it. For example, the quit command ( \\q ) will produce a SysCmd struct with cmd = \"q\" . Macros Next we define a couple macros that well help us in the bison grammar for easily creating nodes. Though the \"easily\" part may not become apparent until we begin supporting a lot more node types. #define new_node(size, tag) \\ ({ Node* _result; \\ _result = (Node*)malloc(size); \\ _result->type = (tag); \\ _result; \\ }) #define create_node(_type_) ((_type_*) new_node(sizeof(_type_), T_##_type_)) When the parser encounters a system command, the grammar rule will have it run: SysCmd* sc = create_node(SysCmd); Using macro expansion, it's able to allocate the correct amount of memory for the SysCmd node type and set the type property correctly. The create_node macro is essentially shorthand for: SysCmd* sc = malloc(sizeof(SysCmd)); sc->type = T_SysCmd; API Finally, we want to define the interface functions to our AST: void free_node(Node* n); void print_node(Node* n); At the moment, we only need two functions - one for cleanup and one for printing/debugging. Again, because we use the Node as our basic building block, we're only exposing free_node and print_node instead of free_syscmd and print_syscmd (plus any other future node types). Full File src/include/parser/parsetree.h #ifndef PARSETREE_H #define PARSETREE_H typedef enum NodeTag { T_SysCmd } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; #define new_node(size, tag) \\ ({ Node* _result; \\ _result = (Node*)malloc(size); \\ _result->type = (tag); \\ _result; \\ }) #define create_node(_type_) ((_type_*) new_node(sizeof(_type_), T_##_type_)) void free_node(Node* n); void print_node(Node* n); #endif /* PARSETREE_H */","title":"AST Interface"},{"location":"02-ast/ast-interface/#ast-interface","text":"The first thing we need to do is begin writing the header file for our AST. In it we'll define all structs available to the parser, as well as some helper functions for creating and destroying nodes.","title":"AST Interface"},{"location":"02-ast/ast-interface/#the-node","text":"The most basic unit of our AST is the Node struct - every single node in our tree is a Node . A Node can be one of many different types that we'll define later, but they're all just the same base unit. The primary benefit of doing it this way is minimizing the number of tree-walker functions we need to expose to external processes. We only need to expose a single void process_node(Node* n) function that is capable of handling everything. If we had separate node types, e.g. SelectNode , FromNode , SortNode , etc. We would need to expose a different function for each of them. Moreover, there's a clever trick we can do with macros that makes creating different node types in the parser very easy.","title":"The Node"},{"location":"02-ast/ast-interface/#header-file","text":"","title":"Header File"},{"location":"02-ast/ast-interface/#structs","text":"First we define our structs: typedef enum NodeTag { T_SysCmd } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; The NodeTag enum is how we signify the type of node we're dealing with. Next up is the Node , our basic AST building block. Containing only a single property, the Node 's only purpose is a common junction for jumping between node types. And finally, we introduce the SysCmd struct. All SysCmd structs will have type = T_SysCmd , and a string representing the command the user passed to it. For example, the quit command ( \\q ) will produce a SysCmd struct with cmd = \"q\" .","title":"Structs"},{"location":"02-ast/ast-interface/#macros","text":"Next we define a couple macros that well help us in the bison grammar for easily creating nodes. Though the \"easily\" part may not become apparent until we begin supporting a lot more node types. #define new_node(size, tag) \\ ({ Node* _result; \\ _result = (Node*)malloc(size); \\ _result->type = (tag); \\ _result; \\ }) #define create_node(_type_) ((_type_*) new_node(sizeof(_type_), T_##_type_)) When the parser encounters a system command, the grammar rule will have it run: SysCmd* sc = create_node(SysCmd); Using macro expansion, it's able to allocate the correct amount of memory for the SysCmd node type and set the type property correctly. The create_node macro is essentially shorthand for: SysCmd* sc = malloc(sizeof(SysCmd)); sc->type = T_SysCmd;","title":"Macros"},{"location":"02-ast/ast-interface/#api","text":"Finally, we want to define the interface functions to our AST: void free_node(Node* n); void print_node(Node* n); At the moment, we only need two functions - one for cleanup and one for printing/debugging. Again, because we use the Node as our basic building block, we're only exposing free_node and print_node instead of free_syscmd and print_syscmd (plus any other future node types).","title":"API"},{"location":"02-ast/ast-interface/#full-file","text":"src/include/parser/parsetree.h #ifndef PARSETREE_H #define PARSETREE_H typedef enum NodeTag { T_SysCmd } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; #define new_node(size, tag) \\ ({ Node* _result; \\ _result = (Node*)malloc(size); \\ _result->type = (tag); \\ _result; \\ }) #define create_node(_type_) ((_type_*) new_node(sizeof(_type_), T_##_type_)) void free_node(Node* n); void print_node(Node* n); #endif /* PARSETREE_H */","title":"Full File"},{"location":"02-ast/intro/","text":"Abstract Syntax Trees Now that we have the basic building blocks of a SQL parser in place, it's time to start turning it into something useful. In this section we'll be making several config changes to the flex and bison tools, as well as introducing the concept of abstract syntax trees. I'll try to prune out the unnecessary info and instead provide links for further reading if you're interested. An abstract syntax tree (AST) is a very powerful data structure that's often used in compilers. Typically, each node in the tree represents a rule defined in the grammar - though not every rule is represented by a node. Some rules exist only to inform the parser about things like precedence, groupings, etc. The reason parsers produce ASTs is because it's relatively easy to write functions that walk the tree in order to perform its task. In a relational database, the parser AST is sent to the analyzer, which then walks the tree to perform semantic analysis. Basically, it's just checking that named references (tables, columns, etc.) actually exist in the database. Example SQL AST Consider this SQL query: Select col1, col2 From tableA Where col1 = 5 Order By col2 The resulting AST from the parser might look something like this: Any downstream process would walk through the tree by starting at the root node, a Select node, and traverse down through each branch to the leaf nodes. The tree-walker would need to know what child nodes to expect, but that's not a difficult thing to do. Goals If you remember from the previous section, we built a parser that is able to recognize three commands: insert , select , and quit . Our goal for this section is to refactor the parser to return an AST to the caller, and actually implement the quit command. Refactoring will involve changing some flex and bison config options that I won't delve very deep into. We're also going to change the quit syntax from the word \"quit\" into \\q - the same command postgres' psql utility uses. This is an important change because it lays the foundation for implementing other system commands we might want later down the road. We'll be telling our parser that command beginning with a backslash should be treated as a system command.","title":"Intro"},{"location":"02-ast/intro/#abstract-syntax-trees","text":"Now that we have the basic building blocks of a SQL parser in place, it's time to start turning it into something useful. In this section we'll be making several config changes to the flex and bison tools, as well as introducing the concept of abstract syntax trees. I'll try to prune out the unnecessary info and instead provide links for further reading if you're interested. An abstract syntax tree (AST) is a very powerful data structure that's often used in compilers. Typically, each node in the tree represents a rule defined in the grammar - though not every rule is represented by a node. Some rules exist only to inform the parser about things like precedence, groupings, etc. The reason parsers produce ASTs is because it's relatively easy to write functions that walk the tree in order to perform its task. In a relational database, the parser AST is sent to the analyzer, which then walks the tree to perform semantic analysis. Basically, it's just checking that named references (tables, columns, etc.) actually exist in the database.","title":"Abstract Syntax Trees"},{"location":"02-ast/intro/#example-sql-ast","text":"Consider this SQL query: Select col1, col2 From tableA Where col1 = 5 Order By col2 The resulting AST from the parser might look something like this: Any downstream process would walk through the tree by starting at the root node, a Select node, and traverse down through each branch to the leaf nodes. The tree-walker would need to know what child nodes to expect, but that's not a difficult thing to do.","title":"Example SQL AST"},{"location":"02-ast/intro/#goals","text":"If you remember from the previous section, we built a parser that is able to recognize three commands: insert , select , and quit . Our goal for this section is to refactor the parser to return an AST to the caller, and actually implement the quit command. Refactoring will involve changing some flex and bison config options that I won't delve very deep into. We're also going to change the quit syntax from the word \"quit\" into \\q - the same command postgres' psql utility uses. This is an important change because it lays the foundation for implementing other system commands we might want later down the road. We'll be telling our parser that command beginning with a backslash should be treated as a system command.","title":"Goals"},{"location":"02-ast/parser-interface/","text":"Parser Interface Since interacting with our parser is going to get more complicated, I am going to pull some of that code out of our main function. We'll define a very basic parse.h header: #include \"parsetree.h\" Node* parse_sql(); Very simple. We just expose a function that returns the root Node of an AST. And we define the function as follows: Node* parse_sql() { Node* n; yyscan_t scanner; if (yylex_init(&scanner) != 0) { printf(\"scan init failed\\n\"); return NULL; } int e = yyparse(&n, scanner); yylex_destroy(scanner); if (e != 0) printf(\"Parse error\\n\"); return n; } First we have to initialize the scanner, something we didn't have to do before. This is why we needed flex to generate a header file for us - we need access to its structs and functions. Then we call yyparse just like last time; however, now we're able to send it our Node* struct so that it can build an AST for us. Finally, since we initialized a scanner, we also have to destroy it. Then we can return the AST to the caller. Full Files src/include/parser/parse.h #ifndef PARSE_H #define PARSE_H #include \"parsetree.h\" Node* parse_sql(); #endif /* PARSE_H */ src/parser/parse.c #include <stdio.h> #include \"gram.tab.h\" #include \"scan.lex.h\" #include \"parser/parsetree.h\" Node* parse_sql() { Node* n; yyscan_t scanner; if (yylex_init(&scanner) != 0) { printf(\"scan init failed\\n\"); return NULL; } int e = yyparse(&n, scanner); yylex_destroy(scanner); if (e != 0) printf(\"Parse error\\n\"); return n; }","title":"Parser Interface"},{"location":"02-ast/parser-interface/#parser-interface","text":"Since interacting with our parser is going to get more complicated, I am going to pull some of that code out of our main function. We'll define a very basic parse.h header: #include \"parsetree.h\" Node* parse_sql(); Very simple. We just expose a function that returns the root Node of an AST. And we define the function as follows: Node* parse_sql() { Node* n; yyscan_t scanner; if (yylex_init(&scanner) != 0) { printf(\"scan init failed\\n\"); return NULL; } int e = yyparse(&n, scanner); yylex_destroy(scanner); if (e != 0) printf(\"Parse error\\n\"); return n; } First we have to initialize the scanner, something we didn't have to do before. This is why we needed flex to generate a header file for us - we need access to its structs and functions. Then we call yyparse just like last time; however, now we're able to send it our Node* struct so that it can build an AST for us. Finally, since we initialized a scanner, we also have to destroy it. Then we can return the AST to the caller.","title":"Parser Interface"},{"location":"02-ast/parser-interface/#full-files","text":"src/include/parser/parse.h #ifndef PARSE_H #define PARSE_H #include \"parsetree.h\" Node* parse_sql(); #endif /* PARSE_H */ src/parser/parse.c #include <stdio.h> #include \"gram.tab.h\" #include \"scan.lex.h\" #include \"parser/parsetree.h\" Node* parse_sql() { Node* n; yyscan_t scanner; if (yylex_init(&scanner) != 0) { printf(\"scan init failed\\n\"); return NULL; } int e = yyparse(&n, scanner); yylex_destroy(scanner); if (e != 0) printf(\"Parse error\\n\"); return n; }","title":"Full Files"},{"location":"02-ast/parser-refactor/","text":"Parser Refactor Now we've come to the potentially confusing part where we reconfigure flex and bison. Again, I will not be thoroughly explaining how these changes affect what flex and bison do under the hood. However, I will explain how it changes how we interact with the parser. You may have noticed the parser we built in the first section consumed our input and printed out a message. Our call to yyparse did not return anything, so there was nothing for us to send along to a downstream process. We need to reconfigure the parser to be able to return an AST to the caller. Flex The changes to our scanner are small, but they do introduce a new flex concept that I didn't cover in the last section: start states. %option noyywrap nodefault case-insensitive +%option bison-bridge reentrant +%option header-file=\"scan.lex.h\" + +%x SYSCMD %{ Simply put, two new options bison-bridge and reentrant work with the bison changes to allow it to work with our own custom structs and return them to us. The second new option line tells flex to generate a header file for us, which will be important when we refactor the code that calls the parser. Finally, the %x SYSCMD is what flex calls a start state, more specifically, an exclusive start state. This allows us to control which regex patters can be matched and when. %% + /* beginning a system command */ +[\\\\] { BEGIN SYSCMD; } + + /* valid system command inputs */ +<SYSCMD>[A-Za-z]+ { yylval->str = strdup(yytext); return SYS_CMD; } + /* keywords */ INSERT { return INSERT; } -QUIT { return QUIT; } The first regex pattern, [\\\\] tells flex what to look for in order to enter the SYSCMD state. The second new pattern is tagged with the start state and means flex is only allowed to match this pattern if it is already in the SYSCMD state. Once matched, we strdup the text into the yylval union and send it along to bison. The yylval union matches exactly the union we'll define in our grammar file. How all of this interplay works between flex and bison is not something I'll cover, so if you're interested you should read the documentation for each tool. And since we're changing how system commands are parsed, we no longer need the QUIT token. Bison The changes to our grammar are a lot more involved because we're essentially rewriting how it parses tokens from scratch. Config +%define api.pure true %{ #include <stdio.h> #include <stdarg.h> +#include \"include/parser/parsetree.h\" %} The new %define turns bison into a pure parser, which for us means we're able to pass in a struct when we call it. This is absolutely necessary since we want bison to build an AST for us. The struct we send to bison is the root node of our AST. We also need to include the AST header file so bison has access to the necessary structs and functions. Data Types This next set of changes defines the data interface between flex and bison. %} +%union { + char* str; + + struct Node* node; +} + +%parse-param { struct Node** n } +%param { void* scanner } As I mentioned above, the %union determines how the yylval union available to flex is defined. Anything we put in this union is settable by flex during the lexing stage. The %parse-param and %param add arguments to the yyparse function. %parse-param allows us to send our struct to bison so that it can build the AST for us. %param also lets us send data, but in this case bison forwards it along to yylex from flex. This is necessary because we are now setting up our own scanner instead of letting flex do it for us. Next, we need to define our new tokens and types: +%token <str> SYS_CMD + /* reserved keywords in alphabetical order */ %token INSERT - -%token QUIT %token SELECT +%type <node> cmd stmt sys_cmd select_stmt insert_stmt + -%start cmd +%start query %% Since we added a way for flex to tokenize a SYS_CMD , we need to define that token in our grammar file. And since we actually care WHAT it is that flex matched, we define a SYS_CMD as a str type. Note, type definitions like this MUST be defined in the %union before we can use them in the %token and %type tags. Now that we have our AST defined in the parsetree header file, we know that every node in the tree is going to be a Node struct. So we tell bison that all of our rules defined below are <node> types. And finally, I'm changing the %start from cmd to a new rule called query . This means I only need to write the termination code once. Grammar And finally we get to the rules. We're rewriting everything from scratch, so you can start by deleting everything in the second section of your grammar file (between the two %% symbols). This chunk of code is the entirety of our grammar at this point. %% query: cmd { *n = $1; YYACCEPT; } ; cmd: stmt | sys_cmd ; sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; stmt: select_stmt | insert_stmt ; select_stmt: SELECT { printf(\"SELECT command received\\n\"); $$ = NULL; } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); $$ = NULL; } ; %% I've added some bison functionality that wasn't necessary in the first iteration, so it might be a little confusing right now, but stay with me. Starting at the bottom with the select_stmt and insert_stmt rules. You'll notice they're largely the same as before, but we added a $$ = NULL; line. $$ is special bison syntax. In this case $$ represents the left side of our grammar rule definition. So in the insert_stmt rule, when we write $$ = NULL; , we're saying insert_stmt = NULL and in any other rules that insert_stmt bubbles up to its value will be NULL . Moving up to the stmt rule, we see it's defined as either a select_stmt or insert_stmt , but there's no code attached to it. Bison actually injects default code to all rules we don't specify them for. The default action is { $$ = $1; } . Just like $$ represents the left side of a rule, $1 corresponds to the right side of a rule - specifically, the first term of the right side of the rule. This will come in to play more when we start defining rules with multiple terms. Note: in the stmt rule, select_stmt and insert_stmt are separated by a pipe ( | ), meaning they're independent. Bison injects the default code for BOTH of them. This means they're both the first term on the right side of the rule. It's the same as writing this: stmt: select_stmt { $$ = $1; } | insert_stmt { $$ = $1; } ; Writing a rule like this is just a way to group similar rules together so that upstream rules only need to write a single code block instead of separate identical code blocks. Next, we get to the sys_cmd rule. When we have a more robust parser, this is what a lot of our grammar will look like. sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; We start by creating a new Node with the helper macro we wrote in the parsetree header. Then we set the cmd property to whatever flex tokenized and sent to bison - $1 . And finally we set the value of the left-hand side of the rule to the SysCmd node. Since we defined the sys_cmd rule to be a node type, we have to cast SysCmd* to Node* . Moving up the rules section, we see the cmd rule is similar to the stmt rule - just grouping things together. Finally, we get to the top of the tree: the query rule, which has some code we haven't seen before. When we get to this point, we set *n = $1; . We know $1 is equal to whatever previous processing stored in the result of the cmd rule, but what is *n ? Remember in the first section of the grammar file, we defined %parse-param { struct Node** n } . That's what n is referring to, and *n just means we're dereferencing it. This is important because the %parse-param is one of the arguments we supply to yyparse when we call it, and setting *n = $1 is how bison returns the AST back to the caller. The next line, YYACCEPT; , just tells bison we've hit the end of our parsing journey and are happy with the result. Flex to Bison Walkthrough Let's take a step back and walk through how we get from user input to the sys_cmd code block. When a user types \\quit in the terminal, flex starts reading one character at a time. It sees the \\ character and immediately recognizes this as matching the SYSCMD start state pattern, i.e. /* beginning a system command */ [\\\\] { BEGIN SYSCMD; } Now everything following the \\ is matched by the exclusive SYSCMD patterns, which in our case is any number of alphabet characters, i.e. /* valid system command inputs */ <SYSCMD>[A-Za-z]+ { yylval->str = strdup(yytext); return SYS_CMD; } Flex tries to match the longest chunk of input it can, so the entirety of the word quit is matched and stored in the yytext variable. This variable does not stick around, so we need to strdup it and store it in yylval->str and return the SYS_CMD token. Bison then consumes the token and finds the applicable grammar rule for it, sys_cmd: SYS_CMD . Since we defined %token <str> SYS_CMD as corresponding to the char* str; property of the %union , that's what becomes available in the $1 value. That's a lot to keep track of, I know, but here are the applicable pieces of the grammar file: %union { char* str; ... } %token <str> SYS_CMD %% sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; When flex sets yylval->str = \"quit\"; , bison does some magic behind the scenes and essentially sets $1 = \"quit\"; . Which means we can set sc->cmd = $1; and that stores the \"quit\" string in the cmd property. It's a lot to wrap your head around, but as we continue to add rules to our grammar it will begin to make a lot more sense.","title":"Parser Refactor"},{"location":"02-ast/parser-refactor/#parser-refactor","text":"Now we've come to the potentially confusing part where we reconfigure flex and bison. Again, I will not be thoroughly explaining how these changes affect what flex and bison do under the hood. However, I will explain how it changes how we interact with the parser. You may have noticed the parser we built in the first section consumed our input and printed out a message. Our call to yyparse did not return anything, so there was nothing for us to send along to a downstream process. We need to reconfigure the parser to be able to return an AST to the caller.","title":"Parser Refactor"},{"location":"02-ast/parser-refactor/#flex","text":"The changes to our scanner are small, but they do introduce a new flex concept that I didn't cover in the last section: start states. %option noyywrap nodefault case-insensitive +%option bison-bridge reentrant +%option header-file=\"scan.lex.h\" + +%x SYSCMD %{ Simply put, two new options bison-bridge and reentrant work with the bison changes to allow it to work with our own custom structs and return them to us. The second new option line tells flex to generate a header file for us, which will be important when we refactor the code that calls the parser. Finally, the %x SYSCMD is what flex calls a start state, more specifically, an exclusive start state. This allows us to control which regex patters can be matched and when. %% + /* beginning a system command */ +[\\\\] { BEGIN SYSCMD; } + + /* valid system command inputs */ +<SYSCMD>[A-Za-z]+ { yylval->str = strdup(yytext); return SYS_CMD; } + /* keywords */ INSERT { return INSERT; } -QUIT { return QUIT; } The first regex pattern, [\\\\] tells flex what to look for in order to enter the SYSCMD state. The second new pattern is tagged with the start state and means flex is only allowed to match this pattern if it is already in the SYSCMD state. Once matched, we strdup the text into the yylval union and send it along to bison. The yylval union matches exactly the union we'll define in our grammar file. How all of this interplay works between flex and bison is not something I'll cover, so if you're interested you should read the documentation for each tool. And since we're changing how system commands are parsed, we no longer need the QUIT token.","title":"Flex"},{"location":"02-ast/parser-refactor/#bison","text":"The changes to our grammar are a lot more involved because we're essentially rewriting how it parses tokens from scratch.","title":"Bison"},{"location":"02-ast/parser-refactor/#config","text":"+%define api.pure true %{ #include <stdio.h> #include <stdarg.h> +#include \"include/parser/parsetree.h\" %} The new %define turns bison into a pure parser, which for us means we're able to pass in a struct when we call it. This is absolutely necessary since we want bison to build an AST for us. The struct we send to bison is the root node of our AST. We also need to include the AST header file so bison has access to the necessary structs and functions.","title":"Config"},{"location":"02-ast/parser-refactor/#data-types","text":"This next set of changes defines the data interface between flex and bison. %} +%union { + char* str; + + struct Node* node; +} + +%parse-param { struct Node** n } +%param { void* scanner } As I mentioned above, the %union determines how the yylval union available to flex is defined. Anything we put in this union is settable by flex during the lexing stage. The %parse-param and %param add arguments to the yyparse function. %parse-param allows us to send our struct to bison so that it can build the AST for us. %param also lets us send data, but in this case bison forwards it along to yylex from flex. This is necessary because we are now setting up our own scanner instead of letting flex do it for us. Next, we need to define our new tokens and types: +%token <str> SYS_CMD + /* reserved keywords in alphabetical order */ %token INSERT - -%token QUIT %token SELECT +%type <node> cmd stmt sys_cmd select_stmt insert_stmt + -%start cmd +%start query %% Since we added a way for flex to tokenize a SYS_CMD , we need to define that token in our grammar file. And since we actually care WHAT it is that flex matched, we define a SYS_CMD as a str type. Note, type definitions like this MUST be defined in the %union before we can use them in the %token and %type tags. Now that we have our AST defined in the parsetree header file, we know that every node in the tree is going to be a Node struct. So we tell bison that all of our rules defined below are <node> types. And finally, I'm changing the %start from cmd to a new rule called query . This means I only need to write the termination code once.","title":"Data Types"},{"location":"02-ast/parser-refactor/#grammar","text":"And finally we get to the rules. We're rewriting everything from scratch, so you can start by deleting everything in the second section of your grammar file (between the two %% symbols). This chunk of code is the entirety of our grammar at this point. %% query: cmd { *n = $1; YYACCEPT; } ; cmd: stmt | sys_cmd ; sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; stmt: select_stmt | insert_stmt ; select_stmt: SELECT { printf(\"SELECT command received\\n\"); $$ = NULL; } ; insert_stmt: INSERT { printf(\"INSERT command received\\n\"); $$ = NULL; } ; %% I've added some bison functionality that wasn't necessary in the first iteration, so it might be a little confusing right now, but stay with me. Starting at the bottom with the select_stmt and insert_stmt rules. You'll notice they're largely the same as before, but we added a $$ = NULL; line. $$ is special bison syntax. In this case $$ represents the left side of our grammar rule definition. So in the insert_stmt rule, when we write $$ = NULL; , we're saying insert_stmt = NULL and in any other rules that insert_stmt bubbles up to its value will be NULL . Moving up to the stmt rule, we see it's defined as either a select_stmt or insert_stmt , but there's no code attached to it. Bison actually injects default code to all rules we don't specify them for. The default action is { $$ = $1; } . Just like $$ represents the left side of a rule, $1 corresponds to the right side of a rule - specifically, the first term of the right side of the rule. This will come in to play more when we start defining rules with multiple terms. Note: in the stmt rule, select_stmt and insert_stmt are separated by a pipe ( | ), meaning they're independent. Bison injects the default code for BOTH of them. This means they're both the first term on the right side of the rule. It's the same as writing this: stmt: select_stmt { $$ = $1; } | insert_stmt { $$ = $1; } ; Writing a rule like this is just a way to group similar rules together so that upstream rules only need to write a single code block instead of separate identical code blocks. Next, we get to the sys_cmd rule. When we have a more robust parser, this is what a lot of our grammar will look like. sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; We start by creating a new Node with the helper macro we wrote in the parsetree header. Then we set the cmd property to whatever flex tokenized and sent to bison - $1 . And finally we set the value of the left-hand side of the rule to the SysCmd node. Since we defined the sys_cmd rule to be a node type, we have to cast SysCmd* to Node* . Moving up the rules section, we see the cmd rule is similar to the stmt rule - just grouping things together. Finally, we get to the top of the tree: the query rule, which has some code we haven't seen before. When we get to this point, we set *n = $1; . We know $1 is equal to whatever previous processing stored in the result of the cmd rule, but what is *n ? Remember in the first section of the grammar file, we defined %parse-param { struct Node** n } . That's what n is referring to, and *n just means we're dereferencing it. This is important because the %parse-param is one of the arguments we supply to yyparse when we call it, and setting *n = $1 is how bison returns the AST back to the caller. The next line, YYACCEPT; , just tells bison we've hit the end of our parsing journey and are happy with the result.","title":"Grammar"},{"location":"02-ast/parser-refactor/#flex-to-bison-walkthrough","text":"Let's take a step back and walk through how we get from user input to the sys_cmd code block. When a user types \\quit in the terminal, flex starts reading one character at a time. It sees the \\ character and immediately recognizes this as matching the SYSCMD start state pattern, i.e. /* beginning a system command */ [\\\\] { BEGIN SYSCMD; } Now everything following the \\ is matched by the exclusive SYSCMD patterns, which in our case is any number of alphabet characters, i.e. /* valid system command inputs */ <SYSCMD>[A-Za-z]+ { yylval->str = strdup(yytext); return SYS_CMD; } Flex tries to match the longest chunk of input it can, so the entirety of the word quit is matched and stored in the yytext variable. This variable does not stick around, so we need to strdup it and store it in yylval->str and return the SYS_CMD token. Bison then consumes the token and finds the applicable grammar rule for it, sys_cmd: SYS_CMD . Since we defined %token <str> SYS_CMD as corresponding to the char* str; property of the %union , that's what becomes available in the $1 value. That's a lot to keep track of, I know, but here are the applicable pieces of the grammar file: %union { char* str; ... } %token <str> SYS_CMD %% sys_cmd: SYS_CMD { SysCmd* sc = create_node(SysCmd); sc->cmd = $1; $$ = (Node*)sc; } ; When flex sets yylval->str = \"quit\"; , bison does some magic behind the scenes and essentially sets $1 = \"quit\"; . Which means we can set sc->cmd = $1; and that stores the \"quit\" string in the cmd property. It's a lot to wrap your head around, but as we continue to add rules to our grammar it will begin to make a lot more sense.","title":"Flex to Bison Walkthrough"},{"location":"02-ast/putting-it-together/","text":"Putting It Together With all of the complex pieces code written, all we need to do is refactor the main.c file and update our Makefile . Here's the full diff of src/main.c : #include <stdio.h> #include <stdlib.h> +#include <stdbool.h> +#include <string.h> #include \"gram.tab.h\" +#include \"parser/parsetree.h\" +#include \"parser/parse.h\" static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { - print_prompt(); - while(!yyparse()) { + while(true) { + print_prompt(); + Node* n = parse_sql(); + + if (n == NULL) continue; + + switch (n->type) { + case T_SysCmd: + if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { + print_node(n); + free_node(n); + printf(\"Shutting down...\\n\"); + return EXIT_SUCCESS; + } + default: + print_node(n); + free_node(n); + } + } return EXIT_SUCCESS; } We essentially rewrite the main function from scratch. Using an infinite loop, we make a call to the parse function and get an AST in return. If the tree is NULL , as is the case for our current insert_stmt and select_stmt grammar paths, we jump back to the beginning of the loop. If we receive an actual AST, we check to see if the user provided the \\quit command. If so, we clean up our allocated memory and exit the program. Makefile Here's the full diff of the inner Makefile ( src/Makefile ): CC = gcc LEX = flex YACC = bison -CFLAGS = -fsanitize=address -static-libasan -g +CFLAGS = -I./ -I./include -fsanitize=address -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. -SRC_FILES = main.c +SRC_FILES = main.c \\ + parser/parse.c \\ + parser/parsetree.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c + rm -f $(wildcard *.lex.*) rm -f $(BUILD_DIR)/$(TARGET_EXEC) Project Structure And here is how our repo structure looks: \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 include \u2502 \u2514\u2500\u2500 parser \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u2514\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u251c\u2500\u2500 parse.c \u251c\u2500\u2500 parsetree.c \u2514\u2500\u2500 scan.l Running the Program $ make *** There will be several warnings, but you can safely ignore them *** make[1]: Leaving directory '***' $ ./burkeql bql > select SELECT command received bql > insert INSERT command received bql > \\q ====== Node ====== = Type: SysCmd = Cmd: q bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ Our parser still recognizes the select and insert tokens, but we don't do anything with them. It also recognizes a \\ as a system command flag, but \\q isn't a valid system command so we don't do anything with it. But when we type \\quit , our code recognizes the system command flag AND knows to actually quit the program.","title":"Putting It Together"},{"location":"02-ast/putting-it-together/#putting-it-together","text":"With all of the complex pieces code written, all we need to do is refactor the main.c file and update our Makefile . Here's the full diff of src/main.c : #include <stdio.h> #include <stdlib.h> +#include <stdbool.h> +#include <string.h> #include \"gram.tab.h\" +#include \"parser/parsetree.h\" +#include \"parser/parse.h\" static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { - print_prompt(); - while(!yyparse()) { + while(true) { + print_prompt(); + Node* n = parse_sql(); + + if (n == NULL) continue; + + switch (n->type) { + case T_SysCmd: + if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { + print_node(n); + free_node(n); + printf(\"Shutting down...\\n\"); + return EXIT_SUCCESS; + } + default: + print_node(n); + free_node(n); + } + } return EXIT_SUCCESS; } We essentially rewrite the main function from scratch. Using an infinite loop, we make a call to the parse function and get an AST in return. If the tree is NULL , as is the case for our current insert_stmt and select_stmt grammar paths, we jump back to the beginning of the loop. If we receive an actual AST, we check to see if the user provided the \\quit command. If so, we clean up our allocated memory and exit the program.","title":"Putting It Together"},{"location":"02-ast/putting-it-together/#makefile","text":"Here's the full diff of the inner Makefile ( src/Makefile ): CC = gcc LEX = flex YACC = bison -CFLAGS = -fsanitize=address -static-libasan -g +CFLAGS = -I./ -I./include -fsanitize=address -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. -SRC_FILES = main.c +SRC_FILES = main.c \\ + parser/parse.c \\ + parser/parsetree.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c + rm -f $(wildcard *.lex.*) rm -f $(BUILD_DIR)/$(TARGET_EXEC)","title":"Makefile"},{"location":"02-ast/putting-it-together/#project-structure","text":"And here is how our repo structure looks: \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 include \u2502 \u2514\u2500\u2500 parser \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u2514\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u251c\u2500\u2500 parse.c \u251c\u2500\u2500 parsetree.c \u2514\u2500\u2500 scan.l","title":"Project Structure"},{"location":"02-ast/putting-it-together/#running-the-program","text":"$ make *** There will be several warnings, but you can safely ignore them *** make[1]: Leaving directory '***' $ ./burkeql bql > select SELECT command received bql > insert INSERT command received bql > \\q ====== Node ====== = Type: SysCmd = Cmd: q bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ Our parser still recognizes the select and insert tokens, but we don't do anything with them. It also recognizes a \\ as a system command flag, but \\q isn't a valid system command so we don't do anything with it. But when we type \\quit , our code recognizes the system command flag AND knows to actually quit the program.","title":"Running the Program"},{"location":"03-db-page/page-structure/","text":"The Database Page In this section I am going to introduce and discuss the concept of a database \"page\". I won't be writing any code, so if that's all you're interested in, I suggest you skip to the next section. TODO: Add more diagrams, especially for the Record Structure section. Data File Databases store data in files on disk. They aren't doing any magic voodoo behind the scenes (although they're so clever it kind of seems like magic), it's just reading and writing data to regular files. Some of the magic comes in to play with how the storage engine organizes data in those files. Not all RDBMS platforms implement storage in the same way. Some (e.g. postgres) use hundreds of different files - you create a new table, you get a new set of files just for that table. Others (e.g. Microsoft SQL Server) use far fewer files - in its default setup, you get two files. One for the data and one for the transaction log. The system we're building is going to be similar to MS SQL Server - one file for the data, and one for the transaction log.. if I ever get around to implementing it. Moreover, the structure of our data pages will not align with any existing RDBMS out there. Instead I'm using a combination of MS SQL Server and Postgres - taking the things I like about both and ignoring what I don't like. Data Page Within the data files, the storage engine organizes the data into uniformly-sized chunks - typically 8kB. The is the most basic unit that database systems work with. If you ask the DB to return a single row from a table, it first has to read an entire 8kB page into memory, then it can scan the page for the record you requested. So we have 8kB pages, how are they structured? Page Header You can think of a data page as being composed of three sections. The first is the page header, which contains metadata about itself. The size and type of information stored in a page header differ from system to system. In our system, we will be working with pages that have 20-byte headers. The header fields are as follows: Header Field Size Description pageId 4-bytes One-based page identifier. Used to mark the byte position of the page in the file. pageType 1-byte Identifies the type of page. 0 = data page, 1 = index page indexLevel 1-byte Level of the page in an index. 0 = leaf level (or heap page) prevPageId 4-bytes Points to the previous page at the same indexLevel . If there is no previous page, then its value is 0. nextPageId 4-bytes Points to the next page at the same indexLevel . If there is no next page, then its value is 0. numRecords 2-bytes Number of records stored on the page freeBytes 2-bytes Number of unclaimed bytes on the page freeData 2-bytes Number of continuous free bytes between the end of the last record and the beginning of the slot array pageId The pageId 's purpose is to tell the storage engine where it belongs within the data file. In a database with 8kB pages, we know that pageId=1 belongs at the beginning of the file. pageId=3 belongs at byte offset (3-1) * 8192 = 16,384 in the file. The first thing that probably popped out at you is the pageId being a one-based identifier instead of zero-based. That's because we need to reserve zero as \"nothing\" for the prevPageId and nextPageId header fields. The \"nothing\" indicating there is not prev or next page. pageType and indexLevel These fields will be important when we implement indexes. But it does serve an important purpose for the query engine. The data stored on index pages looks a lot different than the data stored on data pages, so the query engine needs to know which type of data it's dealing with. prevPageId and nextPageId Database pages exist as part of a doubly-linked list, and these two fields serve as the pointers in the list. Since we're using a single file for the data in our database, we have to store the data for multiple different tables in the same file. This means the data for tableA will not be a contiguous chunk of the file - it will have pages from tableB interspersed. These prev and next pointers help the query engine efficiently scan the file for the data it cares about. numRecords Exactly as the name suggests, it stores the number of records on the page. This field won't be very useful in the near term, but it will help us later on when we decide to store statistics about the tables in our database. freeBytes and freeData These two will often have the same value, but they serve two different roles. The former is used to determine if the storage engine can reorganize and compact the existing data records to make room for a new one. The latter is used to determine if a new record can fit on the page or not. In a typical transactional database, we have lots of inserts, updates, and deletes happening all the time. Let's say we have a data page that's completely full and a user wants to delete a record in the middle of the page. The storage engine will mark it as deleted, but it WILL NOT reorganize the existing records towards the beginning of the page. Instead, there will be a hole of empty space in the middle. Because the empty space is in the middle of the page, when a new insert comes along the storage engine will still consider this page full because the value stored in freeData is not large enough. However, freeBytes IS large enough, so the storage engine might decide now is a good time to compact the existing records on the page in order to make room at the end of the page for the new one. Slot Array Growing from the end of the page towards the beginning, each slot array item consumes 4-bytes and stores two pieces of information: A byte offset from the beginning of the page to the beginning of its corresponding data record The byte length of the data record The slot array is also responsible for maintaining the order of records stored on the page. This is not as significant in a heap as it is in an index; however, it's important to understand that the records themselves ARE NOT physically stored in any particular order. The slot array is what determines the sort order of data records on the page. The slot array grows from the end of the page towards the beginning. For each new record inserted on the page, the data itself is appended to the end of the last record on the page, and a new slot array item is prepended to the beginning of the slot array. This means in order for a page to have enough space for the new record, it must have enough empty space for the record itself AND space for the new slot array entry. Record Structure Data records are added to the page starting at the end of the header and growing towards the end of the page. Much like data pages, data records also have a fixed-length header that stores certain metadata about the record. The header fields we use in this project are most similar to postgres because the way we handle transaction isolation will be like postgres. In fact, we won't even use most of these fields for a while, but building them in right now so we don't have to do as much refactoring in the future. The record header is a fixed 12 bytes long with the following fields: Header Field Size Description xmin 4-bytes Transaction Id that inserted the record xmax 4-bytes Transaction Id that deleted the record infomask 2-bytes Bit fields containing meta-information about record, e.g. if the row has Nullable columns or variable-length columns. nullOffset 2-bytes The byte-offset from the beginning of the record to the start of the Null bitmap xmin , xmax , and some of the bit fields in infomask won't be used until we implement transaction isolation. Record Data Layout After the header, the columns in the record are organized in a particular manner. To demonstrate this, we'll use the following table definition as an example: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); This table has both fixed-length ( Int ) and variable-length ( Varchar ) columns, as well as nullable columns. After the record header, the storage engine stores data in the following order: Fixed-length columns Null bitmap (iff the table has Nullable columns) Variable-length columns (if any) So if we inserted data like this: Insert Into person ( person_id, first_name, last_name, age ) Values (1, Null, 'burke', 30); At a high level, the engine would store the data on disk like so: | <record_header> | 1 | 30 | <null_bitmap> | burke | Notice the two fixed-length columns come first, sorted in the order they were defined in the Create Table statement, then the Null bitmap, and finally the variable-length columns. However, since first_name is Null, the storage engine does not need to store anything for it in the variable-length portion of the record because the Null bitmap will let the engine know that it's Null. Variable-Length Offsets The last thing we need to cover is the storage overhead associated with Varchar columns. Each non-null Varchar column has a corresponding 2-byte field storing the length of the data. These 2-byte fields are stored immediately before the Varchar value. If we zoom in on the data shown above: | ... | <null_bitmap> | 7-burke | It's a little wonky to represent it like this. But, we store 7 to represent the entire length of the Varchar column: 2-bytes for the overhead + 5-bytes for the data (1-byte per character).","title":"Page Structure"},{"location":"03-db-page/page-structure/#the-database-page","text":"In this section I am going to introduce and discuss the concept of a database \"page\". I won't be writing any code, so if that's all you're interested in, I suggest you skip to the next section. TODO: Add more diagrams, especially for the Record Structure section.","title":"The Database Page"},{"location":"03-db-page/page-structure/#data-file","text":"Databases store data in files on disk. They aren't doing any magic voodoo behind the scenes (although they're so clever it kind of seems like magic), it's just reading and writing data to regular files. Some of the magic comes in to play with how the storage engine organizes data in those files. Not all RDBMS platforms implement storage in the same way. Some (e.g. postgres) use hundreds of different files - you create a new table, you get a new set of files just for that table. Others (e.g. Microsoft SQL Server) use far fewer files - in its default setup, you get two files. One for the data and one for the transaction log. The system we're building is going to be similar to MS SQL Server - one file for the data, and one for the transaction log.. if I ever get around to implementing it. Moreover, the structure of our data pages will not align with any existing RDBMS out there. Instead I'm using a combination of MS SQL Server and Postgres - taking the things I like about both and ignoring what I don't like.","title":"Data File"},{"location":"03-db-page/page-structure/#data-page","text":"Within the data files, the storage engine organizes the data into uniformly-sized chunks - typically 8kB. The is the most basic unit that database systems work with. If you ask the DB to return a single row from a table, it first has to read an entire 8kB page into memory, then it can scan the page for the record you requested. So we have 8kB pages, how are they structured?","title":"Data Page"},{"location":"03-db-page/page-structure/#page-header","text":"You can think of a data page as being composed of three sections. The first is the page header, which contains metadata about itself. The size and type of information stored in a page header differ from system to system. In our system, we will be working with pages that have 20-byte headers. The header fields are as follows: Header Field Size Description pageId 4-bytes One-based page identifier. Used to mark the byte position of the page in the file. pageType 1-byte Identifies the type of page. 0 = data page, 1 = index page indexLevel 1-byte Level of the page in an index. 0 = leaf level (or heap page) prevPageId 4-bytes Points to the previous page at the same indexLevel . If there is no previous page, then its value is 0. nextPageId 4-bytes Points to the next page at the same indexLevel . If there is no next page, then its value is 0. numRecords 2-bytes Number of records stored on the page freeBytes 2-bytes Number of unclaimed bytes on the page freeData 2-bytes Number of continuous free bytes between the end of the last record and the beginning of the slot array pageId The pageId 's purpose is to tell the storage engine where it belongs within the data file. In a database with 8kB pages, we know that pageId=1 belongs at the beginning of the file. pageId=3 belongs at byte offset (3-1) * 8192 = 16,384 in the file. The first thing that probably popped out at you is the pageId being a one-based identifier instead of zero-based. That's because we need to reserve zero as \"nothing\" for the prevPageId and nextPageId header fields. The \"nothing\" indicating there is not prev or next page. pageType and indexLevel These fields will be important when we implement indexes. But it does serve an important purpose for the query engine. The data stored on index pages looks a lot different than the data stored on data pages, so the query engine needs to know which type of data it's dealing with. prevPageId and nextPageId Database pages exist as part of a doubly-linked list, and these two fields serve as the pointers in the list. Since we're using a single file for the data in our database, we have to store the data for multiple different tables in the same file. This means the data for tableA will not be a contiguous chunk of the file - it will have pages from tableB interspersed. These prev and next pointers help the query engine efficiently scan the file for the data it cares about. numRecords Exactly as the name suggests, it stores the number of records on the page. This field won't be very useful in the near term, but it will help us later on when we decide to store statistics about the tables in our database. freeBytes and freeData These two will often have the same value, but they serve two different roles. The former is used to determine if the storage engine can reorganize and compact the existing data records to make room for a new one. The latter is used to determine if a new record can fit on the page or not. In a typical transactional database, we have lots of inserts, updates, and deletes happening all the time. Let's say we have a data page that's completely full and a user wants to delete a record in the middle of the page. The storage engine will mark it as deleted, but it WILL NOT reorganize the existing records towards the beginning of the page. Instead, there will be a hole of empty space in the middle. Because the empty space is in the middle of the page, when a new insert comes along the storage engine will still consider this page full because the value stored in freeData is not large enough. However, freeBytes IS large enough, so the storage engine might decide now is a good time to compact the existing records on the page in order to make room at the end of the page for the new one.","title":"Page Header"},{"location":"03-db-page/page-structure/#slot-array","text":"Growing from the end of the page towards the beginning, each slot array item consumes 4-bytes and stores two pieces of information: A byte offset from the beginning of the page to the beginning of its corresponding data record The byte length of the data record The slot array is also responsible for maintaining the order of records stored on the page. This is not as significant in a heap as it is in an index; however, it's important to understand that the records themselves ARE NOT physically stored in any particular order. The slot array is what determines the sort order of data records on the page. The slot array grows from the end of the page towards the beginning. For each new record inserted on the page, the data itself is appended to the end of the last record on the page, and a new slot array item is prepended to the beginning of the slot array. This means in order for a page to have enough space for the new record, it must have enough empty space for the record itself AND space for the new slot array entry.","title":"Slot Array"},{"location":"03-db-page/page-structure/#record-structure","text":"Data records are added to the page starting at the end of the header and growing towards the end of the page. Much like data pages, data records also have a fixed-length header that stores certain metadata about the record. The header fields we use in this project are most similar to postgres because the way we handle transaction isolation will be like postgres. In fact, we won't even use most of these fields for a while, but building them in right now so we don't have to do as much refactoring in the future. The record header is a fixed 12 bytes long with the following fields: Header Field Size Description xmin 4-bytes Transaction Id that inserted the record xmax 4-bytes Transaction Id that deleted the record infomask 2-bytes Bit fields containing meta-information about record, e.g. if the row has Nullable columns or variable-length columns. nullOffset 2-bytes The byte-offset from the beginning of the record to the start of the Null bitmap xmin , xmax , and some of the bit fields in infomask won't be used until we implement transaction isolation.","title":"Record Structure"},{"location":"03-db-page/page-structure/#record-data-layout","text":"After the header, the columns in the record are organized in a particular manner. To demonstrate this, we'll use the following table definition as an example: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); This table has both fixed-length ( Int ) and variable-length ( Varchar ) columns, as well as nullable columns. After the record header, the storage engine stores data in the following order: Fixed-length columns Null bitmap (iff the table has Nullable columns) Variable-length columns (if any) So if we inserted data like this: Insert Into person ( person_id, first_name, last_name, age ) Values (1, Null, 'burke', 30); At a high level, the engine would store the data on disk like so: | <record_header> | 1 | 30 | <null_bitmap> | burke | Notice the two fixed-length columns come first, sorted in the order they were defined in the Create Table statement, then the Null bitmap, and finally the variable-length columns. However, since first_name is Null, the storage engine does not need to store anything for it in the variable-length portion of the record because the Null bitmap will let the engine know that it's Null.","title":"Record Data Layout"},{"location":"03-db-page/page-structure/#variable-length-offsets","text":"The last thing we need to cover is the storage overhead associated with Varchar columns. Each non-null Varchar column has a corresponding 2-byte field storing the length of the data. These 2-byte fields are stored immediately before the Varchar value. If we zoom in on the data shown above: | ... | <null_bitmap> | 7-burke | It's a little wonky to represent it like this. But, we store 7 to represent the entire length of the Varchar column: 2-bytes for the overhead + 5-bytes for the data (1-byte per character).","title":"Variable-Length Offsets"},{"location":"04-data-persistence/01-intro/","text":"Data Persistence In this section we are finally going to start building a legit database. We will implement the logic for writing data to a table and persisting that data to disk. As part of this effort we will begin building a bona-fide storage engine, which needs to be able to read and write files to disk, as well as pull individual data pages into memory. For the data file interface, we're not going to get too crazy. We just need a mechanism that checks a specific directory for a data file and will create the file if it doesn't exist. Beyond that, we just need to write a couple simple functions that are able to pull specific data pages from disk into memory, as well as write data pages from memory to disk. The interface for data pages is comparatively more complex. We will need methods for updating each of the header fields, as well as logic to make sure those fields are synchronized with any changes to the data on the page. Moreover, we need methods that can construct a data record from the user-provided input data, and be able to maintain the slot array when inserting data. But before we do all that fun stuff, we have a boring task to take care of first: a configuration file. There are a few settings I'll want to be able to easily change down the road, and a config file seems to be the easiest way to do it. Buckle up, this section is going to be a long one.","title":"Intro"},{"location":"04-data-persistence/01-intro/#data-persistence","text":"In this section we are finally going to start building a legit database. We will implement the logic for writing data to a table and persisting that data to disk. As part of this effort we will begin building a bona-fide storage engine, which needs to be able to read and write files to disk, as well as pull individual data pages into memory. For the data file interface, we're not going to get too crazy. We just need a mechanism that checks a specific directory for a data file and will create the file if it doesn't exist. Beyond that, we just need to write a couple simple functions that are able to pull specific data pages from disk into memory, as well as write data pages from memory to disk. The interface for data pages is comparatively more complex. We will need methods for updating each of the header fields, as well as logic to make sure those fields are synchronized with any changes to the data on the page. Moreover, we need methods that can construct a data record from the user-provided input data, and be able to maintain the slot array when inserting data. But before we do all that fun stuff, we have a boring task to take care of first: a configuration file. There are a few settings I'll want to be able to easily change down the road, and a config file seems to be the easiest way to do it. Buckle up, this section is going to be a long one.","title":"Data Persistence"},{"location":"04-data-persistence/02-config-file/","text":"Config File There are a handful of configuration settings I'll want to play with in the future, and putting them in a config file is much easier than stashing them in some buried header file. For now, our config file is only going to have one parameter: DATA_FILE . It stores the absolute path to the database's data file. Here is what the config file looks like: burkeql.conf # Config file for the BurkeQL database # Absolute location of the data file DATA_FILE=/path/to/data/file Make sure to set this path to whatever makes sense on your machine. For me, I plan to have all database files in a folder named db_files at the root of this repository. I want to keep them local to my code because we're going to be inspecting the binary contents of the file and having them local just makes it easier. For example, my config file looks like this: DATA_FILE=/home/burke/source_control/burkeql-db/db_files/main.dbd Note the .dbd file extension. This stands for \"database data.\" In the future, we'll introduce a transaction log file that has the extension .dbl : \"database log.\" You can use whatever extension you like, or no extension at all - it doesn't matter. Make sure you name the config file burkeql.conf and put it at the root level of your repository. For reference, this is what my repo structure looks like: \u251c\u2500\u2500 burkeql.conf <--- Config file at root level of the repo \u251c\u2500\u2500 db_files \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 parser \u2502 \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u251c\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u251c\u2500\u2500 parse.c \u251c\u2500\u2500 parsetree.c \u2514\u2500\u2500 scan.l","title":"Config File"},{"location":"04-data-persistence/02-config-file/#config-file","text":"There are a handful of configuration settings I'll want to play with in the future, and putting them in a config file is much easier than stashing them in some buried header file. For now, our config file is only going to have one parameter: DATA_FILE . It stores the absolute path to the database's data file. Here is what the config file looks like: burkeql.conf # Config file for the BurkeQL database # Absolute location of the data file DATA_FILE=/path/to/data/file Make sure to set this path to whatever makes sense on your machine. For me, I plan to have all database files in a folder named db_files at the root of this repository. I want to keep them local to my code because we're going to be inspecting the binary contents of the file and having them local just makes it easier. For example, my config file looks like this: DATA_FILE=/home/burke/source_control/burkeql-db/db_files/main.dbd Note the .dbd file extension. This stands for \"database data.\" In the future, we'll introduce a transaction log file that has the extension .dbl : \"database log.\" You can use whatever extension you like, or no extension at all - it doesn't matter. Make sure you name the config file burkeql.conf and put it at the root level of your repository. For reference, this is what my repo structure looks like: \u251c\u2500\u2500 burkeql.conf <--- Config file at root level of the repo \u251c\u2500\u2500 db_files \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 parser \u2502 \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u251c\u2500\u2500 parser \u251c\u2500\u2500 gram.y \u251c\u2500\u2500 parse.c \u251c\u2500\u2500 parsetree.c \u2514\u2500\u2500 scan.l","title":"Config File"},{"location":"04-data-persistence/03-loading-config/","text":"Loading Config With the config file format defined, we need to write the code that reads the config file line-by-line and sets the valid values in a global Config object. Let's begin by writing the header file for our config code: typedef enum ConfigParameter { CONF_DATA_FILE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; } Config; The ConfigParameter enum is going to be the list of valid config options to put in our burkeql.conf file. This list will grow as we add more to the database system, but for now all we need is the data file location and a value for an unrecognized parameter. The Config struct is going to be a global object initialized at the beginning of the main function, and populated shortly thereafter. Except for CONF_UNRECOGNIZED , the properties in the Config struct will correspond one-to-one with the enum values. Next, we need to define functions we want to expose to the rest of our program. Config* new_config(); void free_config(Config* conf); bool set_global_config(Config* conf); void print_config(Config* conf); The first two are just responsible for allocating and freeing memory associated with the Config object. set_global_config is the workhorse. It will be called towards the beginning of main() and is responsible for opening the config file, reading its contents, and setting the provided values in the Config object. Returns true if it succeeds, and false otherwise. print_config is just there for debugging. After we set the config values, we'll want to print them to the terminal just to make sure everything went as expected. Implementation First up, let's start with basic opening and closing of files. static FILE* read_config_file() { FILE* fp = fopen(\"burkeql.conf\", \"r\"); return fp; } static void close_config_file(FILE* fp) { fclose(fp); } Next up, we need memory management functions and a debugging function that prints info to the console, new_config , free_config , and print_config : Config* new_config() { Config* conf = malloc(sizeof(Config)); return conf; } void free_config(Config* conf) { if (conf->dataFile != NULL) free(conf->dataFile); free(conf); } void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); } Very straightforward. Allocate the memory we need. Free everything that was allocated. And print the only config property we currently have. Now we can write our primary worker function, which has a lot more going on by comparison, and even utilizes some static helper functions: bool set_global_config(Config* conf) { FILE* fp = read_config_file(); char* line = NULL; size_t len = 0; ssize_t read; ConfigParameter p; if (fp == NULL) { printf(\"Unable to read config file: burkeql.conf\\n\"); return false; } We start by declaring/initializing the variables we'll need when we loop through the config file. Then we have a simple error check that bails out early if there's an issue. /** * loops through the config file and sets values in the * global Config object */ while ((read = getline(&line, &len, fp)) != -1) { // skip comment lines or empty lines if (strncmp(line, \"#\", 1) == 0 || read <= 1) continue; // parse out the key and value char* param = strtok(line, \"=\"); char* value = strtok(NULL, \"=\"); p = parse_config_param(param); if (p == CONF_UNRECOGNIZED) continue; set_config_value(conf, p, value); } This is the meat of our set_global_config function. We use a fairly common loop condition to read through a file one line at a time until it hits an EOF . We then check if the line begins with a # character, indicating a comment line, or if the length of the line is 0, indicating an empty line. If either of those conditions is met, then we skip to the next line. Next, we use the strtok function to split the parameter name and its value into separate variables. Since we know they are separated by an \"=\" symbol, we pass that as the second argument to the function. Then we use one of our helper functions to match the value of param to one of the enum values in ConfigParameter . If it's a valid parameter, we call our second helper function to set the value in our global object. Finally, we free up any memory we allocated for this function: if (line) free(line); close_config_file(fp); return true; } parse_config_param is a really simple function. Just a bunch of if(strcmp(... statements to determine the correct enum value: static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; return CONF_UNRECOGNIZED; } set_config_value follows a similar pattern, except we run a switch statement on the enum value returned by the above function: static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; } } Updating main() The change to main.c is pretty simple. We just need to include the new config.h header and write the code to load config values into the global Config object. #include \"parser/parsetree.h\" #include \"parser/parse.h\" +#include \"global/config.h\" + +Config* conf; static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { + // initialize global config + conf = new_config(); + + if (!set_global_config(conf)) { + return EXIT_FAILURE; + } + + // print config + print_config(conf); while(true) { print_prompt(); Makefile Lastly, we need to update the Makefile by adding the new c files to the SRC_FILES variable. SRC_FILES = main.c \\ parser/parse.c \\ + parser/parsetree.c \\ + global/config.c Running the Program Before you run the program, make sure you create a burkeql.conf file in whatever directory you'll execute the program from. I do everything at the root level of the code repository to keep it simple. My config file looks like this: burkeql.conf # Config file for the BurkeQL database # Absolute location of the data file DATA_FILE=/home/burke/source_control/burkeql-db/db_files/main.dbd Now we can compile and run the program to see if our config file is properly parsed. Remember, you can safely ignore warnings about functions produced by flex and bison (e.g. yylex , yyerror , etc.). $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ As you can see, our code correctly reads and parses the config file and stores the value in the global config object. Keep in mind the parser we wrote for the config file is extremely basic. That means it's easy to break it. However, we do kill the program if there's an issue parsing the config file, so it shouldn't be too dangerous to leave it as is. Full Files Here's the folder layout I have at this point: \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 burkeql.conf \u251c\u2500\u2500 db_files <-- Not necessary yet, but will be in the next section \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 global \u2502 \u2514\u2500\u2500 config.c \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 global \u2502 \u2502 \u2514\u2500\u2500 config.h \u2502 \u251c\u2500\u2500 parser \u2502 \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u251c\u2500\u2500 parser \u2502 \u251c\u2500\u2500 gram.y \u2502 \u251c\u2500\u2500 parse.c \u2502 \u251c\u2500\u2500 parsetree.c \u2502 \u2514\u2500\u2500 scan.l And the current state of all the files we changed. src/Makefile CC = gcc LEX = flex YACC = bison CFLAGS = -I./ -I./include -fsanitize=address -fsanitize=undefined -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c rm -f $(wildcard *.lex.*) rm -f $(BUILD_DIR)/$(TARGET_EXEC) src/include/global/config.h #ifndef CONFIG_H #define CONFIG_H #include <stdbool.h> typedef enum ConfigParameter { CONF_DATA_FILE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; } Config; Config* new_config(); void free_config(Config* conf); bool set_global_config(Config* conf); void print_config(Config* conf); #endif /* CONFIG_H */ src/global/config.c #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <string.h> #include \"global/config.h\" Config* new_config() { Config* conf = malloc(sizeof(Config)); return conf; } void free_config(Config* conf) { if (conf->dataFile != NULL) free(conf->dataFile); free(conf); } void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); } static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; return CONF_UNRECOGNIZED; } static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; } } static FILE* read_config_file() { FILE* fp = fopen(\"burkeql.conf\", \"r\"); return fp; } static void close_config_file(FILE* fp) { fclose(fp); } bool set_global_config(Config* conf) { FILE* fp = read_config_file(); char* line = NULL; size_t len = 0; ssize_t read; ConfigParameter p; if (fp == NULL) { printf(\"Unable to read config file: burkeql.conf\\n\"); return false; } /** * loops through the config file and sets values in the * global Config object */ while ((read = getline(&line, &len, fp)) != -1) { // skip comment lines or empty lines if (strncmp(line, \"#\", 1) == 0 || read <= 1) continue; // parse out the key and value char* param = strtok(line, \"=\"); char* value = strtok(NULL, \"=\"); p = parse_config_param(param); if (p == CONF_UNRECOGNIZED) continue; set_config_value(conf, p, value); } if (line) free(line); close_config_file(fp); return true; } src/main.c #include <stdio.h> #include <stdlib.h> #include <stdbool.h> #include <string.h> #include \"gram.tab.h\" #include \"parser/parsetree.h\" #include \"parser/parse.h\" #include \"global/config.h\" Config* conf; static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { // initialize global config conf = new_config(); if (!set_global_config(conf)) { return EXIT_FAILURE; } // print config print_config(conf); while(true) { print_prompt(); Node* n = parse_sql(); if (n == NULL) continue; switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { print_node(n); free_node(n); printf(\"Shutting down...\\n\"); return EXIT_SUCCESS; } default: print_node(n); free_node(n); } } return EXIT_SUCCESS; }","title":"Loading Config"},{"location":"04-data-persistence/03-loading-config/#loading-config","text":"With the config file format defined, we need to write the code that reads the config file line-by-line and sets the valid values in a global Config object. Let's begin by writing the header file for our config code: typedef enum ConfigParameter { CONF_DATA_FILE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; } Config; The ConfigParameter enum is going to be the list of valid config options to put in our burkeql.conf file. This list will grow as we add more to the database system, but for now all we need is the data file location and a value for an unrecognized parameter. The Config struct is going to be a global object initialized at the beginning of the main function, and populated shortly thereafter. Except for CONF_UNRECOGNIZED , the properties in the Config struct will correspond one-to-one with the enum values. Next, we need to define functions we want to expose to the rest of our program. Config* new_config(); void free_config(Config* conf); bool set_global_config(Config* conf); void print_config(Config* conf); The first two are just responsible for allocating and freeing memory associated with the Config object. set_global_config is the workhorse. It will be called towards the beginning of main() and is responsible for opening the config file, reading its contents, and setting the provided values in the Config object. Returns true if it succeeds, and false otherwise. print_config is just there for debugging. After we set the config values, we'll want to print them to the terminal just to make sure everything went as expected.","title":"Loading Config"},{"location":"04-data-persistence/03-loading-config/#implementation","text":"First up, let's start with basic opening and closing of files. static FILE* read_config_file() { FILE* fp = fopen(\"burkeql.conf\", \"r\"); return fp; } static void close_config_file(FILE* fp) { fclose(fp); } Next up, we need memory management functions and a debugging function that prints info to the console, new_config , free_config , and print_config : Config* new_config() { Config* conf = malloc(sizeof(Config)); return conf; } void free_config(Config* conf) { if (conf->dataFile != NULL) free(conf->dataFile); free(conf); } void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); } Very straightforward. Allocate the memory we need. Free everything that was allocated. And print the only config property we currently have. Now we can write our primary worker function, which has a lot more going on by comparison, and even utilizes some static helper functions: bool set_global_config(Config* conf) { FILE* fp = read_config_file(); char* line = NULL; size_t len = 0; ssize_t read; ConfigParameter p; if (fp == NULL) { printf(\"Unable to read config file: burkeql.conf\\n\"); return false; } We start by declaring/initializing the variables we'll need when we loop through the config file. Then we have a simple error check that bails out early if there's an issue. /** * loops through the config file and sets values in the * global Config object */ while ((read = getline(&line, &len, fp)) != -1) { // skip comment lines or empty lines if (strncmp(line, \"#\", 1) == 0 || read <= 1) continue; // parse out the key and value char* param = strtok(line, \"=\"); char* value = strtok(NULL, \"=\"); p = parse_config_param(param); if (p == CONF_UNRECOGNIZED) continue; set_config_value(conf, p, value); } This is the meat of our set_global_config function. We use a fairly common loop condition to read through a file one line at a time until it hits an EOF . We then check if the line begins with a # character, indicating a comment line, or if the length of the line is 0, indicating an empty line. If either of those conditions is met, then we skip to the next line. Next, we use the strtok function to split the parameter name and its value into separate variables. Since we know they are separated by an \"=\" symbol, we pass that as the second argument to the function. Then we use one of our helper functions to match the value of param to one of the enum values in ConfigParameter . If it's a valid parameter, we call our second helper function to set the value in our global object. Finally, we free up any memory we allocated for this function: if (line) free(line); close_config_file(fp); return true; } parse_config_param is a really simple function. Just a bunch of if(strcmp(... statements to determine the correct enum value: static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; return CONF_UNRECOGNIZED; } set_config_value follows a similar pattern, except we run a switch statement on the enum value returned by the above function: static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; } }","title":"Implementation"},{"location":"04-data-persistence/03-loading-config/#updating-main","text":"The change to main.c is pretty simple. We just need to include the new config.h header and write the code to load config values into the global Config object. #include \"parser/parsetree.h\" #include \"parser/parse.h\" +#include \"global/config.h\" + +Config* conf; static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { + // initialize global config + conf = new_config(); + + if (!set_global_config(conf)) { + return EXIT_FAILURE; + } + + // print config + print_config(conf); while(true) { print_prompt();","title":"Updating main()"},{"location":"04-data-persistence/03-loading-config/#makefile","text":"Lastly, we need to update the Makefile by adding the new c files to the SRC_FILES variable. SRC_FILES = main.c \\ parser/parse.c \\ + parser/parsetree.c \\ + global/config.c","title":"Makefile"},{"location":"04-data-persistence/03-loading-config/#running-the-program","text":"Before you run the program, make sure you create a burkeql.conf file in whatever directory you'll execute the program from. I do everything at the root level of the code repository to keep it simple. My config file looks like this: burkeql.conf # Config file for the BurkeQL database # Absolute location of the data file DATA_FILE=/home/burke/source_control/burkeql-db/db_files/main.dbd Now we can compile and run the program to see if our config file is properly parsed. Remember, you can safely ignore warnings about functions produced by flex and bison (e.g. yylex , yyerror , etc.). $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ As you can see, our code correctly reads and parses the config file and stores the value in the global config object. Keep in mind the parser we wrote for the config file is extremely basic. That means it's easy to break it. However, we do kill the program if there's an issue parsing the config file, so it shouldn't be too dangerous to leave it as is.","title":"Running the Program"},{"location":"04-data-persistence/03-loading-config/#full-files","text":"Here's the folder layout I have at this point: \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 burkeql.conf \u251c\u2500\u2500 db_files <-- Not necessary yet, but will be in the next section \u2514\u2500\u2500 src \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 global \u2502 \u2514\u2500\u2500 config.c \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 global \u2502 \u2502 \u2514\u2500\u2500 config.h \u2502 \u251c\u2500\u2500 parser \u2502 \u2502 \u251c\u2500\u2500 parse.h \u2502 \u2502 \u2514\u2500\u2500 parsetree.h \u251c\u2500\u2500 main.c \u251c\u2500\u2500 parser \u2502 \u251c\u2500\u2500 gram.y \u2502 \u251c\u2500\u2500 parse.c \u2502 \u251c\u2500\u2500 parsetree.c \u2502 \u2514\u2500\u2500 scan.l And the current state of all the files we changed. src/Makefile CC = gcc LEX = flex YACC = bison CFLAGS = -I./ -I./include -fsanitize=address -fsanitize=undefined -static-libasan -g TARGET_EXEC = burkeql BUILD_DIR = .. SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c $(BUILD_DIR)/$(TARGET_EXEC): gram.tab.o lex.yy.o ${SRC_FILES} ${CC} ${CFLAGS} -o $@ $? gram.tab.c gram.tab.h: parser/gram.y ${YACC} -vd $? lex.yy.c: parser/scan.l ${LEX} -o $*.c $< lex.yy.o: gram.tab.h lex.yy.c clean: rm -f $(wildcard *.o) rm -f $(wildcard *.output) rm -f $(wildcard *.tab.*) rm -f lex.yy.c rm -f $(wildcard *.lex.*) rm -f $(BUILD_DIR)/$(TARGET_EXEC) src/include/global/config.h #ifndef CONFIG_H #define CONFIG_H #include <stdbool.h> typedef enum ConfigParameter { CONF_DATA_FILE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; } Config; Config* new_config(); void free_config(Config* conf); bool set_global_config(Config* conf); void print_config(Config* conf); #endif /* CONFIG_H */ src/global/config.c #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <string.h> #include \"global/config.h\" Config* new_config() { Config* conf = malloc(sizeof(Config)); return conf; } void free_config(Config* conf) { if (conf->dataFile != NULL) free(conf->dataFile); free(conf); } void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); } static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; return CONF_UNRECOGNIZED; } static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; } } static FILE* read_config_file() { FILE* fp = fopen(\"burkeql.conf\", \"r\"); return fp; } static void close_config_file(FILE* fp) { fclose(fp); } bool set_global_config(Config* conf) { FILE* fp = read_config_file(); char* line = NULL; size_t len = 0; ssize_t read; ConfigParameter p; if (fp == NULL) { printf(\"Unable to read config file: burkeql.conf\\n\"); return false; } /** * loops through the config file and sets values in the * global Config object */ while ((read = getline(&line, &len, fp)) != -1) { // skip comment lines or empty lines if (strncmp(line, \"#\", 1) == 0 || read <= 1) continue; // parse out the key and value char* param = strtok(line, \"=\"); char* value = strtok(NULL, \"=\"); p = parse_config_param(param); if (p == CONF_UNRECOGNIZED) continue; set_config_value(conf, p, value); } if (line) free(line); close_config_file(fp); return true; } src/main.c #include <stdio.h> #include <stdlib.h> #include <stdbool.h> #include <string.h> #include \"gram.tab.h\" #include \"parser/parsetree.h\" #include \"parser/parse.h\" #include \"global/config.h\" Config* conf; static void print_prompt() { printf(\"bql > \"); } int main(int argc, char** argv) { // initialize global config conf = new_config(); if (!set_global_config(conf)) { return EXIT_FAILURE; } // print config print_config(conf); while(true) { print_prompt(); Node* n = parse_sql(); if (n == NULL) continue; switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { print_node(n); free_node(n); printf(\"Shutting down...\\n\"); return EXIT_SUCCESS; } default: print_node(n); free_node(n); } } return EXIT_SUCCESS; }","title":"Full Files"},{"location":"04-data-persistence/04-file-interface/","text":"File Interface The file interface is going to be really simple. All we need to do is create a struct that holds a file descriptor and two functions to open and close files. Let's start by writing the header file: src/include/storage/file.h typedef struct FileDesc { char* filename; int fd; } FileDesc; FileDesc* file_open(char* filename); void file_close(FileDesc* fdesc); The FileDesc struct (short for FileDescriptor) keeps track of the file's name and an int representing the file descriptor. The file_open function is responsible for allocating memory for the FileDesc object and opening the provided file. And the file_close function simply closes the open file and frees the memory used by FileDesc . Opening and Closing Files We'll be using the open and close system calls to interact with any files our program needs. To open a file, we first allocate memory for our file descriptor struct, then attempt to open the file. If open returns -1 , then we free the file descriptor and return a NULL . If open succeeds, we set the filename property and return the file descriptor object. FileDesc* file_open(char* filename) { FileDesc* fdesc = malloc(sizeof(FileDesc)); fdesc->fd = open(filename, O_RDWR | // read/write mode O_CREAT, // create file if it doesn't exist S_IWUSR | // user write permission S_IRUSR // user read permission ); if (fdesc->fd == -1) { free(fdesc); return NULL; } fdesc->filename = strdup(filename); return fdesc; } Closing a file is just as straightforward. The only extra thing we need to do is free the memory allocated for the file descriptor and its elements. void file_close(FileDesc* fdesc) { if (fdesc->fd != -1) { close(fdesc->fd); } if (fdesc->filename != NULL) free(fdesc->filename); free(fdesc); } That's it. We'll be using these two functions to read in the config settings, as well as reading from the actual data file. Full Files src/include/storage/file.h #ifndef FILE_H #define FILE_H #include <stdint.h> #include <stdio.h> typedef struct FileDesc { char* filename; int fd; } FileDesc; FileDesc* file_open(char* filename); void file_close(FileDesc* fdesc); #endif /* FILE_H */ src/storage/file.c #include <string.h> #include <stdlib.h> #include <fcntl.h> #include <sys/stat.h> #include <stdio.h> #include <unistd.h> #include \"storage/file.h\" FileDesc* file_open(char* filename) { FileDesc* fdesc = malloc(sizeof(FileDesc)); fdesc->fd = open(filename, O_RDWR | // read/write mode O_CREAT, // create file if it doesn't exist S_IWUSR | // user write permission S_IRUSR // user read permission ); if (fdesc->fd == -1) { free(fdesc); return NULL; } fdesc->filename = strdup(filename); return fdesc; } void file_close(FileDesc* fdesc) { if (fdesc->fd != -1) { close(fdesc->fd); } if (fdesc->filename != NULL) free(fdesc->filename); free(fdesc); }","title":"DB File Interface"},{"location":"04-data-persistence/04-file-interface/#file-interface","text":"The file interface is going to be really simple. All we need to do is create a struct that holds a file descriptor and two functions to open and close files. Let's start by writing the header file: src/include/storage/file.h typedef struct FileDesc { char* filename; int fd; } FileDesc; FileDesc* file_open(char* filename); void file_close(FileDesc* fdesc); The FileDesc struct (short for FileDescriptor) keeps track of the file's name and an int representing the file descriptor. The file_open function is responsible for allocating memory for the FileDesc object and opening the provided file. And the file_close function simply closes the open file and frees the memory used by FileDesc .","title":"File Interface"},{"location":"04-data-persistence/04-file-interface/#opening-and-closing-files","text":"We'll be using the open and close system calls to interact with any files our program needs. To open a file, we first allocate memory for our file descriptor struct, then attempt to open the file. If open returns -1 , then we free the file descriptor and return a NULL . If open succeeds, we set the filename property and return the file descriptor object. FileDesc* file_open(char* filename) { FileDesc* fdesc = malloc(sizeof(FileDesc)); fdesc->fd = open(filename, O_RDWR | // read/write mode O_CREAT, // create file if it doesn't exist S_IWUSR | // user write permission S_IRUSR // user read permission ); if (fdesc->fd == -1) { free(fdesc); return NULL; } fdesc->filename = strdup(filename); return fdesc; } Closing a file is just as straightforward. The only extra thing we need to do is free the memory allocated for the file descriptor and its elements. void file_close(FileDesc* fdesc) { if (fdesc->fd != -1) { close(fdesc->fd); } if (fdesc->filename != NULL) free(fdesc->filename); free(fdesc); } That's it. We'll be using these two functions to read in the config settings, as well as reading from the actual data file.","title":"Opening and Closing Files"},{"location":"04-data-persistence/04-file-interface/#full-files","text":"src/include/storage/file.h #ifndef FILE_H #define FILE_H #include <stdint.h> #include <stdio.h> typedef struct FileDesc { char* filename; int fd; } FileDesc; FileDesc* file_open(char* filename); void file_close(FileDesc* fdesc); #endif /* FILE_H */ src/storage/file.c #include <string.h> #include <stdlib.h> #include <fcntl.h> #include <sys/stat.h> #include <stdio.h> #include <unistd.h> #include \"storage/file.h\" FileDesc* file_open(char* filename) { FileDesc* fdesc = malloc(sizeof(FileDesc)); fdesc->fd = open(filename, O_RDWR | // read/write mode O_CREAT, // create file if it doesn't exist S_IWUSR | // user write permission S_IRUSR // user read permission ); if (fdesc->fd == -1) { free(fdesc); return NULL; } fdesc->filename = strdup(filename); return fdesc; } void file_close(FileDesc* fdesc) { if (fdesc->fd != -1) { close(fdesc->fd); } if (fdesc->filename != NULL) free(fdesc->filename); free(fdesc); }","title":"Full Files"},{"location":"04-data-persistence/05-page-interface/","text":"DB Page Interface We're FINALLY to the fun part. Well, at least I think this is the fun part. We get to start writing the code that serves as the backbone of the storage engine: the page interface. Using the concepts discussed in the Page Structure section, we're going to write the functionality that allocates a new page, maintains header fields, and reads/writes a page from/to disk. Page Header The first thing we want to do is define a Page type. Since a data page is just a fixed-size block of memory, we can use a char* as the base data type. This isn't super necessary, but it will be helpful as the codebase grows to define function parameters as a Page instead of the ambiguous char* . src/include/storage/page.h typedef char* Page; Remember from the concepts section, each data page has a fixed 20-byte header. We can model that with a struct as such: #pragma pack(push, 1) typedef struct PageHeader { uint32_t pageId; uint8_t pageType; uint8_t indexLevel; uint32_t prevPageId; uint32_t nextPageId; uint16_t numRecords; uint16_t freeBytes; uint16_t freeData; } PageHeader; The #pragma pack(push, 1) line tells the compiler not to align any of the struct properties to the machine word boundary. Disabling memory alignment comes at a performance penalty, but efficiency is not the goal of this project. Slot Array Next we want to define a struct to represent item pointers in the slot array. Slot pointers are just a pair of 2-byte integers containing the byte-offset from the beginning of the page to its corresponding record, and the byte-length of the record data. typedef SlotPointer { uint16_t offset; uint16_t length; } SlotPointer; API Now that we have our structs defined, we can start thinking about functionality we need to expose to the rest of the program. A useful way of doing this is thinking about our program's lifecycle and how it will interact with a data page. To start, let's narrow the scope to a database with a data file that contains only a single page. When the program starts, it will open the file and attempt to read the page into memory. If the file is empty, we create a brand new page. Then our program will flush that page to disk and close the file. Right now we aren't going to worry about storing data records. First up are a pair of functions responsible for reading and writing data pages to disk: Page read_page(int fd, uint32_t pageId); void flush_page(int fd, Page pg); It's a fairly simple task, the caller just needs to have the file pointer readily available. And remember we defined Page as a char* above, so we're just passing a pointer to the block of memory containing our data page. Next up, we need a way to allocate memory for the data page and a matching function that frees the memory when we're done with it. Page new_page(); void free_page(Page pg); And that's all we need for basic disk read/write actions. In the next section we'll write the code for each of these functions, as well as some utility functionality. Full File src/include/storage/page.h #ifndef PAGE_H #define PAGE_H #include <stdint.h> #include <stdbool.h> #include <stdio.h> typedef char* Page; #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct PageHeader { uint32_t pageId; uint8_t pageType; uint8_t indexLevel; uint32_t prevPageId; uint32_t nextPageId; uint16_t numRecords; uint16_t freeBytes; uint16_t freeData; } PageHeader; typedef struct SlotPointer { uint16_t offset; uint16_t length; } SlotPointer; Page new_page(); void free_page(Page pg); Page read_page(int fd, uint32_t pageId); void flush_page(int fd, Page pg); #endif /* PAGE_H */","title":"DB Page Interface"},{"location":"04-data-persistence/05-page-interface/#db-page-interface","text":"We're FINALLY to the fun part. Well, at least I think this is the fun part. We get to start writing the code that serves as the backbone of the storage engine: the page interface. Using the concepts discussed in the Page Structure section, we're going to write the functionality that allocates a new page, maintains header fields, and reads/writes a page from/to disk.","title":"DB Page Interface"},{"location":"04-data-persistence/05-page-interface/#page-header","text":"The first thing we want to do is define a Page type. Since a data page is just a fixed-size block of memory, we can use a char* as the base data type. This isn't super necessary, but it will be helpful as the codebase grows to define function parameters as a Page instead of the ambiguous char* . src/include/storage/page.h typedef char* Page; Remember from the concepts section, each data page has a fixed 20-byte header. We can model that with a struct as such: #pragma pack(push, 1) typedef struct PageHeader { uint32_t pageId; uint8_t pageType; uint8_t indexLevel; uint32_t prevPageId; uint32_t nextPageId; uint16_t numRecords; uint16_t freeBytes; uint16_t freeData; } PageHeader; The #pragma pack(push, 1) line tells the compiler not to align any of the struct properties to the machine word boundary. Disabling memory alignment comes at a performance penalty, but efficiency is not the goal of this project.","title":"Page Header"},{"location":"04-data-persistence/05-page-interface/#slot-array","text":"Next we want to define a struct to represent item pointers in the slot array. Slot pointers are just a pair of 2-byte integers containing the byte-offset from the beginning of the page to its corresponding record, and the byte-length of the record data. typedef SlotPointer { uint16_t offset; uint16_t length; } SlotPointer;","title":"Slot Array"},{"location":"04-data-persistence/05-page-interface/#api","text":"Now that we have our structs defined, we can start thinking about functionality we need to expose to the rest of the program. A useful way of doing this is thinking about our program's lifecycle and how it will interact with a data page. To start, let's narrow the scope to a database with a data file that contains only a single page. When the program starts, it will open the file and attempt to read the page into memory. If the file is empty, we create a brand new page. Then our program will flush that page to disk and close the file. Right now we aren't going to worry about storing data records. First up are a pair of functions responsible for reading and writing data pages to disk: Page read_page(int fd, uint32_t pageId); void flush_page(int fd, Page pg); It's a fairly simple task, the caller just needs to have the file pointer readily available. And remember we defined Page as a char* above, so we're just passing a pointer to the block of memory containing our data page. Next up, we need a way to allocate memory for the data page and a matching function that frees the memory when we're done with it. Page new_page(); void free_page(Page pg); And that's all we need for basic disk read/write actions. In the next section we'll write the code for each of these functions, as well as some utility functionality.","title":"API"},{"location":"04-data-persistence/05-page-interface/#full-file","text":"src/include/storage/page.h #ifndef PAGE_H #define PAGE_H #include <stdint.h> #include <stdbool.h> #include <stdio.h> typedef char* Page; #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct PageHeader { uint32_t pageId; uint8_t pageType; uint8_t indexLevel; uint32_t prevPageId; uint32_t nextPageId; uint16_t numRecords; uint16_t freeBytes; uint16_t freeData; } PageHeader; typedef struct SlotPointer { uint16_t offset; uint16_t length; } SlotPointer; Page new_page(); void free_page(Page pg); Page read_page(int fd, uint32_t pageId); void flush_page(int fd, Page pg); #endif /* PAGE_H */","title":"Full File"},{"location":"04-data-persistence/06-page-implementation/","text":"DB Page Implementation Before we write the code that implements our Page API, we need to add a new global config parameter that tells the DB engine how many bytes a data page consumes. I'm going to start with an absurdly small value: 128 bytes. Add this to the burkeql.conf file: # Byte size of a data page PAGE_SIZE=128 Next, we need to update the config.h header file to inform it of our new config parameter: typedef enum ConfigParameter { CONF_DATA_FILE, + CONF_PAGE_SIZE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; + int pageSize; } Config; And finally, we need to make the associated updates to our functions in config.c : void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); + printf(\"= PAGE_SIZE: %d\\n\", conf->pageSize); } static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; + if (strcmp(p, \"PAGE_SIZE\") == 0) return CONF_PAGE_SIZE; return CONF_UNRECOGNIZED; } static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; + case CONF_PAGE_SIZE: + conf->pageSize = atoi(v); + break; } } Page API Using the header file we wrote in the previous section, let's dive straight into the code. First, the easy ones: allocating and freeing the page block: extern Config* conf; Page new_page() { Page pg = malloc(conf->pageSize); memset(pg, 0, conf->pageSize); return pg; } void free_page(Page pg) { if (pg != NULL) free(pg); } It's about as straightforward as you can get. We simply allocate enough memory for a fixed-size data page and return the pointer. And we pair it with a function that frees the memory. Now let's read a page block from disk into a Page object: Page read_page(int fd, uint32_t pageId) { Page pg = new_page(); lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_read = read(fd, pg, conf->pageSize); if (bytes_read != conf->pageSize) { printf(\"Bytes read: %d\\n\", bytes_read); PageHeader* pgHdr = (PageHeader*)pg; /* Since this is a brand new page, we need to set the header fields appropriately */ pgHdr->pageId = pageId; pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); pgHdr->freeData = conf->pageSize - sizeof(PageHeader); } return pg; } Our function takes two inputs: a file descriptor and a pageId . This implies the caller will already have an open file, and it knows exactly which page block it wants to pull from disk. First, we need to allocate memory as a landing spot for the data we intend to pull from disk. Then we tell the file pointer to move to offset (pageId - 1) * conf->pageSize bytes from the beginning of the file ( SEEK_SET ). It's important to note that OUR pageId s are 1-based, but the math required to get the byte offset operates on 0-based pageIds. Next we read conf->pageSize bytes into our Page memory block. The read function returns the number of bytes read from disk. If the value is not equal to the size of a page block, then there was an error, or the page does not exist in the file. If the page doesn't exist, then we need to set the PageHeader fields to values that represent a blank data page. We're currently not using pageType , indexLevel , prevPageId , and nextPageId so we don't need to set any values. As for the rest, we do need to set some values. The pageId is pretty straightforward; the caller asked for a specific pageId , so we need to make sure that's what we return. freeBytes and freeData are always the same value for an empty page - just a count of empty bytes on the page. The page header is the only space being used, so we just subtract 20-bytes from the size of a full page. And the last field, numRecords , does not need to be explicitly set because it is already zero. Next up, we need to write a function to flush data pages to disk. void flush_page(int fd, Page pg) { int pageId = ((PageHeader*)pg)->pageId; lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_written = write(fd, pg, conf->pageSize); if (bytes_written != conf->pageSize) { printf(\"Page flush unsuccessful\\n\"); } } This one is comparatively a lot simpler. First we just need to extract the pageId from the header, then we tell the file pointer to move to the beginning of the spot where this page should be written. Then we write the page to disk. The last piece is just an informational console log to tell us if something unexpected happened. We'll implement error handling later. Updating main.c and Makefile Before we can demonstrate reading and writing pages to disk, we need to make some small updates to our main function and the Makefile. src/main.c #include \"global/config.h\" +#include \"storage/file.h\" +#include \"storage/page.h\" Config* conf; print_config(conf); + FileDesc* fdesc = file_open(conf->dataFile); + Page pg = read_page(fdesc->fd, 1); + while(true) { print_prompt(); At the very beginning of our program, we want to open the data file and attempt to read the first page into memory. switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { print_node(n); free_node(n); printf(\"Shutting down...\\n\"); + flush_page(fdesc->fd, pg); + free_page(pg); + file_close(fdesc); return EXIT_SUCCESS; } default: When the database receives the quit command, we want to flush the data page that's currently in memory to disk, close the file, then shut down. src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ + storage/page.c Running the Program Now we can compile and run our program to test out these changes. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ We didn't change anything with the parser, so the interesting stuff doesn't happen here. We just needed to run and quit the program so that it would write a page to our data file for us. Let's take a look at its contents: Using the xxd command, we can inspect the contents of our binary data file. xxd [filename] shows the hex representation of each byte in the file. Every pair of characters represents a single byte. The orange box I highlighted contains the 4-byte pageId header field, which we set to a value of 1 when we created the empty page. Note: my machine is a Little-endian machine, which means it stores the \"little-end\" of a byte sequence first. As an example, say we have a two-byte ( uint16_t ) integer 38,924 represented in binary. As humans, we would show the binary value as: 10011000 00011110 This is the same as Big-endian in the machine world. A Little-endian machine would represent it in reverse order: 00011110 10011000 This is why we see the pageId field with the smallest byte first instead of 0000 0001 <-- hex representation. Also, note that endian-ness only affects the order of bytes, it DOES NOT affect the order of the individual bits within the bytes. The green box contains the 12-bytes of the header that are uninteresting to us right now. And the two blue boxes represent the 2-byte freeBytes and freeData fields. The \"human readable\" way to represent this hex value would be 0x006c , which translates to 108 in decimal. And this is exactly what we want; the page is empty except for the 20-byte header, and we set the page size to 128 bytes in our config file, so there are 108 unused bytes on the page right now. And that covers it. In the next section, we're going to update our lexer and parser to prepare for inserting data into our data page.","title":"DB Page Implementation"},{"location":"04-data-persistence/06-page-implementation/#db-page-implementation","text":"Before we write the code that implements our Page API, we need to add a new global config parameter that tells the DB engine how many bytes a data page consumes. I'm going to start with an absurdly small value: 128 bytes. Add this to the burkeql.conf file: # Byte size of a data page PAGE_SIZE=128 Next, we need to update the config.h header file to inform it of our new config parameter: typedef enum ConfigParameter { CONF_DATA_FILE, + CONF_PAGE_SIZE, CONF_UNRECOGNIZED } ConfigParameter; typedef struct Config { char* dataFile; + int pageSize; } Config; And finally, we need to make the associated updates to our functions in config.c : void print_config(Config* conf) { printf(\"====== BurkeQL Config ======\\n\"); printf(\"= DATA_FILE: %s\\n\", conf->dataFile); + printf(\"= PAGE_SIZE: %d\\n\", conf->pageSize); } static ConfigParameter parse_config_param(char* p) { if (strcmp(p, \"DATA_FILE\") == 0) return CONF_DATA_FILE; + if (strcmp(p, \"PAGE_SIZE\") == 0) return CONF_PAGE_SIZE; return CONF_UNRECOGNIZED; } static void set_config_value(Config* conf, ConfigParameter p, char* v) { switch (p) { case CONF_DATA_FILE: v[strcspn(v, \"\\r\\n\")] = 0; // remove trailing newline character if it exists conf->dataFile = strdup(v); break; + case CONF_PAGE_SIZE: + conf->pageSize = atoi(v); + break; } }","title":"DB Page Implementation"},{"location":"04-data-persistence/06-page-implementation/#page-api","text":"Using the header file we wrote in the previous section, let's dive straight into the code. First, the easy ones: allocating and freeing the page block: extern Config* conf; Page new_page() { Page pg = malloc(conf->pageSize); memset(pg, 0, conf->pageSize); return pg; } void free_page(Page pg) { if (pg != NULL) free(pg); } It's about as straightforward as you can get. We simply allocate enough memory for a fixed-size data page and return the pointer. And we pair it with a function that frees the memory. Now let's read a page block from disk into a Page object: Page read_page(int fd, uint32_t pageId) { Page pg = new_page(); lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_read = read(fd, pg, conf->pageSize); if (bytes_read != conf->pageSize) { printf(\"Bytes read: %d\\n\", bytes_read); PageHeader* pgHdr = (PageHeader*)pg; /* Since this is a brand new page, we need to set the header fields appropriately */ pgHdr->pageId = pageId; pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); pgHdr->freeData = conf->pageSize - sizeof(PageHeader); } return pg; } Our function takes two inputs: a file descriptor and a pageId . This implies the caller will already have an open file, and it knows exactly which page block it wants to pull from disk. First, we need to allocate memory as a landing spot for the data we intend to pull from disk. Then we tell the file pointer to move to offset (pageId - 1) * conf->pageSize bytes from the beginning of the file ( SEEK_SET ). It's important to note that OUR pageId s are 1-based, but the math required to get the byte offset operates on 0-based pageIds. Next we read conf->pageSize bytes into our Page memory block. The read function returns the number of bytes read from disk. If the value is not equal to the size of a page block, then there was an error, or the page does not exist in the file. If the page doesn't exist, then we need to set the PageHeader fields to values that represent a blank data page. We're currently not using pageType , indexLevel , prevPageId , and nextPageId so we don't need to set any values. As for the rest, we do need to set some values. The pageId is pretty straightforward; the caller asked for a specific pageId , so we need to make sure that's what we return. freeBytes and freeData are always the same value for an empty page - just a count of empty bytes on the page. The page header is the only space being used, so we just subtract 20-bytes from the size of a full page. And the last field, numRecords , does not need to be explicitly set because it is already zero. Next up, we need to write a function to flush data pages to disk. void flush_page(int fd, Page pg) { int pageId = ((PageHeader*)pg)->pageId; lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_written = write(fd, pg, conf->pageSize); if (bytes_written != conf->pageSize) { printf(\"Page flush unsuccessful\\n\"); } } This one is comparatively a lot simpler. First we just need to extract the pageId from the header, then we tell the file pointer to move to the beginning of the spot where this page should be written. Then we write the page to disk. The last piece is just an informational console log to tell us if something unexpected happened. We'll implement error handling later.","title":"Page API"},{"location":"04-data-persistence/06-page-implementation/#updating-mainc-and-makefile","text":"Before we can demonstrate reading and writing pages to disk, we need to make some small updates to our main function and the Makefile. src/main.c #include \"global/config.h\" +#include \"storage/file.h\" +#include \"storage/page.h\" Config* conf; print_config(conf); + FileDesc* fdesc = file_open(conf->dataFile); + Page pg = read_page(fdesc->fd, 1); + while(true) { print_prompt(); At the very beginning of our program, we want to open the data file and attempt to read the first page into memory. switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { print_node(n); free_node(n); printf(\"Shutting down...\\n\"); + flush_page(fdesc->fd, pg); + free_page(pg); + file_close(fdesc); return EXIT_SUCCESS; } default: When the database receives the quit command, we want to flush the data page that's currently in memory to disk, close the file, then shut down. src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ + storage/page.c","title":"Updating main.c and Makefile"},{"location":"04-data-persistence/06-page-implementation/#running-the-program","text":"Now we can compile and run our program to test out these changes. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... $ We didn't change anything with the parser, so the interesting stuff doesn't happen here. We just needed to run and quit the program so that it would write a page to our data file for us. Let's take a look at its contents: Using the xxd command, we can inspect the contents of our binary data file. xxd [filename] shows the hex representation of each byte in the file. Every pair of characters represents a single byte. The orange box I highlighted contains the 4-byte pageId header field, which we set to a value of 1 when we created the empty page. Note: my machine is a Little-endian machine, which means it stores the \"little-end\" of a byte sequence first. As an example, say we have a two-byte ( uint16_t ) integer 38,924 represented in binary. As humans, we would show the binary value as: 10011000 00011110 This is the same as Big-endian in the machine world. A Little-endian machine would represent it in reverse order: 00011110 10011000 This is why we see the pageId field with the smallest byte first instead of 0000 0001 <-- hex representation. Also, note that endian-ness only affects the order of bytes, it DOES NOT affect the order of the individual bits within the bytes. The green box contains the 12-bytes of the header that are uninteresting to us right now. And the two blue boxes represent the 2-byte freeBytes and freeData fields. The \"human readable\" way to represent this hex value would be 0x006c , which translates to 108 in decimal. And this is exactly what we want; the page is empty except for the 20-byte header, and we set the page size to 128 bytes in our config file, so there are 108 unused bytes on the page right now. And that covers it. In the next section, we're going to update our lexer and parser to prepare for inserting data into our data page.","title":"Running the Program"},{"location":"04-data-persistence/07-parser-refactor-insert/","text":"Parser Refactor - Insert In order to support insert operations, we need to update our lexer/parser to identify when we want to insert data. Fair warning, the next several \"chapters\" of this guide are going to involve writing a lot of code we're going to throw out in future sections. I'm doing it this way because it allows me to focus on a specific topic while keeping the necessary code footprint small. As we go through the next few chapters, we'll slowly build our permanent codebase, only using the temporary code as stepping stones towards a more robust database program. You might be wondering how we're going to insert data into a table we haven't created yet. Simple, we're going to hard-code a basic table definition into our program - this is the first of the throwaway code we'll be writing. Our table can be defined by the following DDL: Create Table person ( person_id Int Not Null, name Char(20) Not Null ); So we need to update the lexer and parser to identify when we want to insert data AND what the values that we pass to it are. Our grammar isn't going to look like SQL just yet. We're going to define an insert statement as such: bql > insert [person_id] [name] Where person_id is a raw number and name is a QUOTED string (single quotes). So if I wanted to insert person_id: 5, name: Chris Burke, I would write: bql > insert 5 'Chris Burke' Parsetree src/include/parser/parsetree.h typedef enum NodeTag { - T_SysCmd + T_SysCmd, + T_InsertStmt } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; +typedef struct InsertStmt { + NodeTag type; + int personId; + char* name; +} InsertStmt; In our parsetree header we're just adding a new node type and struct to support our new insert statement. Here, you can see we're beginning to hard-code our table definition. src/parser/parsetree.c #include <stdlib.h> +#include <string.h> #include <stdio.h> #include \"parser/parsetree.h\" static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } +static void free_insert_stmt(InsertStmt* ins) { + if (ins == NULL) return; + + if (ins->name != NULL) free(ins->name); +} + void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; + case T_InsertStmt: + free_insert_stmt((InsertStmt*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } In our parsetree implementation, we need to write a function that can free the new InsertStmt struct. Remember, we don't need a special function to allocate memory for it because we wrote that create_node macro in the parsetree header. void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; + case T_InsertStmt: + printf(\"= Type: Insert\\n\"); + printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); + printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); + break; default: printf(\"print_node() | unknown node type\\n\"); } } I'm also adding the insert node to our print_node logic just so we can make sure the parser processes everything correctly. +char* str_strip_quotes(char* str) { + int length = strlen(str); + char* finalStr = malloc(length - 1); + memcpy(finalStr, str + 1, length - 2); + finalStr[length - 2] = '\\0'; + free(str); + return finalStr; +} Lastly, we need a helper function to strip the single quote characters off of the matched input from the lexer. When we write insert 5 'chris burke' , our lexer will match all of 'chris burke' , including the quote characters. We don't want to store those in the table, so we need a way of stripping them out. Lexer src/parser/scan.l SELECT { return SELECT; } + /* numbers */ +-?[0-9]+ { yylval->intval = atoi(yytext); return INTNUM; } + + /* strings */ +'(\\\\.|[^'\\n])*' { yylval->str = strdup(yytext); return STRING; } + /* everything else */ [ \\t\\n] /* whitespace */ We need to write some additional regex patterns to match integers and quoted strings. If you understand regex, these should make sense to you. This string pattern might be a little confusing, but essentially it will match anything surrounded by single quotes except for line terminators and a single quote character itself. We'll worry about allowing escaped single quotes later. We're also adding two new tokens: INTNUM and STRING - defined in the grammar file. When we match these, we need to provide a value to send along to bison. That's where the yylval->... assignment statement comes from. The intval and str properties are defined in bison's %union section and their values will be made available to whatever code parses the associated token. Grammar src/parser/gram.y %union { char* str; + int intval; struct Node* node; } %parse-param { struct Node** n } %param { void* scanner } +%token <str> SYS_CMD STRING + +%token <intval> INTNUM /* reserved keywords in alphabetical order */ %token INSERT Here we add the intval property to our union, which gives flex access to it via yylval->intval . And we also add our new tokens. The difference between %token <type_name> TOKEN_NAME and %token TOKEN_NAME is just that those with an associated <type_name> will have a value attached to them. +insert_stmt: INSERT INTNUM STRING { - printf(\"INSERT command received\\n\"); - $$ = NULL; + InsertStmt* ins = create_node(InsertStmt); + ins->personId = $2; + ins->name = str_strip_quotes($3); + $$ = (Node*)ins; } ; Running the Program Since we didn't add any new files, we don't need to add anything to the Makefile. We can just compile and run: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 5 'chris burke' ====== Node ====== = Type: Insert = person_id: 5 = name: chris burke bql > As expected, our parser was able to identify both the Int and string we want to insert, as well as successfully strip the single quote characters from the string.","title":"Parser Refactor - Insert"},{"location":"04-data-persistence/07-parser-refactor-insert/#parser-refactor-insert","text":"In order to support insert operations, we need to update our lexer/parser to identify when we want to insert data. Fair warning, the next several \"chapters\" of this guide are going to involve writing a lot of code we're going to throw out in future sections. I'm doing it this way because it allows me to focus on a specific topic while keeping the necessary code footprint small. As we go through the next few chapters, we'll slowly build our permanent codebase, only using the temporary code as stepping stones towards a more robust database program. You might be wondering how we're going to insert data into a table we haven't created yet. Simple, we're going to hard-code a basic table definition into our program - this is the first of the throwaway code we'll be writing. Our table can be defined by the following DDL: Create Table person ( person_id Int Not Null, name Char(20) Not Null ); So we need to update the lexer and parser to identify when we want to insert data AND what the values that we pass to it are. Our grammar isn't going to look like SQL just yet. We're going to define an insert statement as such: bql > insert [person_id] [name] Where person_id is a raw number and name is a QUOTED string (single quotes). So if I wanted to insert person_id: 5, name: Chris Burke, I would write: bql > insert 5 'Chris Burke'","title":"Parser Refactor - Insert"},{"location":"04-data-persistence/07-parser-refactor-insert/#parsetree","text":"src/include/parser/parsetree.h typedef enum NodeTag { - T_SysCmd + T_SysCmd, + T_InsertStmt } NodeTag; typedef struct Node { NodeTag type; } Node; typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; +typedef struct InsertStmt { + NodeTag type; + int personId; + char* name; +} InsertStmt; In our parsetree header we're just adding a new node type and struct to support our new insert statement. Here, you can see we're beginning to hard-code our table definition. src/parser/parsetree.c #include <stdlib.h> +#include <string.h> #include <stdio.h> #include \"parser/parsetree.h\" static void free_syscmd(SysCmd* sc) { if (sc == NULL) return; free(sc->cmd); } +static void free_insert_stmt(InsertStmt* ins) { + if (ins == NULL) return; + + if (ins->name != NULL) free(ins->name); +} + void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; + case T_InsertStmt: + free_insert_stmt((InsertStmt*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } In our parsetree implementation, we need to write a function that can free the new InsertStmt struct. Remember, we don't need a special function to allocate memory for it because we wrote that create_node macro in the parsetree header. void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; + case T_InsertStmt: + printf(\"= Type: Insert\\n\"); + printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); + printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); + break; default: printf(\"print_node() | unknown node type\\n\"); } } I'm also adding the insert node to our print_node logic just so we can make sure the parser processes everything correctly. +char* str_strip_quotes(char* str) { + int length = strlen(str); + char* finalStr = malloc(length - 1); + memcpy(finalStr, str + 1, length - 2); + finalStr[length - 2] = '\\0'; + free(str); + return finalStr; +} Lastly, we need a helper function to strip the single quote characters off of the matched input from the lexer. When we write insert 5 'chris burke' , our lexer will match all of 'chris burke' , including the quote characters. We don't want to store those in the table, so we need a way of stripping them out.","title":"Parsetree"},{"location":"04-data-persistence/07-parser-refactor-insert/#lexer","text":"src/parser/scan.l SELECT { return SELECT; } + /* numbers */ +-?[0-9]+ { yylval->intval = atoi(yytext); return INTNUM; } + + /* strings */ +'(\\\\.|[^'\\n])*' { yylval->str = strdup(yytext); return STRING; } + /* everything else */ [ \\t\\n] /* whitespace */ We need to write some additional regex patterns to match integers and quoted strings. If you understand regex, these should make sense to you. This string pattern might be a little confusing, but essentially it will match anything surrounded by single quotes except for line terminators and a single quote character itself. We'll worry about allowing escaped single quotes later. We're also adding two new tokens: INTNUM and STRING - defined in the grammar file. When we match these, we need to provide a value to send along to bison. That's where the yylval->... assignment statement comes from. The intval and str properties are defined in bison's %union section and their values will be made available to whatever code parses the associated token.","title":"Lexer"},{"location":"04-data-persistence/07-parser-refactor-insert/#grammar","text":"src/parser/gram.y %union { char* str; + int intval; struct Node* node; } %parse-param { struct Node** n } %param { void* scanner } +%token <str> SYS_CMD STRING + +%token <intval> INTNUM /* reserved keywords in alphabetical order */ %token INSERT Here we add the intval property to our union, which gives flex access to it via yylval->intval . And we also add our new tokens. The difference between %token <type_name> TOKEN_NAME and %token TOKEN_NAME is just that those with an associated <type_name> will have a value attached to them. +insert_stmt: INSERT INTNUM STRING { - printf(\"INSERT command received\\n\"); - $$ = NULL; + InsertStmt* ins = create_node(InsertStmt); + ins->personId = $2; + ins->name = str_strip_quotes($3); + $$ = (Node*)ins; } ;","title":"Grammar"},{"location":"04-data-persistence/07-parser-refactor-insert/#running-the-program","text":"Since we didn't add any new files, we don't need to add anything to the Makefile. We can just compile and run: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 5 'chris burke' ====== Node ====== = Type: Insert = person_id: 5 = name: chris burke bql > As expected, our parser was able to identify both the Int and string we want to insert, as well as successfully strip the single quote characters from the string.","title":"Running the Program"},{"location":"04-data-persistence/08-serializing-and-inserting-data/","text":"Serializing Data In this section, we are going to take the data from our insert statement and serialize it into a data Record that can then be inserted into the data page. The serializer needs to know a few things about our table/columns in order to process the data we give to it. Specifically, it needs to know how many columns are in the table and information about each column, such as the column's name, data type, its position in the record, and the byte-length. As for the data, we need a convenient format to stuff our data into and pass it to the serializer - something that can take any kind of data type and trojan horse it into the serializer. In order to do this, we're going to work with two new concepts: the RecordDescriptor and the Datum . These two concepts might not make a ton of sense right now, but they will be essential when we start working tables/columns that aren't hard-coded. RecordDescriptor A RecordDescriptor is very appropriately named - it contains information that describes a table record. We'll use it to store information like the number of columns and a list of the columns with info about each one. Right now, our is actually quite simple: src/include/storage/record.h typedef struct RecordDescriptor { int ncols; Column cols[]; } RecordDescriptor; We have a property, ncols , that contains the number of columns stored in the data record. Currently, we're working with a hard-coded table definition that contains two columns. There is also a flexible array member that contains a list of Column objects, which are defined as follows: src/include/storage/record.h #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct Column { char* colname; DataType dataType; int colnum; /* 0-based col index */ int len; /* byte-length of the column */ } Column; You can think of this as a \"ColumnDescriptor\". We're storing some metadata about the column so that our serializer knows what to do with the data it receives. DataType is an enum containing all supported data types by our database program: src/include/storage/record.h typedef enum DataType { DT_INT, /* 4-bytes, signed */ DT_CHAR /* Byte-size defined at table creation */ } DataType; As we add support for more data types, we'll need to an entry in this enum. Datum The singular form of the word \"data,\" this is meant to be a super generic wrapper around any value that flows to and from a data page. We use something like a Datum so that we can stuff multiple different data types into an array when reading or writing column values from a data record. Say we have a table with two columns, an Int and a Char(20) . It's much easier to convert each of them to a Datum before sending it through a serializer than it would be to write a million switch/case statements for all of the different supported data types in all of the intermediate functions between the client and the serializer. Instead, we only need to write one switch/case inside the serializer. Everything else just has to deal with a Datum . Fortunately implementing this logic is pretty simple: src/include/storage/datum.h typedef unsigned long Datum; Datum int32GetDatum(int32_t i); Datum charGetDatum(char* c); int32_t datumGetInt32(Datum d); char* datumGetString(Datum d); We define a Datum as an unsigned long , and a pair of conversion functions for each data type we support. Right now we support the Int and Char database data types, so we need a function to convert them from their C-types to the Datum , and another to convert from the Datum back to the C-type. And those functions could not be simpler, we're simply casting pointers back and forth: src/storage/datum.c Datum int32GetDatum(int32_t i) { return (Datum) i; } Datum charGetDatum(char* c) { return (Datum) c; } int32_t datumGetInt32(Datum d) { return (int32_t) d; } char* datumGetString(Datum d) { return (char*) d; } record.h Before we move on to the serializing section, I want to round out the record.h header file. Similar to the Page type we created earlier, we're going to create a generic Record type that's an alias for a char* : src/include/storage/record.h typedef char* Record; We're also going to expose a few functions that will be mainstays in our database program. Record record_init(uint16_t recordLen); void free_record(Record r); void free_record_desc(RecordDescriptor* rd); void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); void fill_record(RecordDescriptor* rd, Record r, Datum* data); record_init and free_record are your basic allocator/free pair of functions. free_record_desc . You're probably wondering why there's not allocator function to go with this one. It's because the allocator needs to have hard-coded values at this point, so it's going to be in main.c for now. construct_column_desc is just a way to populate a Column object with the necessary metadata. fill_record is the main serializer function. It takes your Datum array and serializes each column value according to the information it finds in the RecordDescriptor . Serializing and Inserting Data Quick warning before we get into the code. You might be confused by the way I'm organizing the code here - some of it is throwaway code I'm sticking in the main.c file knowing it'll be refactored or wholesale rewritten later, and other pieces serve as groundwork for the more permanent code. So if you find yourself asking \"why is this function here instead of in ?\", just bear with me for a while. It'll be more organized down the road. Okay, so how do we use RecordDescriptor s and Datum s to turn our data into a table record? First, let's list the steps we need to take: Construct a RecordDescriptor for the table we want to insert our data into. Allocate a block of memory with the exact amount of space our data record needs. This is the landing zone for the serialized data. Allocate memory for a Datum array and fill it by converting our input data into Datum s. Serialize Insert Constructing a RecordDescriptor Since we're working with a hard-coded table, we can hard-code the contents of a RecordDescriptor : src/main.c static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (2 * sizeof(Column))); rd->ncols = 2; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); return rd; } We start by allocating memory for the RecordDescriptor object. We are taking advantage of C's flexible array member pattern here. The struct definition includes a Column array without a defined size, which means we get to define its size at the same time we malloc the parent object. Everything after sizeof(RecordDescriptor) will be part of the Column array. The rest of the function is where we hard-code the table definition. We know there are two columns. We know person_id is a DT_INT , the 0th column, and consumes 4 bytes. Similarly, we know name is a DT_CHAR , the 1st column, and consumes 20 bytes. Note: since the RecordDescriptor contains a Column array, NOT a Column* , we need to & the item in the array in order to pass the pointer to the function. src/storage/record/c void free_record_desc(RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].colname != NULL) { free(rd->cols[i].colname); } } free(rd); } void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len) { col->colname = strdup(colname); col->dataType = type; col->colnum = colnum; col->len = len; } free_record_desc and construct_column_desc are a couple of those permanent functions that will stick with us until the end. Of course we always need to pair the allocator function with a matching free function. We loop through the column array and free the char* pointers if they're present; however, we DO NOT free each array item. This is because they were allocated by the code used to allocate the RecordDescriptor . When we free the RecordDescriptor , we also free the Column array. construct_column_desc is just a utility that sets the Column struct values. Allocate a Record Next up, we need to allocate memory for the record. By the time our program gets to this point, it will have all the information it needs to determine the byte-length of the record it's going to serialize. So our allocator function asks for it as a function parameter: src/storage/record.c Record record_init(uint16_t recordLen) { Record r = malloc(recordLen); memset(r, 0, recordLen); return r; } void free_record(Record r) { if (r != NULL) free(r); } Almost identical to the functions that allocate/free memory for a Page . The Datum Array and Serializing Data We have a primary driver function for serializing data, called serialize_data . It is responsible for creating and populating the Datum array and serializing the data. It is another throwaway function, so the functionality lives in main.c for now. src/main.c static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name) { Datum* data = malloc(rd->ncols * sizeof(Datum)); populate_datum_array(data, person_id, name); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Pretty simple, just four lines that make calls to other functions performing the hard work. It takes a RecordDescriptor and Record as input, so by now the caller must have both of those objects ready to go. populate_datum_array is yet another throwaway function that simply converts our data values to Datum types and stashes them in the array. We'll cover fill_record below, but note the second input paramter. We're passing in r + sizeof(RecordHeader) . This function is going to copy our Datum data into the Record landing zone starting at the memory address we give it. Since we want the header data to stay as is, we need to give it the first writable address of that block of memory, hence + sizeof(RecordHeader) . src/main.c static void populate_datum_array(Datum* data, int32_t person_id, char* name) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); } Here we're simply converting person_id and name to Datum types and storing them in the array. Next we have the meat and potatoes: fill_record . This function is where the serialization takes place. src/storage/record.c void fill_record(RecordDescriptor* rd, Record r, Datum* data) { for (int i = 0; i < rd->ncols; i++) { Column* col = &rd->cols[i]; fill_val(col, &r, data[i]); } } It's actually pretty simple. We just loop through the RecordDescriptor 's Column array and call fill_val . fill_val on the other hand can get quite gnarly. Notice the second parameter; we pass in the address of a pointer - remember Record is just a char* . Why is that necessary? I'll explain below. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { case DT_INT: dataLen = 4; int32_t valInt = datumGetInt32(datum); memcpy(data, &valInt, dataLen); break; case DT_CHAR: dataLen = col->len; char* str = strdup(datumGetString(datum)); int charLen = strlen(str); if (charLen > dataLen) charLen = dataLen; memcpy(data, str, charLen); free(str); break; } data += dataLen; *dataP = data; } This is where we actually do the serializing. The interesting bit is handled in the switch block where we perform data type specific actions to deposit our raw data into the Record landing zone. But first, a note about the char** dataP parameter and the second line of the function: char* data = *dataP . Why do we pass in the memory address of a pointer, then immediately dereference it back to a pointer and store it in a different variable? In C, we pass parameters into functions by value , meaning if we take a pointer and perform some pointer arithmetic on it, e.g. data += dataLen , it will not persist when we return from the function. Sometimes we want that behavior, sometimes we don't. In this case, we definitely do not - we want the pointer arithmetic to persist. The obvious question then becomes \"why do we want the pointer arithmetic to stick?\" Because we call fill_val in a loop. Each iteration writes some data to the Record memory block and after we write something, we want to move the pointer to the next writable address. In our case, we first write the 4-byte person_id int to the Record . Now we want to move the pointer by 4 bytes so that the next call to fill_val doesn't overwrite our person_id data. Anyways, back to the serializer function. In the switch block we convert our Datum value back to its C-type, according to the info in the Column descriptor. Then we memcpy it into the Record landing zone. In the DT_CHAR case we have extra logic to truncate any strings that exceed the length defined in the Column descriptor. After that, we move the Record pointer forward by the number of bytes consumed by the data type. It might look weird that we convert our person_id and name values from their C-types into Datum s, then back to their C-types before copying them into the Record . But, remember a Datum is just an easy way to have a singular data type such that we can have an array containing multiple different types of data. Inserting a Record Now that the hard part is done, we get to tell our database to add the serialized Record to the data page. Our primary driver function (again, a throwaway piece of code): src/main.c #define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) static bool insert_record(Page pg, int32_t person_id, char* name) { RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); serialize_data(rd, r, person_id, name); bool insertSuccessful = page_insert(pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } We go through all of the steps mentioned above: construct a RecordDescriptor , allocate a Record , and serialize the data. Then we attempt to insert the record on the page. Also note the RECORD_LEN macro. Since we have the table hard-coded, we know exactly how many bytes each record will consume. The page_insert function is a permanent function, so we need to add it to the header file: src/include/storage/page.h void free_page(Page pg); +bool page_insert(Page pg, Record data, uint16_t length); + Page read_page(int fd, uint32_t pageId); And its definition: src/storage/page.c bool page_insert(Page pg, Record data, uint16_t length) { int spaceRequired = length + sizeof(SlotPointer); if (!page_has_space(pg, spaceRequired)) return false; SlotPointer* sp = malloc(sizeof(SlotPointer)); sp->length = length; /** * Calculating the new record's offset position: * PAGE_SIZE - * SLOT_ARRAY_SIZE - * `freeData` */ int slotArraySize = ((PageHeader*)pg)->numRecords * sizeof(SlotPointer); sp->offset = conf->pageSize - slotArraySize - ((PageHeader*)pg)->freeData; /* copy the record data to the correct spot on the page */ memcpy(pg + sp->offset, data, length); /* prepend the new slot pointer to the slot array */ int newSlotOffset = conf->pageSize - slotArraySize - sizeof(SlotPointer); memcpy(pg + newSlotOffset, sp, sizeof(SlotPointer)); /* update header fields */ PageHeader* pgHdr = (PageHeader*)pg; pgHdr->numRecords++; pgHdr->freeBytes -= spaceRequired; pgHdr->freeData = conf->pageSize - (slotArraySize + sizeof(SlotPointer)) - (sp->offset + length); free(sp); return true; } The first thing we need to do is determine if the page has enough available space for a new record. The caller tells us how many bytes the data itself consumes with the length parameter. And since this code is responsible for maintaining the integrity of the Page , it knows that the true cost of inserting a new record also includes 4 bytes for an additional SlotPointer . Se we write a helper function to do this check for us: src/storage/page.c static bool page_has_space(Page pg, int length) { int availableSpace = ((PageHeader*)pg)->freeData; return availableSpace >= length; } We just check if the freeData header property is large enough for the new record and its slot pointer. If there is enough space on the page, we continue by allocating a new SlotPointer and setting its length property. Then we determine where on the page to write the new data record. We'll place it immediately after the last consumed byte on the page, whether that's the page header or another record, it makes no difference. In order to get our offset, we can start at the end of the page and work backwards. Using the global pageSize , we subtract the number of bytes consumed by the slot array, then we subtract the number of continuous, unused bytes between the slot array and the first claimed byte; that's our new record's location. After we memcpy the new record to the page, we need to prepend the new slot pointer to the slot array. This one is easy, it just goes 4 bytes before the beginning of the slot array. Finally, we need to update the page header to reflect the new data record. Of the header fields we're actually using right now, we increment numRecords , subtract spaceRequired from freeBytes , and calculate the size of the continuous block of data between the last data record and the slot array. Right now, freeBytes and freeData will always end up being the same value. Once we introduce Update statements, they will be able to diverge. After we insert the record, we free the slot pointer and return true . Let's Try It Out src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); flush_page(fdesc->fd, pg); free_page(pg); file_close(fdesc); return EXIT_SUCCESS; } + break; + case T_InsertStmt: + int32_t person_id = ((InsertStmt*)n)->personId; + char* name = ((InsertStmt*)n)->name; + if (!insert_record(pg, person_id, name)) { + printf(\"Unable to insert record\\n\"); + } } src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ + storage/page.c \\ + storage/record.c \\ + storage/datum.c Now we can compile and run: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 Bytes read: 0 bql > insert 69 'chris burke' bql > \\quit Shutting down... (I removed the print_node output to reduce the noise shown here) Here I insert one record, then quit the program. Let's take a look at the contents of our data page: Command: xxd db_files/main.dbd The green box is the 20-byte page header, which we covered earlier. Following that is 12-byte record header, of which we currently don't use any fields so it's not very interesting. Then in the red box we have the value for person_id . Remember these bytes are represented in hex code where every two characters in one byte. person_id is a 4-byte integer stored on a little endian machine, and 45 in hex translates to 69 in decimal - as expected. The orange (yellow?) box represents the name column. The hex values just correspond to ASCII codes, and the xxd command translates that to our alphabet on the right. You can see it's exactly what we wanted to insert. Notice the orange box on the left encompases a bunch of bytes after the end of the \"chris burke\" value. That's because our data type is a Char(20) , so it will consume 20 bytes on disk regardless of how much space the actual text needs. Lastly, the purple box is the slot array with a single item. The first two bytes represent the location of the data record on the page - 0x14 translates to 20, meaning the data record starts at byte 20. The next two bytes store the byte-length of its associated record. 0x24 translates to 36, which is exactly how many bytes our record consumes. Now let's try to insert two more records, which would exceed the size of our 128-byte data page. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 669 'I am going to be longer than 20 characters' bql > insert 99 'this should fail' Unable to insert record bql > \\quit Shutting down... The first record I try to insert a value in the name column that exceeds the maximum length of a Char(20) . Then I try to insert a 3rd record, but it won't fit on the page so I get an error message. Let's take a look at the data page now: And we can indeed see that our code cut off everything after 20 characters for the name column. That wraps up the Data Persistence section. In the next section we'll add select support to our little CLI.","title":"Serializing and Inserting Data"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#serializing-data","text":"In this section, we are going to take the data from our insert statement and serialize it into a data Record that can then be inserted into the data page. The serializer needs to know a few things about our table/columns in order to process the data we give to it. Specifically, it needs to know how many columns are in the table and information about each column, such as the column's name, data type, its position in the record, and the byte-length. As for the data, we need a convenient format to stuff our data into and pass it to the serializer - something that can take any kind of data type and trojan horse it into the serializer. In order to do this, we're going to work with two new concepts: the RecordDescriptor and the Datum . These two concepts might not make a ton of sense right now, but they will be essential when we start working tables/columns that aren't hard-coded.","title":"Serializing Data"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#recorddescriptor","text":"A RecordDescriptor is very appropriately named - it contains information that describes a table record. We'll use it to store information like the number of columns and a list of the columns with info about each one. Right now, our is actually quite simple: src/include/storage/record.h typedef struct RecordDescriptor { int ncols; Column cols[]; } RecordDescriptor; We have a property, ncols , that contains the number of columns stored in the data record. Currently, we're working with a hard-coded table definition that contains two columns. There is also a flexible array member that contains a list of Column objects, which are defined as follows: src/include/storage/record.h #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct Column { char* colname; DataType dataType; int colnum; /* 0-based col index */ int len; /* byte-length of the column */ } Column; You can think of this as a \"ColumnDescriptor\". We're storing some metadata about the column so that our serializer knows what to do with the data it receives. DataType is an enum containing all supported data types by our database program: src/include/storage/record.h typedef enum DataType { DT_INT, /* 4-bytes, signed */ DT_CHAR /* Byte-size defined at table creation */ } DataType; As we add support for more data types, we'll need to an entry in this enum.","title":"RecordDescriptor"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#datum","text":"The singular form of the word \"data,\" this is meant to be a super generic wrapper around any value that flows to and from a data page. We use something like a Datum so that we can stuff multiple different data types into an array when reading or writing column values from a data record. Say we have a table with two columns, an Int and a Char(20) . It's much easier to convert each of them to a Datum before sending it through a serializer than it would be to write a million switch/case statements for all of the different supported data types in all of the intermediate functions between the client and the serializer. Instead, we only need to write one switch/case inside the serializer. Everything else just has to deal with a Datum . Fortunately implementing this logic is pretty simple: src/include/storage/datum.h typedef unsigned long Datum; Datum int32GetDatum(int32_t i); Datum charGetDatum(char* c); int32_t datumGetInt32(Datum d); char* datumGetString(Datum d); We define a Datum as an unsigned long , and a pair of conversion functions for each data type we support. Right now we support the Int and Char database data types, so we need a function to convert them from their C-types to the Datum , and another to convert from the Datum back to the C-type. And those functions could not be simpler, we're simply casting pointers back and forth: src/storage/datum.c Datum int32GetDatum(int32_t i) { return (Datum) i; } Datum charGetDatum(char* c) { return (Datum) c; } int32_t datumGetInt32(Datum d) { return (int32_t) d; } char* datumGetString(Datum d) { return (char*) d; }","title":"Datum"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#recordh","text":"Before we move on to the serializing section, I want to round out the record.h header file. Similar to the Page type we created earlier, we're going to create a generic Record type that's an alias for a char* : src/include/storage/record.h typedef char* Record; We're also going to expose a few functions that will be mainstays in our database program. Record record_init(uint16_t recordLen); void free_record(Record r); void free_record_desc(RecordDescriptor* rd); void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); void fill_record(RecordDescriptor* rd, Record r, Datum* data); record_init and free_record are your basic allocator/free pair of functions. free_record_desc . You're probably wondering why there's not allocator function to go with this one. It's because the allocator needs to have hard-coded values at this point, so it's going to be in main.c for now. construct_column_desc is just a way to populate a Column object with the necessary metadata. fill_record is the main serializer function. It takes your Datum array and serializes each column value according to the information it finds in the RecordDescriptor .","title":"record.h"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#serializing-and-inserting-data","text":"Quick warning before we get into the code. You might be confused by the way I'm organizing the code here - some of it is throwaway code I'm sticking in the main.c file knowing it'll be refactored or wholesale rewritten later, and other pieces serve as groundwork for the more permanent code. So if you find yourself asking \"why is this function here instead of in ?\", just bear with me for a while. It'll be more organized down the road. Okay, so how do we use RecordDescriptor s and Datum s to turn our data into a table record? First, let's list the steps we need to take: Construct a RecordDescriptor for the table we want to insert our data into. Allocate a block of memory with the exact amount of space our data record needs. This is the landing zone for the serialized data. Allocate memory for a Datum array and fill it by converting our input data into Datum s. Serialize Insert","title":"Serializing and Inserting Data"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#constructing-a-recorddescriptor","text":"Since we're working with a hard-coded table, we can hard-code the contents of a RecordDescriptor : src/main.c static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (2 * sizeof(Column))); rd->ncols = 2; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); return rd; } We start by allocating memory for the RecordDescriptor object. We are taking advantage of C's flexible array member pattern here. The struct definition includes a Column array without a defined size, which means we get to define its size at the same time we malloc the parent object. Everything after sizeof(RecordDescriptor) will be part of the Column array. The rest of the function is where we hard-code the table definition. We know there are two columns. We know person_id is a DT_INT , the 0th column, and consumes 4 bytes. Similarly, we know name is a DT_CHAR , the 1st column, and consumes 20 bytes. Note: since the RecordDescriptor contains a Column array, NOT a Column* , we need to & the item in the array in order to pass the pointer to the function. src/storage/record/c void free_record_desc(RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].colname != NULL) { free(rd->cols[i].colname); } } free(rd); } void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len) { col->colname = strdup(colname); col->dataType = type; col->colnum = colnum; col->len = len; } free_record_desc and construct_column_desc are a couple of those permanent functions that will stick with us until the end. Of course we always need to pair the allocator function with a matching free function. We loop through the column array and free the char* pointers if they're present; however, we DO NOT free each array item. This is because they were allocated by the code used to allocate the RecordDescriptor . When we free the RecordDescriptor , we also free the Column array. construct_column_desc is just a utility that sets the Column struct values.","title":"Constructing a RecordDescriptor"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#allocate-a-record","text":"Next up, we need to allocate memory for the record. By the time our program gets to this point, it will have all the information it needs to determine the byte-length of the record it's going to serialize. So our allocator function asks for it as a function parameter: src/storage/record.c Record record_init(uint16_t recordLen) { Record r = malloc(recordLen); memset(r, 0, recordLen); return r; } void free_record(Record r) { if (r != NULL) free(r); } Almost identical to the functions that allocate/free memory for a Page .","title":"Allocate a Record"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#the-datum-array-and-serializing-data","text":"We have a primary driver function for serializing data, called serialize_data . It is responsible for creating and populating the Datum array and serializing the data. It is another throwaway function, so the functionality lives in main.c for now. src/main.c static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name) { Datum* data = malloc(rd->ncols * sizeof(Datum)); populate_datum_array(data, person_id, name); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Pretty simple, just four lines that make calls to other functions performing the hard work. It takes a RecordDescriptor and Record as input, so by now the caller must have both of those objects ready to go. populate_datum_array is yet another throwaway function that simply converts our data values to Datum types and stashes them in the array. We'll cover fill_record below, but note the second input paramter. We're passing in r + sizeof(RecordHeader) . This function is going to copy our Datum data into the Record landing zone starting at the memory address we give it. Since we want the header data to stay as is, we need to give it the first writable address of that block of memory, hence + sizeof(RecordHeader) . src/main.c static void populate_datum_array(Datum* data, int32_t person_id, char* name) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); } Here we're simply converting person_id and name to Datum types and storing them in the array. Next we have the meat and potatoes: fill_record . This function is where the serialization takes place. src/storage/record.c void fill_record(RecordDescriptor* rd, Record r, Datum* data) { for (int i = 0; i < rd->ncols; i++) { Column* col = &rd->cols[i]; fill_val(col, &r, data[i]); } } It's actually pretty simple. We just loop through the RecordDescriptor 's Column array and call fill_val . fill_val on the other hand can get quite gnarly. Notice the second parameter; we pass in the address of a pointer - remember Record is just a char* . Why is that necessary? I'll explain below. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { case DT_INT: dataLen = 4; int32_t valInt = datumGetInt32(datum); memcpy(data, &valInt, dataLen); break; case DT_CHAR: dataLen = col->len; char* str = strdup(datumGetString(datum)); int charLen = strlen(str); if (charLen > dataLen) charLen = dataLen; memcpy(data, str, charLen); free(str); break; } data += dataLen; *dataP = data; } This is where we actually do the serializing. The interesting bit is handled in the switch block where we perform data type specific actions to deposit our raw data into the Record landing zone. But first, a note about the char** dataP parameter and the second line of the function: char* data = *dataP . Why do we pass in the memory address of a pointer, then immediately dereference it back to a pointer and store it in a different variable? In C, we pass parameters into functions by value , meaning if we take a pointer and perform some pointer arithmetic on it, e.g. data += dataLen , it will not persist when we return from the function. Sometimes we want that behavior, sometimes we don't. In this case, we definitely do not - we want the pointer arithmetic to persist. The obvious question then becomes \"why do we want the pointer arithmetic to stick?\" Because we call fill_val in a loop. Each iteration writes some data to the Record memory block and after we write something, we want to move the pointer to the next writable address. In our case, we first write the 4-byte person_id int to the Record . Now we want to move the pointer by 4 bytes so that the next call to fill_val doesn't overwrite our person_id data. Anyways, back to the serializer function. In the switch block we convert our Datum value back to its C-type, according to the info in the Column descriptor. Then we memcpy it into the Record landing zone. In the DT_CHAR case we have extra logic to truncate any strings that exceed the length defined in the Column descriptor. After that, we move the Record pointer forward by the number of bytes consumed by the data type. It might look weird that we convert our person_id and name values from their C-types into Datum s, then back to their C-types before copying them into the Record . But, remember a Datum is just an easy way to have a singular data type such that we can have an array containing multiple different types of data.","title":"The Datum Array and Serializing Data"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#inserting-a-record","text":"Now that the hard part is done, we get to tell our database to add the serialized Record to the data page. Our primary driver function (again, a throwaway piece of code): src/main.c #define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) static bool insert_record(Page pg, int32_t person_id, char* name) { RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); serialize_data(rd, r, person_id, name); bool insertSuccessful = page_insert(pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } We go through all of the steps mentioned above: construct a RecordDescriptor , allocate a Record , and serialize the data. Then we attempt to insert the record on the page. Also note the RECORD_LEN macro. Since we have the table hard-coded, we know exactly how many bytes each record will consume. The page_insert function is a permanent function, so we need to add it to the header file: src/include/storage/page.h void free_page(Page pg); +bool page_insert(Page pg, Record data, uint16_t length); + Page read_page(int fd, uint32_t pageId); And its definition: src/storage/page.c bool page_insert(Page pg, Record data, uint16_t length) { int spaceRequired = length + sizeof(SlotPointer); if (!page_has_space(pg, spaceRequired)) return false; SlotPointer* sp = malloc(sizeof(SlotPointer)); sp->length = length; /** * Calculating the new record's offset position: * PAGE_SIZE - * SLOT_ARRAY_SIZE - * `freeData` */ int slotArraySize = ((PageHeader*)pg)->numRecords * sizeof(SlotPointer); sp->offset = conf->pageSize - slotArraySize - ((PageHeader*)pg)->freeData; /* copy the record data to the correct spot on the page */ memcpy(pg + sp->offset, data, length); /* prepend the new slot pointer to the slot array */ int newSlotOffset = conf->pageSize - slotArraySize - sizeof(SlotPointer); memcpy(pg + newSlotOffset, sp, sizeof(SlotPointer)); /* update header fields */ PageHeader* pgHdr = (PageHeader*)pg; pgHdr->numRecords++; pgHdr->freeBytes -= spaceRequired; pgHdr->freeData = conf->pageSize - (slotArraySize + sizeof(SlotPointer)) - (sp->offset + length); free(sp); return true; } The first thing we need to do is determine if the page has enough available space for a new record. The caller tells us how many bytes the data itself consumes with the length parameter. And since this code is responsible for maintaining the integrity of the Page , it knows that the true cost of inserting a new record also includes 4 bytes for an additional SlotPointer . Se we write a helper function to do this check for us: src/storage/page.c static bool page_has_space(Page pg, int length) { int availableSpace = ((PageHeader*)pg)->freeData; return availableSpace >= length; } We just check if the freeData header property is large enough for the new record and its slot pointer. If there is enough space on the page, we continue by allocating a new SlotPointer and setting its length property. Then we determine where on the page to write the new data record. We'll place it immediately after the last consumed byte on the page, whether that's the page header or another record, it makes no difference. In order to get our offset, we can start at the end of the page and work backwards. Using the global pageSize , we subtract the number of bytes consumed by the slot array, then we subtract the number of continuous, unused bytes between the slot array and the first claimed byte; that's our new record's location. After we memcpy the new record to the page, we need to prepend the new slot pointer to the slot array. This one is easy, it just goes 4 bytes before the beginning of the slot array. Finally, we need to update the page header to reflect the new data record. Of the header fields we're actually using right now, we increment numRecords , subtract spaceRequired from freeBytes , and calculate the size of the continuous block of data between the last data record and the slot array. Right now, freeBytes and freeData will always end up being the same value. Once we introduce Update statements, they will be able to diverge. After we insert the record, we free the slot pointer and return true .","title":"Inserting a Record"},{"location":"04-data-persistence/08-serializing-and-inserting-data/#lets-try-it-out","text":"src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); flush_page(fdesc->fd, pg); free_page(pg); file_close(fdesc); return EXIT_SUCCESS; } + break; + case T_InsertStmt: + int32_t person_id = ((InsertStmt*)n)->personId; + char* name = ((InsertStmt*)n)->name; + if (!insert_record(pg, person_id, name)) { + printf(\"Unable to insert record\\n\"); + } } src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ + storage/page.c \\ + storage/record.c \\ + storage/datum.c Now we can compile and run: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 Bytes read: 0 bql > insert 69 'chris burke' bql > \\quit Shutting down... (I removed the print_node output to reduce the noise shown here) Here I insert one record, then quit the program. Let's take a look at the contents of our data page: Command: xxd db_files/main.dbd The green box is the 20-byte page header, which we covered earlier. Following that is 12-byte record header, of which we currently don't use any fields so it's not very interesting. Then in the red box we have the value for person_id . Remember these bytes are represented in hex code where every two characters in one byte. person_id is a 4-byte integer stored on a little endian machine, and 45 in hex translates to 69 in decimal - as expected. The orange (yellow?) box represents the name column. The hex values just correspond to ASCII codes, and the xxd command translates that to our alphabet on the right. You can see it's exactly what we wanted to insert. Notice the orange box on the left encompases a bunch of bytes after the end of the \"chris burke\" value. That's because our data type is a Char(20) , so it will consume 20 bytes on disk regardless of how much space the actual text needs. Lastly, the purple box is the slot array with a single item. The first two bytes represent the location of the data record on the page - 0x14 translates to 20, meaning the data record starts at byte 20. The next two bytes store the byte-length of its associated record. 0x24 translates to 36, which is exactly how many bytes our record consumes. Now let's try to insert two more records, which would exceed the size of our 128-byte data page. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 669 'I am going to be longer than 20 characters' bql > insert 99 'this should fail' Unable to insert record bql > \\quit Shutting down... The first record I try to insert a value in the name column that exceeds the maximum length of a Char(20) . Then I try to insert a 3rd record, but it won't fit on the page so I get an error message. Let's take a look at the data page now: And we can indeed see that our code cut off everything after 20 characters for the name column. That wraps up the Data Persistence section. In the next section we'll add select support to our little CLI.","title":"Let's Try It Out"},{"location":"05-selecting-data/01-parser-refactor-select/","text":"Parser Refactor - Select Now that we're able to insert data into our database, we need to add some functionality that can \"Select\" data back out. We'll start by defining our CLI syntax, then update the lexer/parser to support the new grammar, and finally we'll write the supporting code in our parsetree header and code files. The syntax will be pretty simple: bql > select [colname], [colname], ... ; We look for the SELECT keyword, followed by a comma-separated list of columns we want to select, then ending with a semi-colon. The column names will not be surrounded by quotes - they'll be a class of lexer tokens called IDENTIFIERS . The list of columns can be as long as the user wants, so our parser needs to be smart enough to support a potentially infinite list. And because it can grow forever, we need to introduce a token that tells the parser it's time to stop: the semi-colon. Lexer Starting off simple, the lexer updates are pretty small. We need to add two new match patterns - one for the two punctuation characters, and another for identifiers. src/parser/scan.l + /* operators */ +[,;] { return yytext[0]; } /* strings */ '(\\\\.|''|[^'\\n])*' { yylval->str = strdup(yytext); return STRING; } + /* identifiers */ +[A-Za-z_][A-Za-z0-9_]* { yylval->str = strdup(yytext); return IDENT; } The comma and semi-colon are very straightforward - just those literal characters. Identifiers are also fairly simple. We want any alphanumeric text without whitespace as long as it doesn't begin with a number. Easy, right? Grammar Now let's check out the grammar changes. src/parser/gram.y %union { char* str; int intval; struct Node* node; + struct ParseList* list; } %parse-param { struct Node** n } %param { void* scanner } -%token <str> SYS_CMD STRING +%token <str> SYS_CMD STRING IDENT %token <intval> INTNUM /* reserved keywords in alphabetical order */ %token INSERT %token SELECT -%type <node> cmd stmt sys_cmd select_stmt insert_stmt +%type <node> cmd stmt sys_cmd select_stmt insert_stmt target + +%type <list> target_list %start query We add a new ParseList struct to the union, which will enable us to make lists like the comma-separated list of columns our CLI will accept. We'll cover the details of the struct in the next section. We also need to add a new token for the identifiers our lexer is now able to match. And closing out the definitions section, we have two new grammar rules: target and target_list . As you can probably guess, the target_list is going to be a ParseList to target s. src/parser/gram.y -cmd: stmt +cmd: stmt ';' | sys_cmd ; At the top, we add the terminator semi-colon to the cmd rule when it follows a stmt . With this change we'll also need to end our insert statements with a semi-colon in order for the parser to correctly identify them. sys_cmd s will not need a semi-colon. src/parser/gram.y -select_stmt: SELECT { +select_stmt: SELECT target_list { - printf(\"SELECT command received\\n\"); - $$ = NULL; + SelectStmt* s = create_node(SelectStmt); + s->targetList = $2; + $$ = (Node*)s; + } + ; + +target_list: target { + $$ = create_parselist($1); + } + | target_list ',' target { + $$ = parselist_append($1, $3); + } + ; + +target: IDENT { + ResTarget* r = create_node(ResTarget); + r->name = $1; + $$ = (Node*)r; + } + ; Don't spend too much time worring about new structs/functions you see like ResTarget or create_parselist - I'll cover those below. Instead pay attention to how the grammar is written. Here we have refactored the syntax rules for our select_stmt . We still begin by looking for the SELECT keyword, but now we also require a target_list , which is exactly what it sounds like - a list of targets . Note the clever grammar definition we use here - a valid target_list is either a single target or a target_list followed by another target . This means bison can keep matching target s and appending them to the list until the cows come home (or flex sends a semi-colon). The target itself is just an IDENT token sent by flex, whose value we stuff into a ResTarget node. Parsetree Header These updates to our parser require a fairly significant amount of new code in the parsetree.c file, but first let's go over the changes in the header file. src/include/parser/parsetree.h typedef enum NodeTag { T_SysCmd, - T_InsertStmt + T_InsertStmt, + T_SelectStmt, + T_ParseList, + T_ResTarget } NodeTag; We add three new node types to our library of nodes. The SelectStmt , which we knew was coming, the ParseList , and the ResTarget . The ParseList is just a generic list struct our parser will use to store anything we need to keep in a list. ResTarget - kind of short for \"Result Target\" is a node that stores information about the column our select statement is targeting. src/include/parser/parsetree.h typedef struct Node { NodeTag type; } Node; +typedef struct ParseCell { + void* ptr; +} ParseCell; + +typedef struct ParseList { + NodeTag type; + int length; + int maxLength; + ParseCell* elements; +} ParseList; + typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; We add structs for the ParseList , which keeps an array of ParseCell s. Most often, we'll be using the ParseCell to store a Node , but we define its property to be a generic void* so that we can store whatever we want in it. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int personId; char* name; } InsertStmt; +typedef struct ResTarget { + NodeTag type; + char* name; +} ResTarget; + +typedef struct SelectStmt { + NodeTag type; + ParseList* targetList; +} SelectStmt; Next, we define the ResTarget and SelectStmt nodes. As mentioned above, the ResTarget just stores the name of the column we're \"select\"ing. And the SelectStmt stores a list of ResTarget s. src/include/parser/parsetree.h +#define parselist_make_ptr_cell(v) ((ParseCell) {.ptr = (v)}) + +#define create_parselist(li) new_parselist(parselist_make_ptr_cell(li)) void free_node(Node* n); void print_node(Node* n); char* str_strip_quotes(char* str); +ParseList* new_parselist(ParseCell li); +void free_parselist(ParseList* l); +ParseList* parselist_append(ParseList* l, void* cell); Lastly, we have a couple new macros and a few new functions. Similar to the create_node macro, I wrote the create_parselist macro to simplify list creation. It takes anything as input and first creates a new ParseList of length 1, then it creates a ParseCell and stores whatever we passed in ( li ) in the .ptr field of ParseCell and populates the 1-length array with that ParseCell . Next up, we have three common \"list\" functions: the allocate/free function pair, and a function to append an item to the list. Parsetree Code We have a lot of new code in this file, and it's boring code too, so bear with me. src/parser/parsetree.c +ParseList* new_parselist(ParseCell li) { + ParseList* l = malloc(sizeof(ParseList)); + ParseCell* elements = malloc(sizeof(ParseCell)); + elements[0] = li; + + l->type = T_ParseList; + l->length = 1; + l->maxLength = 1; + l->elements = elements; + + return l; +} + +void free_parselist(ParseList* l) { + if (l != NULL) { + for (int i = 0; i < l->length; i++) { + if (&(l->elements[i]) != NULL) { + free_node((Node*)l->elements[i].ptr); + } + } + + free(l->elements); + } +} Starting with the allocator/free pair of functions. The allocator grabs enough memory for the list struct and a list with a single item - the ParseCell input parameter. We populate the first item of the list and set the list metadata fields. Remember, a ParseList is also a Node , so it needs to have a valid NodeTag in its type property. The free function is standard list free logic. We loop through the elements array and call free_node on the .ptr of each ParseCell . src/parser/parsetree.c +static inline ParseCell* parselist_last_cell(const ParseList* l) { + return &l->elements[l->length]; +} + +#define lcptr(lc) ((lc)->ptr) +#define llast(l) lcptr(parselist_last_cell(l)) + +ParseList* parselist_append(ParseList* l, void* cell) { + if (l->length >= l->maxLength) { + enlarge_list(l); + } + + llast(l) = cell; + + l->length++; + + return l; +} + +static void enlarge_list(ParseList* list) { + if (list == NULL) { + printf(\"ERROR: list is empty!\\n\"); + exit(EXIT_FAILURE); + } else { + if (list->elements == NULL) { + list->elements = malloc(sizeof(ParseCell)); + } else { + list->elements = realloc(list->elements, (list->length + 1) * sizeof(ParseCell)); + } + + list->maxLength++; + } +} The parselist_last_cell and two macros might look confusing, but it's just an easy way to set the last element in a list. Our append function grows the list if necessary, then sets the last cell of the list to the void* cell input parameter. The enlarge_list function will grow the list by 1 each time it's called, using the handy realloc function. Now let's write the free functions for our two new node types: src/parser/parsetree.c +static void free_insert_stmt(InsertStmt* ins) { + if (ins == NULL) return; + + if (ins->name != NULL) free(ins->name); +} + +static void free_selectstmt(SelectStmt* s) { + if (s == NULL) return; + + if (s->targetList != NULL) { + free_parselist(s->targetList); + free(s->targetList); + } +} Pretty straightforward, simply call free() on anything that needs to be free'd. Next, we'll add a few switch/case branches in the big free_node function: src/parser/parsetree.c void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; case T_InsertStmt: free_insert_stmt((InsertStmt*)n); break; + case T_SelectStmt: + free_selectstmt((SelectStmt*)n); + break; + case T_ParseList: + free_parselist((ParseList*)n); + break; + case T_ResTarget: + free_restarget((ResTarget*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } Nothing interesting to see here either. The last thing we need to do is write print_ functions for the new nodes: src/parser/parsetree.c +static void print_restarget(ResTarget* r) { + printf(\"%s\", r->name); +} + +static void print_selectstmt(SelectStmt* s) { + printf(\"= Type: Select\\n\"); + printf(\"= Targets:\\n\"); + + if (s->targetList == NULL || s->targetList->length == 0) { + printf(\"= (none)\\n\"); + return; + } + + for (int i = 0; i < s->targetList->length; i++) { + printf(\"= \"); + print_restarget((ResTarget*)s->targetList->elements[i].ptr); + printf(\"\\n\"); + } +} + void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); break; + case T_SelectStmt: + print_selectstmt((SelectStmt*)n); + break; default: printf(\"print_node() | unknown node type\\n\"); } } Again, not particularly interesting. But at least we're done writing code. Let's run the program and see how we did. Running the Program $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select foo, bar, baz; ====== Node ====== = Type: Select = Targets: = foo = bar = baz bql > And our parser was able to parse each of our target columns as expected.","title":"Parser Refactor - Select"},{"location":"05-selecting-data/01-parser-refactor-select/#parser-refactor-select","text":"Now that we're able to insert data into our database, we need to add some functionality that can \"Select\" data back out. We'll start by defining our CLI syntax, then update the lexer/parser to support the new grammar, and finally we'll write the supporting code in our parsetree header and code files. The syntax will be pretty simple: bql > select [colname], [colname], ... ; We look for the SELECT keyword, followed by a comma-separated list of columns we want to select, then ending with a semi-colon. The column names will not be surrounded by quotes - they'll be a class of lexer tokens called IDENTIFIERS . The list of columns can be as long as the user wants, so our parser needs to be smart enough to support a potentially infinite list. And because it can grow forever, we need to introduce a token that tells the parser it's time to stop: the semi-colon.","title":"Parser Refactor - Select"},{"location":"05-selecting-data/01-parser-refactor-select/#lexer","text":"Starting off simple, the lexer updates are pretty small. We need to add two new match patterns - one for the two punctuation characters, and another for identifiers. src/parser/scan.l + /* operators */ +[,;] { return yytext[0]; } /* strings */ '(\\\\.|''|[^'\\n])*' { yylval->str = strdup(yytext); return STRING; } + /* identifiers */ +[A-Za-z_][A-Za-z0-9_]* { yylval->str = strdup(yytext); return IDENT; } The comma and semi-colon are very straightforward - just those literal characters. Identifiers are also fairly simple. We want any alphanumeric text without whitespace as long as it doesn't begin with a number. Easy, right?","title":"Lexer"},{"location":"05-selecting-data/01-parser-refactor-select/#grammar","text":"Now let's check out the grammar changes. src/parser/gram.y %union { char* str; int intval; struct Node* node; + struct ParseList* list; } %parse-param { struct Node** n } %param { void* scanner } -%token <str> SYS_CMD STRING +%token <str> SYS_CMD STRING IDENT %token <intval> INTNUM /* reserved keywords in alphabetical order */ %token INSERT %token SELECT -%type <node> cmd stmt sys_cmd select_stmt insert_stmt +%type <node> cmd stmt sys_cmd select_stmt insert_stmt target + +%type <list> target_list %start query We add a new ParseList struct to the union, which will enable us to make lists like the comma-separated list of columns our CLI will accept. We'll cover the details of the struct in the next section. We also need to add a new token for the identifiers our lexer is now able to match. And closing out the definitions section, we have two new grammar rules: target and target_list . As you can probably guess, the target_list is going to be a ParseList to target s. src/parser/gram.y -cmd: stmt +cmd: stmt ';' | sys_cmd ; At the top, we add the terminator semi-colon to the cmd rule when it follows a stmt . With this change we'll also need to end our insert statements with a semi-colon in order for the parser to correctly identify them. sys_cmd s will not need a semi-colon. src/parser/gram.y -select_stmt: SELECT { +select_stmt: SELECT target_list { - printf(\"SELECT command received\\n\"); - $$ = NULL; + SelectStmt* s = create_node(SelectStmt); + s->targetList = $2; + $$ = (Node*)s; + } + ; + +target_list: target { + $$ = create_parselist($1); + } + | target_list ',' target { + $$ = parselist_append($1, $3); + } + ; + +target: IDENT { + ResTarget* r = create_node(ResTarget); + r->name = $1; + $$ = (Node*)r; + } + ; Don't spend too much time worring about new structs/functions you see like ResTarget or create_parselist - I'll cover those below. Instead pay attention to how the grammar is written. Here we have refactored the syntax rules for our select_stmt . We still begin by looking for the SELECT keyword, but now we also require a target_list , which is exactly what it sounds like - a list of targets . Note the clever grammar definition we use here - a valid target_list is either a single target or a target_list followed by another target . This means bison can keep matching target s and appending them to the list until the cows come home (or flex sends a semi-colon). The target itself is just an IDENT token sent by flex, whose value we stuff into a ResTarget node.","title":"Grammar"},{"location":"05-selecting-data/01-parser-refactor-select/#parsetree-header","text":"These updates to our parser require a fairly significant amount of new code in the parsetree.c file, but first let's go over the changes in the header file. src/include/parser/parsetree.h typedef enum NodeTag { T_SysCmd, - T_InsertStmt + T_InsertStmt, + T_SelectStmt, + T_ParseList, + T_ResTarget } NodeTag; We add three new node types to our library of nodes. The SelectStmt , which we knew was coming, the ParseList , and the ResTarget . The ParseList is just a generic list struct our parser will use to store anything we need to keep in a list. ResTarget - kind of short for \"Result Target\" is a node that stores information about the column our select statement is targeting. src/include/parser/parsetree.h typedef struct Node { NodeTag type; } Node; +typedef struct ParseCell { + void* ptr; +} ParseCell; + +typedef struct ParseList { + NodeTag type; + int length; + int maxLength; + ParseCell* elements; +} ParseList; + typedef struct SysCmd { NodeTag type; char* cmd; } SysCmd; We add structs for the ParseList , which keeps an array of ParseCell s. Most often, we'll be using the ParseCell to store a Node , but we define its property to be a generic void* so that we can store whatever we want in it. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int personId; char* name; } InsertStmt; +typedef struct ResTarget { + NodeTag type; + char* name; +} ResTarget; + +typedef struct SelectStmt { + NodeTag type; + ParseList* targetList; +} SelectStmt; Next, we define the ResTarget and SelectStmt nodes. As mentioned above, the ResTarget just stores the name of the column we're \"select\"ing. And the SelectStmt stores a list of ResTarget s. src/include/parser/parsetree.h +#define parselist_make_ptr_cell(v) ((ParseCell) {.ptr = (v)}) + +#define create_parselist(li) new_parselist(parselist_make_ptr_cell(li)) void free_node(Node* n); void print_node(Node* n); char* str_strip_quotes(char* str); +ParseList* new_parselist(ParseCell li); +void free_parselist(ParseList* l); +ParseList* parselist_append(ParseList* l, void* cell); Lastly, we have a couple new macros and a few new functions. Similar to the create_node macro, I wrote the create_parselist macro to simplify list creation. It takes anything as input and first creates a new ParseList of length 1, then it creates a ParseCell and stores whatever we passed in ( li ) in the .ptr field of ParseCell and populates the 1-length array with that ParseCell . Next up, we have three common \"list\" functions: the allocate/free function pair, and a function to append an item to the list.","title":"Parsetree Header"},{"location":"05-selecting-data/01-parser-refactor-select/#parsetree-code","text":"We have a lot of new code in this file, and it's boring code too, so bear with me. src/parser/parsetree.c +ParseList* new_parselist(ParseCell li) { + ParseList* l = malloc(sizeof(ParseList)); + ParseCell* elements = malloc(sizeof(ParseCell)); + elements[0] = li; + + l->type = T_ParseList; + l->length = 1; + l->maxLength = 1; + l->elements = elements; + + return l; +} + +void free_parselist(ParseList* l) { + if (l != NULL) { + for (int i = 0; i < l->length; i++) { + if (&(l->elements[i]) != NULL) { + free_node((Node*)l->elements[i].ptr); + } + } + + free(l->elements); + } +} Starting with the allocator/free pair of functions. The allocator grabs enough memory for the list struct and a list with a single item - the ParseCell input parameter. We populate the first item of the list and set the list metadata fields. Remember, a ParseList is also a Node , so it needs to have a valid NodeTag in its type property. The free function is standard list free logic. We loop through the elements array and call free_node on the .ptr of each ParseCell . src/parser/parsetree.c +static inline ParseCell* parselist_last_cell(const ParseList* l) { + return &l->elements[l->length]; +} + +#define lcptr(lc) ((lc)->ptr) +#define llast(l) lcptr(parselist_last_cell(l)) + +ParseList* parselist_append(ParseList* l, void* cell) { + if (l->length >= l->maxLength) { + enlarge_list(l); + } + + llast(l) = cell; + + l->length++; + + return l; +} + +static void enlarge_list(ParseList* list) { + if (list == NULL) { + printf(\"ERROR: list is empty!\\n\"); + exit(EXIT_FAILURE); + } else { + if (list->elements == NULL) { + list->elements = malloc(sizeof(ParseCell)); + } else { + list->elements = realloc(list->elements, (list->length + 1) * sizeof(ParseCell)); + } + + list->maxLength++; + } +} The parselist_last_cell and two macros might look confusing, but it's just an easy way to set the last element in a list. Our append function grows the list if necessary, then sets the last cell of the list to the void* cell input parameter. The enlarge_list function will grow the list by 1 each time it's called, using the handy realloc function. Now let's write the free functions for our two new node types: src/parser/parsetree.c +static void free_insert_stmt(InsertStmt* ins) { + if (ins == NULL) return; + + if (ins->name != NULL) free(ins->name); +} + +static void free_selectstmt(SelectStmt* s) { + if (s == NULL) return; + + if (s->targetList != NULL) { + free_parselist(s->targetList); + free(s->targetList); + } +} Pretty straightforward, simply call free() on anything that needs to be free'd. Next, we'll add a few switch/case branches in the big free_node function: src/parser/parsetree.c void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; case T_InsertStmt: free_insert_stmt((InsertStmt*)n); break; + case T_SelectStmt: + free_selectstmt((SelectStmt*)n); + break; + case T_ParseList: + free_parselist((ParseList*)n); + break; + case T_ResTarget: + free_restarget((ResTarget*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } Nothing interesting to see here either. The last thing we need to do is write print_ functions for the new nodes: src/parser/parsetree.c +static void print_restarget(ResTarget* r) { + printf(\"%s\", r->name); +} + +static void print_selectstmt(SelectStmt* s) { + printf(\"= Type: Select\\n\"); + printf(\"= Targets:\\n\"); + + if (s->targetList == NULL || s->targetList->length == 0) { + printf(\"= (none)\\n\"); + return; + } + + for (int i = 0; i < s->targetList->length; i++) { + printf(\"= \"); + print_restarget((ResTarget*)s->targetList->elements[i].ptr); + printf(\"\\n\"); + } +} + void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); break; + case T_SelectStmt: + print_selectstmt((SelectStmt*)n); + break; default: printf(\"print_node() | unknown node type\\n\"); } } Again, not particularly interesting. But at least we're done writing code. Let's run the program and see how we did.","title":"Parsetree Code"},{"location":"05-selecting-data/01-parser-refactor-select/#running-the-program","text":"$ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select foo, bar, baz; ====== Node ====== = Type: Select = Targets: = foo = bar = baz bql > And our parser was able to parse each of our target columns as expected.","title":"Running the Program"},{"location":"05-selecting-data/02-the-sql-analyzer/","text":"The SQL Analyzer I want to take some time to introduce the SQL analyzer before moving forward with code changes in this section. In modern DBMS's, every query you send to it passes through a handful of stages before you get your results back. You can often generalize them in five stages: Lexer/parser Analyzer Rewriter Planner Executor Right now, we are firmly planted in the lexer/parser (commonly called the parser) stage. This is what we're slowly building up with our flex and bison code. Its sole responsibility is to validate the syntax of the SQL query you give to it. The parser doesn't care if you reference tables/columns/etc that don't actually exist, it just cares that you follow the grammar rules of the query language. As the parser is validating syntax, it is building a parse tree - a type of abstract syntax tree (AST). This is what we're doing with our variety of Node data types. When the parser is done, it sends the parse tree to the analyzer where we perform semantic analysis. All it means is we check to make sure any tables or columns referenced in the query actually exist in the database, and ensure all comparisons or operations on certain data types are allowed - things like that. At this point, we can't write a true analyzer because in order to do its job, it relies on a database's system tables. These are internal tables to the database engine that store metadata about tables, columns, etc. that have been created by the user. Since we haven't implemented system tables yet, we'll need to hard-code a dumb analyzer. Fortunately it'll be pretty simple. src/main.c +#include <strings.h> *** other code removed for brevity *** +static bool analyze_selectstmt(SelectStmt* s) { + for (int i = 0; i < s->targetList->length; i++) { + ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; + if (!(strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0)) return false; + } + return true; +} + +static bool analyze_node(Node* n) { + switch (n->type) { + case T_SelectStmt: + return analyze_selectstmt((SelectStmt*)n); + default: + printf(\"analyze_node() | unhandled node type\"); + } + + return false; +} We just add a couple functions to our temporary code section in main.c . We have a general analyze_node function that will call the analyze_select function, which returns true or false depending on the results of our \"semantic analysis.\" All we're doing is checking if the column name in each ResTarget is either person_id or name . We use strcasecmp , which does a case-insensitive string comparison. src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); flush_page(fdesc->fd, pg); free_page(pg); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; if (!insert_record(pg, person_id, name)) { printf(\"Unable to insert record\\n\"); } + break; + case T_SelectStmt: + if (!analyze_node(n)) { + printf(\"Semantic analysis failed\\n\"); + } + break; } And in our main function we need to make a call to the analyzer. If analysis fails, we simply print a very vague and unhelpful error message - hey that's exactly what all the other SQL parsers/analyzers do too! Running the Program $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name; ====== Node ====== = Type: Select = Targets: = person_id = name bql > select foo, bar, baz; ====== Node ====== = Type: Select = Targets: = foo = bar = baz Semantic analysis failed bql > Here I tried two select statements. The first one is valid, selecting both of the column names in our hard-coded table. The second gets rejected because none of my columns exist in the table.","title":"The SQL Analyzer"},{"location":"05-selecting-data/02-the-sql-analyzer/#the-sql-analyzer","text":"I want to take some time to introduce the SQL analyzer before moving forward with code changes in this section. In modern DBMS's, every query you send to it passes through a handful of stages before you get your results back. You can often generalize them in five stages: Lexer/parser Analyzer Rewriter Planner Executor Right now, we are firmly planted in the lexer/parser (commonly called the parser) stage. This is what we're slowly building up with our flex and bison code. Its sole responsibility is to validate the syntax of the SQL query you give to it. The parser doesn't care if you reference tables/columns/etc that don't actually exist, it just cares that you follow the grammar rules of the query language. As the parser is validating syntax, it is building a parse tree - a type of abstract syntax tree (AST). This is what we're doing with our variety of Node data types. When the parser is done, it sends the parse tree to the analyzer where we perform semantic analysis. All it means is we check to make sure any tables or columns referenced in the query actually exist in the database, and ensure all comparisons or operations on certain data types are allowed - things like that. At this point, we can't write a true analyzer because in order to do its job, it relies on a database's system tables. These are internal tables to the database engine that store metadata about tables, columns, etc. that have been created by the user. Since we haven't implemented system tables yet, we'll need to hard-code a dumb analyzer. Fortunately it'll be pretty simple. src/main.c +#include <strings.h> *** other code removed for brevity *** +static bool analyze_selectstmt(SelectStmt* s) { + for (int i = 0; i < s->targetList->length; i++) { + ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; + if (!(strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0)) return false; + } + return true; +} + +static bool analyze_node(Node* n) { + switch (n->type) { + case T_SelectStmt: + return analyze_selectstmt((SelectStmt*)n); + default: + printf(\"analyze_node() | unhandled node type\"); + } + + return false; +} We just add a couple functions to our temporary code section in main.c . We have a general analyze_node function that will call the analyze_select function, which returns true or false depending on the results of our \"semantic analysis.\" All we're doing is checking if the column name in each ResTarget is either person_id or name . We use strcasecmp , which does a case-insensitive string comparison. src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); flush_page(fdesc->fd, pg); free_page(pg); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; if (!insert_record(pg, person_id, name)) { printf(\"Unable to insert record\\n\"); } + break; + case T_SelectStmt: + if (!analyze_node(n)) { + printf(\"Semantic analysis failed\\n\"); + } + break; } And in our main function we need to make a call to the analyzer. If analysis fails, we simply print a very vague and unhelpful error message - hey that's exactly what all the other SQL parsers/analyzers do too!","title":"The SQL Analyzer"},{"location":"05-selecting-data/02-the-sql-analyzer/#running-the-program","text":"$ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name; ====== Node ====== = Type: Select = Targets: = person_id = name bql > select foo, bar, baz; ====== Node ====== = Type: Select = Targets: = foo = bar = baz Semantic analysis failed bql > Here I tried two select statements. The first one is valid, selecting both of the column names in our hard-coded table. The second gets rejected because none of my columns exist in the table.","title":"Running the Program"},{"location":"05-selecting-data/03-buffer-pool/","text":"Buffer Pool Before we go too far in implementing a simple query engine, we need to refactor how our program is able to interact with pages on disk. Right now, we only allow a single data page, so it's easy to pull it into memory at the beginning of the program, pass it around to any systems that need it, then flush to disk at the end. This won't work when we support multiple pages, and by then we'll have written a lot more code that works with pages so refactoring is best done early. We're going to create a mini buffer pool. What is a Buffer Pool? Relational databases are built to store and work with huge amounts of data. In such situations, the amound of data they store will far exceed the amount of memory they're given by the host machine. So in order to read and write to a lot of data pages while working within its memory constraints, they keep data pages in something called a buffer pool. A buffer pool is a shared pool of memory that keeps track of all data pages currently in memory. If some process needs to read or write from a specific data page, it will ask the buffer pool for the page. If the page is currently in memory, the buffer pool returns a pointer to the data page quick and easy. If the page is not currently in memory, the buffer pool must first determine if it has enough space to house a new page. If there is enough space, then it'll pull the page from disk into memory and return a pointer to the page. If the buffer pool does not have enough space, it need to choose a page to evict from the pool before it can read the requested page into memory. At present, our needs are not that complex, so we can get away with writing a really dumb buffer pool. But it is a solid starting point, and a good way to keep data file/page access constrained to a single subsystem. Buffer Pool Header src/include/buffer/bufpool.h typedef struct BufPoolSlot { uint32_t pageId; Page pg; } BufPoolSlot; typedef struct BufPool { FileDesc* fdesc; int size; uint32_t nextPageId; BufPoolSlot* slots; } BufPool; First we need two structs: one for the buffer pool itself and another for the page slots in the buffer pool. The BufPool struct will be responsible for reading and writing data for a single file only, hence the FileDesc pointer we store in it. It also has a fixed size, which corresponds to the number of slots we allow in the buffer pool - the slots property is an array of BufPoolSlot s. The nextPageId property tells us what pageId to set for any newly created pages. The BufPoolSlot struct represents a single page in memory. When occupied, we set the pageId to whatever page is currently in the slot. If the slot is empty, we set pageId to 0. BufPool* bufpool_init(FileDesc* fdesc, int numSlots); void bufpool_destroy(BufPool* bp); BufPoolSlot* bufpool_read_page(BufPool* bp, uint32_t pageId); BufPoolSlot* bufpool_new_page(BufPool* bp); void bufpool_flush_page(BufPool* bp, uint32_t pageId); void bufpool_flush_all(BufPool* bp); Now our functions, starting with the allocate/free pair. The allocator takes a numSlots parameter which enforces a maximum size of the buffer pool on initialization. This is kind of a hacky way we're using to limit memory consumption of our database. In reality, the rest of the program can consume as much as it wants, we're just restricting the buffer pool. The bufpool_flush_page and bufpool_read_page functions are replacements for the read/write functions we already have. But we also create bufpool_new_page available for any writer process that needs a new page in order to insert data - we currently only support one data page, but that will chagne soon. And finally, the flush_all function is what our program will call when it receives the \\quit command to make sure we're persist any in-memory changes to disk. Buffer Pool Implementation src/buffer/bufpool.c extern Config* conf; BufPool* bufpool_init(FileDesc* fdesc, int numSlots) { BufPool* bp = malloc(sizeof(BufPool)); bp->fdesc = fdesc; bp->size = numSlots; bp->slots = calloc(numSlots, sizeof(BufPoolSlot)); // initialize each slot with a blank page for (int i = 0; i < numSlots; i++) { bp->slots[i].pg = new_page(); bp->slots[i].pageId = 0; } int offset = lseek(fdesc->fd, -(conf->pageSize), SEEK_END); if (offset == -1) { bp->nextPageId = 1; } else { bp->nextPageId = (offset / conf->pageSize) + 1; } return bp; } void bufpool_destroy(BufPool* bp) { if (bp == NULL) return; if (bp->slots != NULL) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pg != NULL) free_page(bp->slots[i].pg); } free(bp->slots); } free(bp); } Starting with the allocator/free functions again. The allocator sets the fdesc and size properties according to its input parameters, then initializes the slots array with a number equal to numSlots . Then we need to initialize each of the slots with blank database pages that are available for an incoming data page being read from disk. Next, we calculate what the nextPageId should be based on how large the file is. Our file size will always be some multiple of the conf->pageSize , so the math fairly easy to do. The free function, as you'd expect, loops through the slots and frees everything we allocated in the beginning. BufPoolSlot* bufpool_read_page(BufPool* bp, uint32_t pageId) { if (pageId == 0) return NULL; BufPoolSlot* slot = bufpool_find_empty_slot(bp); if (slot == NULL) { // evict a page and return the empty slot slot = bufpool_evict_page(bp); } lseek(bp->fdesc->fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_read = read(bp->fdesc->fd, slot->pg, conf->pageSize); if (bytes_read != conf->pageSize) { printf(\"Bytes read: %d\\n\", bytes_read); return NULL; } slot->pageId = pageId; return slot; } The read_page function has more going on than you might expect. It first needs to find an unclaimed slot in the buffer pool, which we wrote a static helper function for (see below). If all slots are occupied, we need to evict a page by flushing it to disk - using another static helper function. After that, we'll have an empty slot we can use to read a page into. static BufPoolSlot* bufpool_find_empty_slot(BufPool* bp) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pageId == 0) return &bp->slots[i]; } return NULL; } static BufPoolSlot* bufpool_evict_page(BufPool* bp) { // just evict the first page for now uint32_t pageId = bp->slots[0].pageId; bufpool_flush_page(bp, pageId); bp->slots[0].pageId = 0; return &bp->slots[0]; } These helper functions are pretty basic. In the first one, we simply loop through the slots array and return the first pageId = 0 slot that we find - NULL otherwise. The evict_page function is extremely simple right now - we're just evicting the first page in the slots array. When we start supporting multiple pages, we'll come up with a better way to decide which page gets flushed. BufPoolSlot* bufpool_new_page(BufPool* bp) { BufPoolSlot* slot = bufpool_find_empty_slot(bp); slot->pageId = bp->nextPageId; bp->nextPageId++; memset(slot->pg, 0, conf->pageSize); PageHeader* pgHdr = ((PageHeader*)slot->pg); pgHdr->pageId = slot->pageId; pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); pgHdr->freeData = conf->pageSize - sizeof(PageHeader); return slot; } Our new page function takes advantage of the buffer pool's nextPageId parameter and creates a blank page in an available slot. void bufpool_flush_page(BufPool* bp, uint32_t pageId) { BufPoolSlot* slot = bufpool_find_slot_by_page_id(bp, pageId); if (slot == NULL) { printf(\"Page does not exist in the buffer pool\\n\"); return; } if (!flush_page(bp->fdesc->fd, slot->pg, pageId)) { printf(\"Flush page failure\\n\"); return; } slot->pageId = 0; } Next up, we have our flush page function. Given a pageId , it finds the slot where that page exists, then calls flush_page to write it to disk. static BufPoolSlot* bufpool_find_slot_by_page_id(BufPool* bp, uint32_t pageId) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pageId == pageId) return &bp->slots[i]; } return NULL; } static bool flush_page(int fd, Page pg, uint32_t pageId) { uint32_t headerPageId = ((PageHeader*)pg)->pageId; if (headerPageId != pageId || pageId == 0) { printf(\"Page Ids do not match\\n\"); return false; } lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_written = write(fd, pg, conf->pageSize); if (bytes_written != conf->pageSize) { printf(\"Bytes written: %d\\n\", bytes_written); return false; } return true; } Finding a slot by pageId is pretty simple - just loop through the slots until you find the matching pageId. The flush_page function is essentially the same as the one we're replacing. Move the file offset to the correct spot, then attempt to write conf->pageSize bytes to the file. void bufpool_flush_all(BufPool* bp) { for (int i = 0; i < bp->size; i++) { BufPoolSlot* slot = &bp->slots[i]; flush_page(bp->fdesc->fd, slot->pg, slot->pageId); } } Finally, we have our flush_all function. Simple loop through the slots array and calling flush_page . Removing Old Code src/include/storage/page.h -Page read_page(int fd, uint32_t pageId); -void flush_page(int fd, Page pg); We no longer need the read and flush functions we wrote in our storage engine code. src/storage/page.c -Page read_page(int fd, uint32_t pageId) { - Page pg = new_page(); - lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); - int bytes_read = read(fd, pg, conf->pageSize); - - if (bytes_read != conf->pageSize) { - printf(\"Bytes read: %d\\n\", bytes_read); - PageHeader* pgHdr = (PageHeader*)pg; - /* Since this is a brand new page, we need to set the header fields appropriately */ - pgHdr->pageId = pageId; - pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); - pgHdr->freeData = conf->pageSize - sizeof(PageHeader); - } - - return pg; -} - -void flush_page(int fd, Page pg) { - int pageId = ((PageHeader*)pg)->pageId; - lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); - int bytes_written = write(fd, pg, conf->pageSize); - - if (bytes_written != conf->pageSize) { - printf(\"Page flush unsuccessful\\n\"); - } -} Updating Main src/main.c /* TEMPORARY CODE SECTION */ #define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) +#define BUFPOOL_SLOTS 1 First we add a new macro so we can control how many buffer pool slots we want to allow. Start with one for now since our database can only work with one page anyways. -static bool insert_record(Page pg, int32_t person_id, char* name) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name) { + BufPoolSlot* slot = bufpool_read_page(bp, 1); + if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); serialize_data(rd, r, person_id, name); - bool insertSuccessful = page_insert(pg, r, RECORD_LEN); + bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Instead of passing a Page around everywhere, we're going to start passing around a pointer to the BufPool . int main(int argc, char** argv) { // initialize global config conf = new_config(); if (!set_global_config(conf)) { return EXIT_FAILURE; } // print config print_config(conf); FileDesc* fdesc = file_open(conf->dataFile); - Page pg = read_page(fdesc->fd, 1); + BufPool* bp = bufpool_init(fdesc, BUFPOOL_SLOTS); while(true) { print_prompt(); Node* n = parse_sql(); if (n == NULL) continue; print_node(n); switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); - flush_page(fdesc->fd, pg); - free_page(pg); + bufpool_flush_all(bp); + bufpool_destroy(bp); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; - if (!insert_record(pg, person_id, name)) { + if (!insert_record(bp, person_id, name)) { printf(\"Unable to insert record\\n\"); } break; case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); } break; } free_node(n); } return EXIT_SUCCESS; } First thought here: our main function is getting too big, we'll need to fix that. Second, we just have a few updates to make sure we're passing around a BufPool pointer instead of a Page . src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ storage/page.c \\ storage/record.c \\ + storage/datum.c \\ + buffer/bufpool.c The program should behave exactly the same as it did before.","title":"Buffer Pool"},{"location":"05-selecting-data/03-buffer-pool/#buffer-pool","text":"Before we go too far in implementing a simple query engine, we need to refactor how our program is able to interact with pages on disk. Right now, we only allow a single data page, so it's easy to pull it into memory at the beginning of the program, pass it around to any systems that need it, then flush to disk at the end. This won't work when we support multiple pages, and by then we'll have written a lot more code that works with pages so refactoring is best done early. We're going to create a mini buffer pool.","title":"Buffer Pool"},{"location":"05-selecting-data/03-buffer-pool/#what-is-a-buffer-pool","text":"Relational databases are built to store and work with huge amounts of data. In such situations, the amound of data they store will far exceed the amount of memory they're given by the host machine. So in order to read and write to a lot of data pages while working within its memory constraints, they keep data pages in something called a buffer pool. A buffer pool is a shared pool of memory that keeps track of all data pages currently in memory. If some process needs to read or write from a specific data page, it will ask the buffer pool for the page. If the page is currently in memory, the buffer pool returns a pointer to the data page quick and easy. If the page is not currently in memory, the buffer pool must first determine if it has enough space to house a new page. If there is enough space, then it'll pull the page from disk into memory and return a pointer to the page. If the buffer pool does not have enough space, it need to choose a page to evict from the pool before it can read the requested page into memory. At present, our needs are not that complex, so we can get away with writing a really dumb buffer pool. But it is a solid starting point, and a good way to keep data file/page access constrained to a single subsystem.","title":"What is a Buffer Pool?"},{"location":"05-selecting-data/03-buffer-pool/#buffer-pool-header","text":"src/include/buffer/bufpool.h typedef struct BufPoolSlot { uint32_t pageId; Page pg; } BufPoolSlot; typedef struct BufPool { FileDesc* fdesc; int size; uint32_t nextPageId; BufPoolSlot* slots; } BufPool; First we need two structs: one for the buffer pool itself and another for the page slots in the buffer pool. The BufPool struct will be responsible for reading and writing data for a single file only, hence the FileDesc pointer we store in it. It also has a fixed size, which corresponds to the number of slots we allow in the buffer pool - the slots property is an array of BufPoolSlot s. The nextPageId property tells us what pageId to set for any newly created pages. The BufPoolSlot struct represents a single page in memory. When occupied, we set the pageId to whatever page is currently in the slot. If the slot is empty, we set pageId to 0. BufPool* bufpool_init(FileDesc* fdesc, int numSlots); void bufpool_destroy(BufPool* bp); BufPoolSlot* bufpool_read_page(BufPool* bp, uint32_t pageId); BufPoolSlot* bufpool_new_page(BufPool* bp); void bufpool_flush_page(BufPool* bp, uint32_t pageId); void bufpool_flush_all(BufPool* bp); Now our functions, starting with the allocate/free pair. The allocator takes a numSlots parameter which enforces a maximum size of the buffer pool on initialization. This is kind of a hacky way we're using to limit memory consumption of our database. In reality, the rest of the program can consume as much as it wants, we're just restricting the buffer pool. The bufpool_flush_page and bufpool_read_page functions are replacements for the read/write functions we already have. But we also create bufpool_new_page available for any writer process that needs a new page in order to insert data - we currently only support one data page, but that will chagne soon. And finally, the flush_all function is what our program will call when it receives the \\quit command to make sure we're persist any in-memory changes to disk.","title":"Buffer Pool Header"},{"location":"05-selecting-data/03-buffer-pool/#buffer-pool-implementation","text":"src/buffer/bufpool.c extern Config* conf; BufPool* bufpool_init(FileDesc* fdesc, int numSlots) { BufPool* bp = malloc(sizeof(BufPool)); bp->fdesc = fdesc; bp->size = numSlots; bp->slots = calloc(numSlots, sizeof(BufPoolSlot)); // initialize each slot with a blank page for (int i = 0; i < numSlots; i++) { bp->slots[i].pg = new_page(); bp->slots[i].pageId = 0; } int offset = lseek(fdesc->fd, -(conf->pageSize), SEEK_END); if (offset == -1) { bp->nextPageId = 1; } else { bp->nextPageId = (offset / conf->pageSize) + 1; } return bp; } void bufpool_destroy(BufPool* bp) { if (bp == NULL) return; if (bp->slots != NULL) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pg != NULL) free_page(bp->slots[i].pg); } free(bp->slots); } free(bp); } Starting with the allocator/free functions again. The allocator sets the fdesc and size properties according to its input parameters, then initializes the slots array with a number equal to numSlots . Then we need to initialize each of the slots with blank database pages that are available for an incoming data page being read from disk. Next, we calculate what the nextPageId should be based on how large the file is. Our file size will always be some multiple of the conf->pageSize , so the math fairly easy to do. The free function, as you'd expect, loops through the slots and frees everything we allocated in the beginning. BufPoolSlot* bufpool_read_page(BufPool* bp, uint32_t pageId) { if (pageId == 0) return NULL; BufPoolSlot* slot = bufpool_find_empty_slot(bp); if (slot == NULL) { // evict a page and return the empty slot slot = bufpool_evict_page(bp); } lseek(bp->fdesc->fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_read = read(bp->fdesc->fd, slot->pg, conf->pageSize); if (bytes_read != conf->pageSize) { printf(\"Bytes read: %d\\n\", bytes_read); return NULL; } slot->pageId = pageId; return slot; } The read_page function has more going on than you might expect. It first needs to find an unclaimed slot in the buffer pool, which we wrote a static helper function for (see below). If all slots are occupied, we need to evict a page by flushing it to disk - using another static helper function. After that, we'll have an empty slot we can use to read a page into. static BufPoolSlot* bufpool_find_empty_slot(BufPool* bp) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pageId == 0) return &bp->slots[i]; } return NULL; } static BufPoolSlot* bufpool_evict_page(BufPool* bp) { // just evict the first page for now uint32_t pageId = bp->slots[0].pageId; bufpool_flush_page(bp, pageId); bp->slots[0].pageId = 0; return &bp->slots[0]; } These helper functions are pretty basic. In the first one, we simply loop through the slots array and return the first pageId = 0 slot that we find - NULL otherwise. The evict_page function is extremely simple right now - we're just evicting the first page in the slots array. When we start supporting multiple pages, we'll come up with a better way to decide which page gets flushed. BufPoolSlot* bufpool_new_page(BufPool* bp) { BufPoolSlot* slot = bufpool_find_empty_slot(bp); slot->pageId = bp->nextPageId; bp->nextPageId++; memset(slot->pg, 0, conf->pageSize); PageHeader* pgHdr = ((PageHeader*)slot->pg); pgHdr->pageId = slot->pageId; pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); pgHdr->freeData = conf->pageSize - sizeof(PageHeader); return slot; } Our new page function takes advantage of the buffer pool's nextPageId parameter and creates a blank page in an available slot. void bufpool_flush_page(BufPool* bp, uint32_t pageId) { BufPoolSlot* slot = bufpool_find_slot_by_page_id(bp, pageId); if (slot == NULL) { printf(\"Page does not exist in the buffer pool\\n\"); return; } if (!flush_page(bp->fdesc->fd, slot->pg, pageId)) { printf(\"Flush page failure\\n\"); return; } slot->pageId = 0; } Next up, we have our flush page function. Given a pageId , it finds the slot where that page exists, then calls flush_page to write it to disk. static BufPoolSlot* bufpool_find_slot_by_page_id(BufPool* bp, uint32_t pageId) { for (int i = 0; i < bp->size; i++) { if (bp->slots[i].pageId == pageId) return &bp->slots[i]; } return NULL; } static bool flush_page(int fd, Page pg, uint32_t pageId) { uint32_t headerPageId = ((PageHeader*)pg)->pageId; if (headerPageId != pageId || pageId == 0) { printf(\"Page Ids do not match\\n\"); return false; } lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); int bytes_written = write(fd, pg, conf->pageSize); if (bytes_written != conf->pageSize) { printf(\"Bytes written: %d\\n\", bytes_written); return false; } return true; } Finding a slot by pageId is pretty simple - just loop through the slots until you find the matching pageId. The flush_page function is essentially the same as the one we're replacing. Move the file offset to the correct spot, then attempt to write conf->pageSize bytes to the file. void bufpool_flush_all(BufPool* bp) { for (int i = 0; i < bp->size; i++) { BufPoolSlot* slot = &bp->slots[i]; flush_page(bp->fdesc->fd, slot->pg, slot->pageId); } } Finally, we have our flush_all function. Simple loop through the slots array and calling flush_page .","title":"Buffer Pool Implementation"},{"location":"05-selecting-data/03-buffer-pool/#removing-old-code","text":"src/include/storage/page.h -Page read_page(int fd, uint32_t pageId); -void flush_page(int fd, Page pg); We no longer need the read and flush functions we wrote in our storage engine code. src/storage/page.c -Page read_page(int fd, uint32_t pageId) { - Page pg = new_page(); - lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); - int bytes_read = read(fd, pg, conf->pageSize); - - if (bytes_read != conf->pageSize) { - printf(\"Bytes read: %d\\n\", bytes_read); - PageHeader* pgHdr = (PageHeader*)pg; - /* Since this is a brand new page, we need to set the header fields appropriately */ - pgHdr->pageId = pageId; - pgHdr->freeBytes = conf->pageSize - sizeof(PageHeader); - pgHdr->freeData = conf->pageSize - sizeof(PageHeader); - } - - return pg; -} - -void flush_page(int fd, Page pg) { - int pageId = ((PageHeader*)pg)->pageId; - lseek(fd, (pageId - 1) * conf->pageSize, SEEK_SET); - int bytes_written = write(fd, pg, conf->pageSize); - - if (bytes_written != conf->pageSize) { - printf(\"Page flush unsuccessful\\n\"); - } -}","title":"Removing Old Code"},{"location":"05-selecting-data/03-buffer-pool/#updating-main","text":"src/main.c /* TEMPORARY CODE SECTION */ #define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) +#define BUFPOOL_SLOTS 1 First we add a new macro so we can control how many buffer pool slots we want to allow. Start with one for now since our database can only work with one page anyways. -static bool insert_record(Page pg, int32_t person_id, char* name) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name) { + BufPoolSlot* slot = bufpool_read_page(bp, 1); + if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); serialize_data(rd, r, person_id, name); - bool insertSuccessful = page_insert(pg, r, RECORD_LEN); + bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Instead of passing a Page around everywhere, we're going to start passing around a pointer to the BufPool . int main(int argc, char** argv) { // initialize global config conf = new_config(); if (!set_global_config(conf)) { return EXIT_FAILURE; } // print config print_config(conf); FileDesc* fdesc = file_open(conf->dataFile); - Page pg = read_page(fdesc->fd, 1); + BufPool* bp = bufpool_init(fdesc, BUFPOOL_SLOTS); while(true) { print_prompt(); Node* n = parse_sql(); if (n == NULL) continue; print_node(n); switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); - flush_page(fdesc->fd, pg); - free_page(pg); + bufpool_flush_all(bp); + bufpool_destroy(bp); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; - if (!insert_record(pg, person_id, name)) { + if (!insert_record(bp, person_id, name)) { printf(\"Unable to insert record\\n\"); } break; case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); } break; } free_node(n); } return EXIT_SUCCESS; } First thought here: our main function is getting too big, we'll need to fix that. Second, we just have a few updates to make sure we're passing around a BufPool pointer instead of a Page . src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ storage/page.c \\ storage/record.c \\ + storage/datum.c \\ + buffer/bufpool.c The program should behave exactly the same as it did before.","title":"Updating Main"},{"location":"05-selecting-data/04-table-scan/","text":"Table Scan We're able to insert data into our database, but that's effectively useless if we don't have any way to read it back out. Our parser can handle basic select statements, so now we need to write the code that reads through a table's data pages and returns an organized list or row/column data. We're going to start out super basic and implement a full table scan. That's where our engine starts at the data page and reads all data rows until it runs out of pages. So what information does our table scan function need to do its job? For starters, it needs a pointer to the BufPool so that it can request data pages be read into memory. It will also need a RecordDescriptor about the table its scanning so that it can correctly parse the bytes stored in each row. And lastly, it needs somewhere to dump the parsed data - we'll be using a doubly linked list. Fair warning: we're going to be writing a lot of code and it's going to be all over the place. Table Access Method With this new table scan functionality, I am starting a new folder for files containing \"access\" methods. I.e. since we're scanning a table is a way of \"accessing\" its data, so the code belongs to the \"access\" section of the codebase. src/include/access/tableam.h #include \"storage/table.h\" #include \"buffer/bufpool.h\" #include \"utility/linkedlist.h\" void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows); Pretty basic stuff here. No structs, just a single function for now. As mentioned above, our table scan method needs access to the buffer pool, metadata about the table and its contents ( TableDesc ), and a landing zone for the data it reads from the data pages. Now, what the hell is a TableDesc ? src/include/storage/table.h #include \"storage/record.h\" typedef struct TableDesc { char* tablename; RecordDescriptor* rd; } TableDesc; TableDesc* new_tabledesc(char* tablename); void free_tabledesc(TableDesc* td); It's just a RecordDescriptor that also contains the table's name. In this header file, we also define the standard allocator/free pair of functions. Let's get them out of the way real quick: src/storage/table.c TableDesc* new_tabledesc(char* tablename) { TableDesc* td = malloc(sizeof(TableDesc)); td->tablename = tablename; td->rd = NULL; return td; } void free_tabledesc(TableDesc* td) { if (td == NULL) return; // if (td->tablename != NULL) free(td->tablename); if (td->rd != NULL) free_record_desc(td->rd); free(td); } Again, these are pretty standard memory management functions. Right now I have that line commented out in the free_tabledesc function because our table is hard-coded. Meaning when we create a new TableDesc , the tablename parameter is hard-coded in our source code, i.e. not malloc 'd, so there is nothing to free. Later on, when we support more than one table, this will get uncommented. Alright, back to the table scan function. The third, and last, parameter is a LinkedList . Let's check it out: src/include/utility/linkedlist.h #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct ListItem { void* ptr; struct ListItem* prev; struct ListItem* next; } ListItem; typedef struct LinkedList { int numItems; ListItem* head; ListItem* tail; } LinkedList; LinkedList* new_linkedlist(); void free_linkedlist(LinkedList* l); void linkedlist_append(LinkedList* l, void* ptr); This is a standard linked list implementation. We store previous and next pointers in each ListItem , and the head and tail of the list in the LinkedList struct. We also have the allocator and free functions, along with an append function. We currently don't need a delete_item function, so I didn't write one. And I am disabling memory alignment on the ListItem because we store Datum arrays in this list and I got a lot of read access errors. The #pragma here gets rid of those. A reminder, I am not an expert at C - just stupid enough to get something working. Anyways, let's look at how the linked list is implemented: src/utility/linkedlist.c LinkedList* new_linkedlist() { LinkedList* l = malloc(sizeof(LinkedList)); l->numItems = 0; l->head = NULL; l->tail = NULL; return l; } void free_linkedlist(LinkedList* l) { if (l == NULL) return; if (l->head != NULL) { ListItem* li = l->head; while (li != NULL) { ListItem* curr = li; li = curr->next; free(curr); } } free(l); } void linkedlist_append(LinkedList* l, void* ptr) { if (l == NULL) { printf(\"Error: tried to append to a NULL LinkedList\"); return; } ListItem* new = malloc(sizeof(ListItem)); new->ptr = ptr; new->next = NULL; new->prev = NULL; if (l->numItems == 0) { l->head = new; } else { ListItem* tail = l->tail; tail->next = new; new->prev = tail; } l->tail = new; l->numItems++; } The allocator is pretty basic - just initializing its properties. The free function loops through the entire list and frees the memory it has consumed. Super Important Note : since this is a generic linked list, the caller is responsible for freeing whatever is stored in the ListItem->ptr memory blocks before calling free_linkedlist . The append function is also pretty standard. We just create a new ListItem and add it to the end of the list, updating the linking chain as needed. Okay, that takes care of everything we need to know about the inputs to our table scan function. Now let's take a look at the function itself. Table Scan Function src/access/tableam.c extern Config* conf; void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows) { BufPoolSlot* slot = bufpool_read_page(bp, 1); while (slot != NULL) { PageHeader* pgHdr = (PageHeader*)slot->pg; int numRecords = pgHdr->numRecords; for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); defill_record(td->rd, slot->pg + sp->offset, row->values); linkedlist_append(rows, row); } slot = bufpool_read_page(bp, pgHdr->nextPageId); } } The basic workflow of a table scan is as follows: Request the first page of the table from the buffer pool Determine the number or records on the page from the header field numRecords Loop through the slot array, finding the location of each record Extract the data from each record ( defill_record ) into a Datum array Append the Datum array to the rows linked list Repeat until we hit the last page in the table At the top, we request pageId 1 from the buffer pool - this is hard-coded right now, but will change when we support multiple tables. Then we enter a loop that continues until we hit the end of the data pages for our table. Inside the while loop, we grab the number of records from the header, which is also how many slot pointers there are in the slot array. From here, we loop through each record and extract each column into a Datum array via defill_record and then append it to our results linked list. RecordSet Let's dive in to some of the new code, starting with the new RecordSetRow data type: src/include/resultset/recordset.h #include \"storage/record.h\" #include \"utility/linkedlist.h\" typedef struct RecordSetRow { Datum* values; } RecordSetRow; typedef struct RecordSet { LinkedList* rows; } RecordSet; RecordSet* new_recordset(); void free_recordset(RecordSet* rs, RecordDescriptor* rd); RecordSetRow* new_recordset_row(int ncols); void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd); Two structs, and their associated allocator/free functions. Right now, RecordSet might seem like a pointless wrapper around a LinkedList , but it will become more complex and necessary as our program evolves. Same thing goes for the RecordSetRow . And as you can probably guess, the linked list in RecordSet will be filled with RecordSetRow items as we scan the table data. The free functions here need to take a RecordDescriptor as input because they'll be responsible for freeing each Datum inside the Datum array. And in order to do so, we need to know what kinds of data are stored in the Datum array. src/resultset/recordset.c RecordSet* new_recordset() { RecordSet* rs = malloc(sizeof(RecordSet)); rs->rows = NULL; return rs; } static void free_recordset_row_columns(RecordSetRow* row, RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].dataType == DT_CHAR) { free((void*)row->values[i]); } } } static void free_recordset_row_list(LinkedList* rows, RecordDescriptor* rd) { ListItem* li = rows->head; while (li != NULL) { free_recordset_row((RecordSetRow*)li->ptr, rd); li = li->next; } } void free_recordset(RecordSet* rs, RecordDescriptor* rd) { if (rs == NULL) return; if (rs->rows != NULL) { free_recordset_row_list(rs->rows, rd); free_linkedlist(rs->rows); } free(rs); } RecordSetRow* new_recordset_row(int ncols) { RecordSetRow* row = malloc(sizeof(RecordSetRow)); row->values = malloc(ncols * sizeof(Datum)); return row; } void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd) { if (row->values != NULL) { free_recordset_row_columns(row, rd); free(row->values); } free(row); } And here's the code for our RecordSet structs. Nothing too complex is going on here - just allocating memory and freeing memory as needed. The only noteworthy function is free_recordset_row_columns . This is where we use the information stored in the RecordDescriptor to free data inside the Datum array. Basic types like ints do not need to be free'd, but char* 's do. So we loop through the column metadata and call free on any DT_CHAR data types in the Datum array. defill_record Now, let's continue along our table scan function. Right now we're inside the for loop and we've just allocated a new RecordSetRow . Here's the relevant section for reference: src/access/tableam.c ( tableam_fullscan() ) for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); defill_record(td->rd, slot->pg + sp->offset, row->values); linkedlist_append(rows, row); } Our next step is to grab the next SlotPointer . Remember, the slot array grows from the end of the page towards the beginning. So, the first slot pointer is the very last 4 bytes on the page, the next one is the 4 bytes preceeding it, and so on. We can get the byte position by using this formula: conf->pageSize - (sizeof(SlotPointer) * (i + 1)) . Once we have its location, we can create a SlotPointer* object for easy reference. Now, we call the workhorse of our table scan method. defill_record uses the information in the RecordDescriptor and marches through the bytes where the data record are stored and extracts them into the Datum array we give to it. We define defill_record in the same place we defined fill_record . src/include/storage/record.h void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); void fill_record(RecordDescriptor* rd, Record r, Datum* data); +void defill_record(RecordDescriptor* rd, Record r, Datum* values); #endif /* RECORD_H */ And here's the code: src/storage/record.c +void defill_record(RecordDescriptor* rd, Record r, Datum* values) { + int offset = sizeof(RecordHeader); + for (int i = 0; i < rd->ncols; i++) { + Column* col = &rd->cols[i]; + values[i] = record_get_col_value(col, r, &offset); + } +} Pretty simple, right? We loop through the columns in our RecordDescriptor and extract each column's value, via a static helper function, into the Datum array. Note that we're passing &offset into the helper function. We want offset to change as we progress through the loop, so we need to pass in the address of the variable if we want the inner function to be able to perform persistent operations on its value. I should mention right now we COULD just perform offset += col->len; at the end of the for loop and be good. However, this method will not work when we get to variable-length columns. The Column object in the RecordDescriptor won't be able to store those lengths because they could be different for each row. The only way to find the length of those columns is to dig further into the Record data, which happens inside these helper functions. Thus, we make them responsible for incrementing offset . src/storage/record.c +static Datum record_get_col_value(Column* col, Record r, int* offset) { + switch (col->dataType) { + case DT_INT: + return record_get_int(r, offset); + case DT_CHAR: + return record_get_char(r, offset, col->len); + default: + printf(\"record_get_col_value() | Unknown data type!\\n\"); + return (Datum)NULL; + } +} This function is a wrapper around type-specific data extraction. As you can imagine, we'll be adding more case statements to this function as we expand our list of supported data types. src/storage/record.c +static Datum record_get_int(Record r, int* offset) { + int32_t intVal; + memcpy(&intVal, r + *offset, 4); + *offset += 4; + return int32GetDatum(intVal); +} + +static Datum record_get_char(Record r, int* offset, int charLen) { + char* pChar = malloc(charLen + 1); + memcpy(pChar, r + *offset, charLen); + pChar[charLen] = '\\0'; + *offset += charLen; + return charGetDatum(pChar); +} And here is where we do the actual data extraction. We create a variable of the correct data type, then memcpy the contents of the Record into that variable. Increment offset by dereferencing it, then return the Datum representation of the value. We have additional complexity for the record_get_char function. When we insert DT_CHAR data, we only store the relevant characters - we DO NOT store the null terminator. So when we extract the value, we need to malloc(charLen + 1) to make sure there's enough memory to append the null terminator to the end of the string. And that's all the code we need to perform a table scan on our table. Unfortunately, it's not enough to get our program to do anything useful just yet. I feel like this section has gone on long enough, so we'll round out the whole process in the next section.","title":"Table Scan"},{"location":"05-selecting-data/04-table-scan/#table-scan","text":"We're able to insert data into our database, but that's effectively useless if we don't have any way to read it back out. Our parser can handle basic select statements, so now we need to write the code that reads through a table's data pages and returns an organized list or row/column data. We're going to start out super basic and implement a full table scan. That's where our engine starts at the data page and reads all data rows until it runs out of pages. So what information does our table scan function need to do its job? For starters, it needs a pointer to the BufPool so that it can request data pages be read into memory. It will also need a RecordDescriptor about the table its scanning so that it can correctly parse the bytes stored in each row. And lastly, it needs somewhere to dump the parsed data - we'll be using a doubly linked list. Fair warning: we're going to be writing a lot of code and it's going to be all over the place.","title":"Table Scan"},{"location":"05-selecting-data/04-table-scan/#table-access-method","text":"With this new table scan functionality, I am starting a new folder for files containing \"access\" methods. I.e. since we're scanning a table is a way of \"accessing\" its data, so the code belongs to the \"access\" section of the codebase. src/include/access/tableam.h #include \"storage/table.h\" #include \"buffer/bufpool.h\" #include \"utility/linkedlist.h\" void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows); Pretty basic stuff here. No structs, just a single function for now. As mentioned above, our table scan method needs access to the buffer pool, metadata about the table and its contents ( TableDesc ), and a landing zone for the data it reads from the data pages. Now, what the hell is a TableDesc ? src/include/storage/table.h #include \"storage/record.h\" typedef struct TableDesc { char* tablename; RecordDescriptor* rd; } TableDesc; TableDesc* new_tabledesc(char* tablename); void free_tabledesc(TableDesc* td); It's just a RecordDescriptor that also contains the table's name. In this header file, we also define the standard allocator/free pair of functions. Let's get them out of the way real quick: src/storage/table.c TableDesc* new_tabledesc(char* tablename) { TableDesc* td = malloc(sizeof(TableDesc)); td->tablename = tablename; td->rd = NULL; return td; } void free_tabledesc(TableDesc* td) { if (td == NULL) return; // if (td->tablename != NULL) free(td->tablename); if (td->rd != NULL) free_record_desc(td->rd); free(td); } Again, these are pretty standard memory management functions. Right now I have that line commented out in the free_tabledesc function because our table is hard-coded. Meaning when we create a new TableDesc , the tablename parameter is hard-coded in our source code, i.e. not malloc 'd, so there is nothing to free. Later on, when we support more than one table, this will get uncommented. Alright, back to the table scan function. The third, and last, parameter is a LinkedList . Let's check it out: src/include/utility/linkedlist.h #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct ListItem { void* ptr; struct ListItem* prev; struct ListItem* next; } ListItem; typedef struct LinkedList { int numItems; ListItem* head; ListItem* tail; } LinkedList; LinkedList* new_linkedlist(); void free_linkedlist(LinkedList* l); void linkedlist_append(LinkedList* l, void* ptr); This is a standard linked list implementation. We store previous and next pointers in each ListItem , and the head and tail of the list in the LinkedList struct. We also have the allocator and free functions, along with an append function. We currently don't need a delete_item function, so I didn't write one. And I am disabling memory alignment on the ListItem because we store Datum arrays in this list and I got a lot of read access errors. The #pragma here gets rid of those. A reminder, I am not an expert at C - just stupid enough to get something working. Anyways, let's look at how the linked list is implemented: src/utility/linkedlist.c LinkedList* new_linkedlist() { LinkedList* l = malloc(sizeof(LinkedList)); l->numItems = 0; l->head = NULL; l->tail = NULL; return l; } void free_linkedlist(LinkedList* l) { if (l == NULL) return; if (l->head != NULL) { ListItem* li = l->head; while (li != NULL) { ListItem* curr = li; li = curr->next; free(curr); } } free(l); } void linkedlist_append(LinkedList* l, void* ptr) { if (l == NULL) { printf(\"Error: tried to append to a NULL LinkedList\"); return; } ListItem* new = malloc(sizeof(ListItem)); new->ptr = ptr; new->next = NULL; new->prev = NULL; if (l->numItems == 0) { l->head = new; } else { ListItem* tail = l->tail; tail->next = new; new->prev = tail; } l->tail = new; l->numItems++; } The allocator is pretty basic - just initializing its properties. The free function loops through the entire list and frees the memory it has consumed. Super Important Note : since this is a generic linked list, the caller is responsible for freeing whatever is stored in the ListItem->ptr memory blocks before calling free_linkedlist . The append function is also pretty standard. We just create a new ListItem and add it to the end of the list, updating the linking chain as needed. Okay, that takes care of everything we need to know about the inputs to our table scan function. Now let's take a look at the function itself.","title":"Table Access Method"},{"location":"05-selecting-data/04-table-scan/#table-scan-function","text":"src/access/tableam.c extern Config* conf; void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows) { BufPoolSlot* slot = bufpool_read_page(bp, 1); while (slot != NULL) { PageHeader* pgHdr = (PageHeader*)slot->pg; int numRecords = pgHdr->numRecords; for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); defill_record(td->rd, slot->pg + sp->offset, row->values); linkedlist_append(rows, row); } slot = bufpool_read_page(bp, pgHdr->nextPageId); } } The basic workflow of a table scan is as follows: Request the first page of the table from the buffer pool Determine the number or records on the page from the header field numRecords Loop through the slot array, finding the location of each record Extract the data from each record ( defill_record ) into a Datum array Append the Datum array to the rows linked list Repeat until we hit the last page in the table At the top, we request pageId 1 from the buffer pool - this is hard-coded right now, but will change when we support multiple tables. Then we enter a loop that continues until we hit the end of the data pages for our table. Inside the while loop, we grab the number of records from the header, which is also how many slot pointers there are in the slot array. From here, we loop through each record and extract each column into a Datum array via defill_record and then append it to our results linked list.","title":"Table Scan Function"},{"location":"05-selecting-data/04-table-scan/#recordset","text":"Let's dive in to some of the new code, starting with the new RecordSetRow data type: src/include/resultset/recordset.h #include \"storage/record.h\" #include \"utility/linkedlist.h\" typedef struct RecordSetRow { Datum* values; } RecordSetRow; typedef struct RecordSet { LinkedList* rows; } RecordSet; RecordSet* new_recordset(); void free_recordset(RecordSet* rs, RecordDescriptor* rd); RecordSetRow* new_recordset_row(int ncols); void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd); Two structs, and their associated allocator/free functions. Right now, RecordSet might seem like a pointless wrapper around a LinkedList , but it will become more complex and necessary as our program evolves. Same thing goes for the RecordSetRow . And as you can probably guess, the linked list in RecordSet will be filled with RecordSetRow items as we scan the table data. The free functions here need to take a RecordDescriptor as input because they'll be responsible for freeing each Datum inside the Datum array. And in order to do so, we need to know what kinds of data are stored in the Datum array. src/resultset/recordset.c RecordSet* new_recordset() { RecordSet* rs = malloc(sizeof(RecordSet)); rs->rows = NULL; return rs; } static void free_recordset_row_columns(RecordSetRow* row, RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].dataType == DT_CHAR) { free((void*)row->values[i]); } } } static void free_recordset_row_list(LinkedList* rows, RecordDescriptor* rd) { ListItem* li = rows->head; while (li != NULL) { free_recordset_row((RecordSetRow*)li->ptr, rd); li = li->next; } } void free_recordset(RecordSet* rs, RecordDescriptor* rd) { if (rs == NULL) return; if (rs->rows != NULL) { free_recordset_row_list(rs->rows, rd); free_linkedlist(rs->rows); } free(rs); } RecordSetRow* new_recordset_row(int ncols) { RecordSetRow* row = malloc(sizeof(RecordSetRow)); row->values = malloc(ncols * sizeof(Datum)); return row; } void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd) { if (row->values != NULL) { free_recordset_row_columns(row, rd); free(row->values); } free(row); } And here's the code for our RecordSet structs. Nothing too complex is going on here - just allocating memory and freeing memory as needed. The only noteworthy function is free_recordset_row_columns . This is where we use the information stored in the RecordDescriptor to free data inside the Datum array. Basic types like ints do not need to be free'd, but char* 's do. So we loop through the column metadata and call free on any DT_CHAR data types in the Datum array.","title":"RecordSet"},{"location":"05-selecting-data/04-table-scan/#defill_record","text":"Now, let's continue along our table scan function. Right now we're inside the for loop and we've just allocated a new RecordSetRow . Here's the relevant section for reference: src/access/tableam.c ( tableam_fullscan() ) for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); defill_record(td->rd, slot->pg + sp->offset, row->values); linkedlist_append(rows, row); } Our next step is to grab the next SlotPointer . Remember, the slot array grows from the end of the page towards the beginning. So, the first slot pointer is the very last 4 bytes on the page, the next one is the 4 bytes preceeding it, and so on. We can get the byte position by using this formula: conf->pageSize - (sizeof(SlotPointer) * (i + 1)) . Once we have its location, we can create a SlotPointer* object for easy reference. Now, we call the workhorse of our table scan method. defill_record uses the information in the RecordDescriptor and marches through the bytes where the data record are stored and extracts them into the Datum array we give to it. We define defill_record in the same place we defined fill_record . src/include/storage/record.h void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); void fill_record(RecordDescriptor* rd, Record r, Datum* data); +void defill_record(RecordDescriptor* rd, Record r, Datum* values); #endif /* RECORD_H */ And here's the code: src/storage/record.c +void defill_record(RecordDescriptor* rd, Record r, Datum* values) { + int offset = sizeof(RecordHeader); + for (int i = 0; i < rd->ncols; i++) { + Column* col = &rd->cols[i]; + values[i] = record_get_col_value(col, r, &offset); + } +} Pretty simple, right? We loop through the columns in our RecordDescriptor and extract each column's value, via a static helper function, into the Datum array. Note that we're passing &offset into the helper function. We want offset to change as we progress through the loop, so we need to pass in the address of the variable if we want the inner function to be able to perform persistent operations on its value. I should mention right now we COULD just perform offset += col->len; at the end of the for loop and be good. However, this method will not work when we get to variable-length columns. The Column object in the RecordDescriptor won't be able to store those lengths because they could be different for each row. The only way to find the length of those columns is to dig further into the Record data, which happens inside these helper functions. Thus, we make them responsible for incrementing offset . src/storage/record.c +static Datum record_get_col_value(Column* col, Record r, int* offset) { + switch (col->dataType) { + case DT_INT: + return record_get_int(r, offset); + case DT_CHAR: + return record_get_char(r, offset, col->len); + default: + printf(\"record_get_col_value() | Unknown data type!\\n\"); + return (Datum)NULL; + } +} This function is a wrapper around type-specific data extraction. As you can imagine, we'll be adding more case statements to this function as we expand our list of supported data types. src/storage/record.c +static Datum record_get_int(Record r, int* offset) { + int32_t intVal; + memcpy(&intVal, r + *offset, 4); + *offset += 4; + return int32GetDatum(intVal); +} + +static Datum record_get_char(Record r, int* offset, int charLen) { + char* pChar = malloc(charLen + 1); + memcpy(pChar, r + *offset, charLen); + pChar[charLen] = '\\0'; + *offset += charLen; + return charGetDatum(pChar); +} And here is where we do the actual data extraction. We create a variable of the correct data type, then memcpy the contents of the Record into that variable. Increment offset by dereferencing it, then return the Datum representation of the value. We have additional complexity for the record_get_char function. When we insert DT_CHAR data, we only store the relevant characters - we DO NOT store the null terminator. So when we extract the value, we need to malloc(charLen + 1) to make sure there's enough memory to append the null terminator to the end of the string. And that's all the code we need to perform a table scan on our table. Unfortunately, it's not enough to get our program to do anything useful just yet. I feel like this section has gone on long enough, so we'll round out the whole process in the next section.","title":"defill_record"},{"location":"05-selecting-data/05-displaying-results/","text":"Displaying Results In this section, we're going to write the code that parses through the data returned by the table scan function and displays them in a pretty way on the terminal. Something like this: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name; ====== Node ====== = Type: Select = Targets: = person_id = name -------- *** Rows: 2 -------- |person_id |name | -------------------------- | 69|chris burke | | 77|hello, world | (Rows: 2) bql > When we run select person_id, name; , the \"backend\" performs the table scan and hands the data back to the \"frontend\". We then parse through everything and determine how wide each column needs to be in order for the tabular format to look pretty. Resultset Header src/include/resultset/resultset_print.h #ifndef RESULTSET_PRINT_H #define RESULTSET_PRINT_H #include \"resultset/recordset.h\" void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets); #endif /* RESULTSET_PRINT_H */ I created a dedicated header file for outputting query results to the console. Right now it just has a single function that encapsulates all the work required to pretty-print our results. We send it a RecordDescriptor* rd that describes all columns in our table, a RecordSet* rs that contains all of the data brought back by the table scan, and a RecordDescriptor* targets that has an entry for each column we want to display. Remember, the table scan function pull back all columns regardless of what our select statement says it wants. So we need this additional RecordDescriptor to tell the print function exactly what we want displayed. It also works the other way, e.g. if we run select person_id, name, person_id, name; , the print function will know it needs to display each column twice. Resultset Print Code This is a rather long file, so bear with me. src/resultset/resultset_print.c void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { printf(\"--------\\n\"); printf(\"*** Rows: %d\\n\", rs->rows->numItems); printf(\"--------\\n\"); if (rs->rows->numItems == 0) return; int* widths = malloc(sizeof(int) * rd->ncols); compute_column_widths(rd, rs, widths); print_column_headers(rd, targets, widths); ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_INT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Starting with the function we expose in the header file, we print a little row count header at the top. And if there aren't any rows to display, we return early. Next we allocate memory for an int array with an entry for each column in our table. Then we offload the computation of those widths to our helper function compute_column_widths : src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } In this function, we have a nested loop that calculates the largest width for a given column across all rows. It adds 1 to the maxLen and sets the value in our widths array. The reason for the additional padding is just so the table display doesn't feel to congested. For DT_INT , we calculate the number of digits to get the maxLen, and for DT_CHAR we use the built in strlen function. The num_digits function is defined as follows: src/resultset/resultset_print.c static int num_digits(int64_t num) { if (num < 0) return num_digits(-num) + 1; // extra '1' is to account for the negative sign in the display if (num < 10) return 1; return 1 + num_digits(num / 10); } Pretty basic recursive function where we add 1 each time we divide the number by 10. Now back to the primary print function. After we compute the column widths, we call print_column_headers : src/resultset/resultset_print.c static void print_column_headers(RecordDescriptor* rd, RecordDescriptor* rs, int* widths) { printf(\"|\"); int totalWidth = 1; for (int i = 0; i < rs->ncols; i++) { int colIndex = get_col_index(rd, rs->cols[i].colname); print_cell_with_padding(rs->cols[i].colname, widths[colIndex], false); totalWidth += (widths[colIndex] + 1); } printf(\"\\n\"); for (int i = 0; i < totalWidth; i++) { printf(\"-\"); } printf(\"\\n\"); } We pass in the RecordDescriptor for the table, the RecordDescriptor for our target list, and the widths array. We start by printing the column boundary \"pipe\" character, then we loop through the columns in our target list. Based on the colname of the target column, we find its location in the RecordDescriptor for the table, which is also the same array index in the widths array. Using this information, we make a call to print_cell_with_padding to output our column name and update our totalWidth tracker. Finally at the end, we print a horizontal row of dashes to separate our column headers from the row data. Let's take a look at the two helper functions we used in printing out the column headers: src/resultset/resultset_print.c static int get_col_index(RecordDescriptor* rd, char* name) { for (int i = 0; i < rd->ncols; i++) { if (strcasecmp(rd->cols[i].colname, name) == 0) return rd->cols[i].colnum; } return -1; } We loop through the table's RecordDescriptor and just do a basic case-insensitive string comparison to find the correct column index. static void print_cell_with_padding(char* cell, int cellWidth, bool isRightAligned) { int padLen = cellWidth - strlen(cell); if (padLen < 0) { printf(\"\\npadLen: %d\\ncellWidth: %d\\n valWidth: %ld\\n\", padLen, cellWidth, strlen(cell)); return; } if (padLen == 0) { printf(\"%s|\", cell); } else if (isRightAligned) { printf(\"%*s%s|\", padLen, \" \", cell); } else { printf(\"%s%*s|\", cell, padLen, \" \"); } } And here we compute how much padding we need by subtracting the strlen of the value we want to print from the previously computed cellWidth . If it's less than zero, we print a sort of error message. If there's no padding, we print the value as-is and immediately follow it with a column divider. Next, we check if the caller wants the value right-aligned. This just determines whether we print the padding on the left or right side of the value. And instead of using a loop to print the padding, we use a clever trick of printf . The %*s will print n number of s . So in the right-align branch, we print %*s%s , meaning we print padLen number of \" \" (space characters) followed by the value of cell . Now, continuing along our primary print function, we enter the looping mechanism that goes row-by-row and prints out each column we declare in our target list. Here's the loop code for reference: src/resultset/resultset_print.c ( resultset_print() ) ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_INT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } We start at the head of the linked list containing our data and extract the Datum array from the ListItem . Then we loop over each column in our target list and print out the corresponding value. For DT_CHAR , we can simply call the same function we went over above, but for DT_INT we need another helper function. src/resultset/resultset_print.c static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } Here we simply compute the number of digits, turn the number into a char* , then call print_cell_with_padding . Updating Main We've covered all the code required to print our query results to the console, now let's update main.c and run the program. src/main.c #include \"gram.tab.h\" #include \"parser/parsetree.h\" #include \"parser/parse.h\" #include \"global/config.h\" #include \"storage/file.h\" #include \"storage/page.h\" +#include \"storage/table.h\" #include \"buffer/bufpool.h\" +#include \"storage/table.h\" +#include \"resultset/recordset.h\" +#include \"resultset/resultset_print.h\" +#include \"access/tableam.h\" +#include \"utility/linkedlist.h\" *** code removed for brevity *** +static RecordDescriptor* construct_record_descriptor_from_target_list(ParseList* targetList) { + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (targetList->length * sizeof(Column))); + rd->ncols = targetList->length; + + for (int i = 0; i < rd->ncols; i++) { + ResTarget* t = (ResTarget*)targetList->elements[i].ptr; + + // we don't care about the data type or length here + construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0); + } + + return rd; +} Starting with a bunch of new include statements and this brand new function in our \"temporary\" section of code. We loop through the list of select targets generated by the parser and create a RecordDescriptor for use elsewhere. Notice we're supplying DT_UNKNOWN to the column constructor. It's because the data type in these column structs is irrelevant - in reality, we're just shoehorning the RecordDescriptor into a role it's not meant to play. But it's good enough for now. We need to add the new data type to the DataType enum: src/include/storage/record.h typedef enum DataType { DT_INT, /* 4-bytes, signed */ DT_CHAR, /* Byte-size defined at table creation */ + DT_UNKNOWN } DataType; And finally, we have the logic that kicks off the select query: src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); bufpool_flush_all(bp); bufpool_destroy(bp); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; if (!insert_record(bp, person_id, name)) { printf(\"Unable to insert record\\n\"); } break; case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); + } else { + TableDesc* td = new_tabledesc(\"person\"); + td->rd = construct_record_descriptor(); + RecordSet* rs = new_recordset(); + rs->rows = new_linkedlist(); + RecordDescriptor* targets = construct_record_descriptor_from_target_list(((SelectStmt*)n)->targetList); + + tableam_fullscan(bp, td, rs->rows); + resultset_print(td->rd, rs, targets); + + free_recordset(rs, td->rd); + free_tabledesc(td); + free_record_desc(targets); + } break; } Essentially, if the analyzer says we're good, we run a full table scan. We start by allocating a TableDesc , a RecordSet , and a RecordDescriptor of our target list. Then we pass them to our table scan and print functions to take care of the heavy lifting. After that, we free up the memory we no longer need. Lastly, we need to update our Makefile : src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ storage/page.c \\ storage/record.c \\ storage/datum.c \\ - buffer/bufpool.c \\ + storage/table.c \\ + buffer/bufpool.c \\ + access/tableam.c \\ + resultset/recordset.c \\ + resultset/resultset_print.c \\ + utility/linkedlist.c Note the red line for bufpool.c is just because we moved it to a different location - it's still in the list. Running The Program I'm starting with a brand new database file by deleting the existing one (if necessary). Run $ rm db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'chris burke'; Bytes read: 0 bql > insert 123456789 'Large number'; bql > select person_id, name; -------- *** Rows: 2 -------- |person_id |name | -------------------------- | 69|chris burke | | 123456789|Large number | (Rows: 2) bql > I removed the node outputs to reduce the noise in the console display. After startup, I insert two records then run a select on both columns, which shows exactly what we expect in the tabular form. What if we select the columns more than once? bql > select person_id, name, PERSON_ID, NAME; -------- *** Rows: 2 -------- |person_id |name |PERSON_ID |NAME | --------------------------------------------------- | 69|chris burke | 69|chris burke | | 123456789|Large number | 123456789|Large number | (Rows: 2) bql > We get all of our target columns displayed to us - even the headers preserve the casing we used in our select statement. That wraps up this section. In the next section we're going to start expanding the data types we support - starting with the Int class.","title":"Displaying Results"},{"location":"05-selecting-data/05-displaying-results/#displaying-results","text":"In this section, we're going to write the code that parses through the data returned by the table scan function and displays them in a pretty way on the terminal. Something like this: $ make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name; ====== Node ====== = Type: Select = Targets: = person_id = name -------- *** Rows: 2 -------- |person_id |name | -------------------------- | 69|chris burke | | 77|hello, world | (Rows: 2) bql > When we run select person_id, name; , the \"backend\" performs the table scan and hands the data back to the \"frontend\". We then parse through everything and determine how wide each column needs to be in order for the tabular format to look pretty.","title":"Displaying Results"},{"location":"05-selecting-data/05-displaying-results/#resultset-header","text":"src/include/resultset/resultset_print.h #ifndef RESULTSET_PRINT_H #define RESULTSET_PRINT_H #include \"resultset/recordset.h\" void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets); #endif /* RESULTSET_PRINT_H */ I created a dedicated header file for outputting query results to the console. Right now it just has a single function that encapsulates all the work required to pretty-print our results. We send it a RecordDescriptor* rd that describes all columns in our table, a RecordSet* rs that contains all of the data brought back by the table scan, and a RecordDescriptor* targets that has an entry for each column we want to display. Remember, the table scan function pull back all columns regardless of what our select statement says it wants. So we need this additional RecordDescriptor to tell the print function exactly what we want displayed. It also works the other way, e.g. if we run select person_id, name, person_id, name; , the print function will know it needs to display each column twice.","title":"Resultset Header"},{"location":"05-selecting-data/05-displaying-results/#resultset-print-code","text":"This is a rather long file, so bear with me. src/resultset/resultset_print.c void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { printf(\"--------\\n\"); printf(\"*** Rows: %d\\n\", rs->rows->numItems); printf(\"--------\\n\"); if (rs->rows->numItems == 0) return; int* widths = malloc(sizeof(int) * rd->ncols); compute_column_widths(rd, rs, widths); print_column_headers(rd, targets, widths); ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_INT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Starting with the function we expose in the header file, we print a little row count header at the top. And if there aren't any rows to display, we return early. Next we allocate memory for an int array with an entry for each column in our table. Then we offload the computation of those widths to our helper function compute_column_widths : src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } In this function, we have a nested loop that calculates the largest width for a given column across all rows. It adds 1 to the maxLen and sets the value in our widths array. The reason for the additional padding is just so the table display doesn't feel to congested. For DT_INT , we calculate the number of digits to get the maxLen, and for DT_CHAR we use the built in strlen function. The num_digits function is defined as follows: src/resultset/resultset_print.c static int num_digits(int64_t num) { if (num < 0) return num_digits(-num) + 1; // extra '1' is to account for the negative sign in the display if (num < 10) return 1; return 1 + num_digits(num / 10); } Pretty basic recursive function where we add 1 each time we divide the number by 10. Now back to the primary print function. After we compute the column widths, we call print_column_headers : src/resultset/resultset_print.c static void print_column_headers(RecordDescriptor* rd, RecordDescriptor* rs, int* widths) { printf(\"|\"); int totalWidth = 1; for (int i = 0; i < rs->ncols; i++) { int colIndex = get_col_index(rd, rs->cols[i].colname); print_cell_with_padding(rs->cols[i].colname, widths[colIndex], false); totalWidth += (widths[colIndex] + 1); } printf(\"\\n\"); for (int i = 0; i < totalWidth; i++) { printf(\"-\"); } printf(\"\\n\"); } We pass in the RecordDescriptor for the table, the RecordDescriptor for our target list, and the widths array. We start by printing the column boundary \"pipe\" character, then we loop through the columns in our target list. Based on the colname of the target column, we find its location in the RecordDescriptor for the table, which is also the same array index in the widths array. Using this information, we make a call to print_cell_with_padding to output our column name and update our totalWidth tracker. Finally at the end, we print a horizontal row of dashes to separate our column headers from the row data. Let's take a look at the two helper functions we used in printing out the column headers: src/resultset/resultset_print.c static int get_col_index(RecordDescriptor* rd, char* name) { for (int i = 0; i < rd->ncols; i++) { if (strcasecmp(rd->cols[i].colname, name) == 0) return rd->cols[i].colnum; } return -1; } We loop through the table's RecordDescriptor and just do a basic case-insensitive string comparison to find the correct column index. static void print_cell_with_padding(char* cell, int cellWidth, bool isRightAligned) { int padLen = cellWidth - strlen(cell); if (padLen < 0) { printf(\"\\npadLen: %d\\ncellWidth: %d\\n valWidth: %ld\\n\", padLen, cellWidth, strlen(cell)); return; } if (padLen == 0) { printf(\"%s|\", cell); } else if (isRightAligned) { printf(\"%*s%s|\", padLen, \" \", cell); } else { printf(\"%s%*s|\", cell, padLen, \" \"); } } And here we compute how much padding we need by subtracting the strlen of the value we want to print from the previously computed cellWidth . If it's less than zero, we print a sort of error message. If there's no padding, we print the value as-is and immediately follow it with a column divider. Next, we check if the caller wants the value right-aligned. This just determines whether we print the padding on the left or right side of the value. And instead of using a loop to print the padding, we use a clever trick of printf . The %*s will print n number of s . So in the right-align branch, we print %*s%s , meaning we print padLen number of \" \" (space characters) followed by the value of cell . Now, continuing along our primary print function, we enter the looping mechanism that goes row-by-row and prints out each column we declare in our target list. Here's the loop code for reference: src/resultset/resultset_print.c ( resultset_print() ) ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_INT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } We start at the head of the linked list containing our data and extract the Datum array from the ListItem . Then we loop over each column in our target list and print out the corresponding value. For DT_CHAR , we can simply call the same function we went over above, but for DT_INT we need another helper function. src/resultset/resultset_print.c static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } Here we simply compute the number of digits, turn the number into a char* , then call print_cell_with_padding .","title":"Resultset Print Code"},{"location":"05-selecting-data/05-displaying-results/#updating-main","text":"We've covered all the code required to print our query results to the console, now let's update main.c and run the program. src/main.c #include \"gram.tab.h\" #include \"parser/parsetree.h\" #include \"parser/parse.h\" #include \"global/config.h\" #include \"storage/file.h\" #include \"storage/page.h\" +#include \"storage/table.h\" #include \"buffer/bufpool.h\" +#include \"storage/table.h\" +#include \"resultset/recordset.h\" +#include \"resultset/resultset_print.h\" +#include \"access/tableam.h\" +#include \"utility/linkedlist.h\" *** code removed for brevity *** +static RecordDescriptor* construct_record_descriptor_from_target_list(ParseList* targetList) { + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (targetList->length * sizeof(Column))); + rd->ncols = targetList->length; + + for (int i = 0; i < rd->ncols; i++) { + ResTarget* t = (ResTarget*)targetList->elements[i].ptr; + + // we don't care about the data type or length here + construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0); + } + + return rd; +} Starting with a bunch of new include statements and this brand new function in our \"temporary\" section of code. We loop through the list of select targets generated by the parser and create a RecordDescriptor for use elsewhere. Notice we're supplying DT_UNKNOWN to the column constructor. It's because the data type in these column structs is irrelevant - in reality, we're just shoehorning the RecordDescriptor into a role it's not meant to play. But it's good enough for now. We need to add the new data type to the DataType enum: src/include/storage/record.h typedef enum DataType { DT_INT, /* 4-bytes, signed */ DT_CHAR, /* Byte-size defined at table creation */ + DT_UNKNOWN } DataType; And finally, we have the logic that kicks off the select query: src/main.c switch (n->type) { case T_SysCmd: if (strcmp(((SysCmd*)n)->cmd, \"quit\") == 0) { free_node(n); printf(\"Shutting down...\\n\"); bufpool_flush_all(bp); bufpool_destroy(bp); file_close(fdesc); return EXIT_SUCCESS; } break; case T_InsertStmt: int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; if (!insert_record(bp, person_id, name)) { printf(\"Unable to insert record\\n\"); } break; case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); + } else { + TableDesc* td = new_tabledesc(\"person\"); + td->rd = construct_record_descriptor(); + RecordSet* rs = new_recordset(); + rs->rows = new_linkedlist(); + RecordDescriptor* targets = construct_record_descriptor_from_target_list(((SelectStmt*)n)->targetList); + + tableam_fullscan(bp, td, rs->rows); + resultset_print(td->rd, rs, targets); + + free_recordset(rs, td->rd); + free_tabledesc(td); + free_record_desc(targets); + } break; } Essentially, if the analyzer says we're good, we run a full table scan. We start by allocating a TableDesc , a RecordSet , and a RecordDescriptor of our target list. Then we pass them to our table scan and print functions to take care of the heavy lifting. After that, we free up the memory we no longer need. Lastly, we need to update our Makefile : src/Makefile SRC_FILES = main.c \\ parser/parse.c \\ parser/parsetree.c \\ global/config.c \\ storage/file.c \\ storage/page.c \\ storage/record.c \\ storage/datum.c \\ - buffer/bufpool.c \\ + storage/table.c \\ + buffer/bufpool.c \\ + access/tableam.c \\ + resultset/recordset.c \\ + resultset/resultset_print.c \\ + utility/linkedlist.c Note the red line for bufpool.c is just because we moved it to a different location - it's still in the list.","title":"Updating Main"},{"location":"05-selecting-data/05-displaying-results/#running-the-program","text":"I'm starting with a brand new database file by deleting the existing one (if necessary). Run $ rm db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'chris burke'; Bytes read: 0 bql > insert 123456789 'Large number'; bql > select person_id, name; -------- *** Rows: 2 -------- |person_id |name | -------------------------- | 69|chris burke | | 123456789|Large number | (Rows: 2) bql > I removed the node outputs to reduce the noise in the console display. After startup, I insert two records then run a select on both columns, which shows exactly what we expect in the tabular form. What if we select the columns more than once? bql > select person_id, name, PERSON_ID, NAME; -------- *** Rows: 2 -------- |person_id |name |PERSON_ID |NAME | --------------------------------------------------- | 69|chris burke | 69|chris burke | | 123456789|Large number | 123456789|Large number | (Rows: 2) bql > We get all of our target columns displayed to us - even the headers preserve the casing we used in our select statement. That wraps up this section. In the next section we're going to start expanding the data types we support - starting with the Int class.","title":"Running The Program"},{"location":"06-data-types-ints/01-hard-coded-table-refactor/","text":"Hard-Coded Table Refactor In this section we're going to implement the storage component of the remaining \"Int\" data types: TinyInt , SmallInt , and BigInt . Since we already have the plumbing in place for the 4-byte Int , this won't be a very difficult addition to implement. However, since we're still working with a hard-coded table definition, we need to (annoyingly) refactor the code associated with that. I know what you're thinking.. when can we ditch the hard-coded table definition? Somewhat soon. We need to introduce the system tables and build a legitimate analyzer first. And we'll be diving into those exact pieces right after we finish expanding our supported data types. As for the Ints , we'll need to work with a different hard-coded table - one that has all four of the different \"Int\" types: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Using the same table we've been working with, I added three new columns, age TinyInt , daily_steps SmallInt , and distance_from_home BigInt .. because maybe our person is an astronaut that's billions of miles from Earth. I was reaching here. The three new data types have different storage footprints compared to the 4-byte Int . Data Type Size Min Value Max Value TinyInt 1-byte 0 255 SmallInt 2-bytes -32,768 32,767 Int 4-bytes -2,147,483,648 2,147,483,647 BigInt 8-bytes -9,223,372,036,854,775,808 9,223,372,036,854,775,807 Yes, the BigInt is absurdly large. Fun fact, a UUID is just a large number and is twice the size (16-bytes) of a BigInt . So when you hear that UUID 's are pretty much guaranteed to be unique.. they're GUARANTEED to be unique. Now that we know the sizes of our new data types, we can calculate the number of bytes each row in our table will need: 12-byte header 4-byte person_id 20-byte name 1-byte age 2-byte daily_steps 8-byte distance_from_home 4-byte slot pointer total : 51 bytes Since the page header consumes the first 20 bytes, we can just barely fit two records on our 128-byte page. Refactoring Let's start adding support for these new data types. Beginning with the obvious: the DataType enum. src/include/storage/record.h typedef enum DataType { + DT_TINYINT, /* 1-byte, unsigned */ + DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ + DT_BIGINT, /* 8-bytes, signed */ DT_CHAR, /* Byte-size defined at table creation */ DT_UNKNOWN } DataType; Next, we can update all of our temporary functions in our main.c file: src/main.c -#define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) +#define RECORD_LEN 47 -static void populate_datum_array(Datum* data, int32_t person_id, char* name) { +static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); + data[2] = uint8GetDatum(age); + data[3] = int16GetDatum(dailySteps); + data[4] = int64GetDatum(distanceFromHome); } We need to update the RECORD_LEN macro to reflect the total footprint of our data records. Note it's 47 instead of 51 (from above) because this macro does not include the 4-byte slot pointer. Next, we add parameters for each of the new columns in our table as well as calls to the datum conversion functions that we will need to write. static RecordDescriptor* construct_record_descriptor() { - RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (2 * sizeof(Column))); + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (5 * sizeof(Column))); - rd->ncols = 2; + rd->ncols = 5; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); + construct_column_desc(&rd->cols[2], \"age\", DT_TINYINT, 2, 1); + construct_column_desc(&rd->cols[3], \"daily_steps\", DT_SMALLINT, 3, 2); + construct_column_desc(&rd->cols[4], \"distance_from_home\", DT_BIGINT, 4, 8); return rd; } We update our RecordDescriptor factory to create a RecordDescriptor with 5 columns now instead of two, and add calls to construct_column_desc with the appropriate inputs for our new columns. -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name) { +static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { Datum* data = malloc(rd->ncols * sizeof(Datum)); - populate_datum_array(data, person_id, name); + populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Our serializer function just needs to include the new columns as parameters, and pass those values to the populate_datum_array function. -static bool insert_record(BufPool* bp, int32_t person_id, char* name) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); - serialize_data(rd, r, person_id, name); + serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome); bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Similar to the above, we just add the new parameters and pass them along. static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; - if (!(strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0)) return false; + if ( + !( + strcasecmp(r->name, \"person_id\") == 0 || + strcasecmp(r->name, \"name\") == 0 || + strcasecmp(r->name, \"age\") == 0 || + strcasecmp(r->name, \"daily_steps\") == 0 || + strcasecmp(r->name, \"distance_from_home\") == 0 + ) + ) { + return false; + } } return true; } And the last of our temporary functions, the analyzer, just needs to check for the new columns in the Select target list. That's it for refactoring the temporary code. Next up, we'll refactor the parser to make sure it looks for the new columns in its insert statement grammar.","title":"Hard-Coded Table Refactor"},{"location":"06-data-types-ints/01-hard-coded-table-refactor/#hard-coded-table-refactor","text":"In this section we're going to implement the storage component of the remaining \"Int\" data types: TinyInt , SmallInt , and BigInt . Since we already have the plumbing in place for the 4-byte Int , this won't be a very difficult addition to implement. However, since we're still working with a hard-coded table definition, we need to (annoyingly) refactor the code associated with that. I know what you're thinking.. when can we ditch the hard-coded table definition? Somewhat soon. We need to introduce the system tables and build a legitimate analyzer first. And we'll be diving into those exact pieces right after we finish expanding our supported data types. As for the Ints , we'll need to work with a different hard-coded table - one that has all four of the different \"Int\" types: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Using the same table we've been working with, I added three new columns, age TinyInt , daily_steps SmallInt , and distance_from_home BigInt .. because maybe our person is an astronaut that's billions of miles from Earth. I was reaching here. The three new data types have different storage footprints compared to the 4-byte Int . Data Type Size Min Value Max Value TinyInt 1-byte 0 255 SmallInt 2-bytes -32,768 32,767 Int 4-bytes -2,147,483,648 2,147,483,647 BigInt 8-bytes -9,223,372,036,854,775,808 9,223,372,036,854,775,807 Yes, the BigInt is absurdly large. Fun fact, a UUID is just a large number and is twice the size (16-bytes) of a BigInt . So when you hear that UUID 's are pretty much guaranteed to be unique.. they're GUARANTEED to be unique. Now that we know the sizes of our new data types, we can calculate the number of bytes each row in our table will need: 12-byte header 4-byte person_id 20-byte name 1-byte age 2-byte daily_steps 8-byte distance_from_home 4-byte slot pointer total : 51 bytes Since the page header consumes the first 20 bytes, we can just barely fit two records on our 128-byte page.","title":"Hard-Coded Table Refactor"},{"location":"06-data-types-ints/01-hard-coded-table-refactor/#refactoring","text":"Let's start adding support for these new data types. Beginning with the obvious: the DataType enum. src/include/storage/record.h typedef enum DataType { + DT_TINYINT, /* 1-byte, unsigned */ + DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ + DT_BIGINT, /* 8-bytes, signed */ DT_CHAR, /* Byte-size defined at table creation */ DT_UNKNOWN } DataType; Next, we can update all of our temporary functions in our main.c file: src/main.c -#define RECORD_LEN 36 // 12-byte header + 4-byte Int + 20-byte Char(20) +#define RECORD_LEN 47 -static void populate_datum_array(Datum* data, int32_t person_id, char* name) { +static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); + data[2] = uint8GetDatum(age); + data[3] = int16GetDatum(dailySteps); + data[4] = int64GetDatum(distanceFromHome); } We need to update the RECORD_LEN macro to reflect the total footprint of our data records. Note it's 47 instead of 51 (from above) because this macro does not include the 4-byte slot pointer. Next, we add parameters for each of the new columns in our table as well as calls to the datum conversion functions that we will need to write. static RecordDescriptor* construct_record_descriptor() { - RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (2 * sizeof(Column))); + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (5 * sizeof(Column))); - rd->ncols = 2; + rd->ncols = 5; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); + construct_column_desc(&rd->cols[2], \"age\", DT_TINYINT, 2, 1); + construct_column_desc(&rd->cols[3], \"daily_steps\", DT_SMALLINT, 3, 2); + construct_column_desc(&rd->cols[4], \"distance_from_home\", DT_BIGINT, 4, 8); return rd; } We update our RecordDescriptor factory to create a RecordDescriptor with 5 columns now instead of two, and add calls to construct_column_desc with the appropriate inputs for our new columns. -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name) { +static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { Datum* data = malloc(rd->ncols * sizeof(Datum)); - populate_datum_array(data, person_id, name); + populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Our serializer function just needs to include the new columns as parameters, and pass those values to the populate_datum_array function. -static bool insert_record(BufPool* bp, int32_t person_id, char* name) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); - serialize_data(rd, r, person_id, name); + serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome); bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Similar to the above, we just add the new parameters and pass them along. static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; - if (!(strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0)) return false; + if ( + !( + strcasecmp(r->name, \"person_id\") == 0 || + strcasecmp(r->name, \"name\") == 0 || + strcasecmp(r->name, \"age\") == 0 || + strcasecmp(r->name, \"daily_steps\") == 0 || + strcasecmp(r->name, \"distance_from_home\") == 0 + ) + ) { + return false; + } } return true; } And the last of our temporary functions, the analyzer, just needs to check for the new columns in the Select target list. That's it for refactoring the temporary code. Next up, we'll refactor the parser to make sure it looks for the new columns in its insert statement grammar.","title":"Refactoring"},{"location":"06-data-types-ints/02-parser-refactor/","text":"Parser Refactor Next up, we need to update our parser grammar to include the new columns in our insert statement. Remember, the table we're working with now is defined as: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Currently our insert statement syntax looks like this: bql > insert [person_id] '[name]'; We're going to change it to this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home]; And fortunately, this type of change is very easy to make. Grammar Update Our lexer already knows how to find numbers when it scans our input (except for a small edge case I'll go over later), so we only need to change the grammar file. src/parser/gram.y -insert_stmt: INSERT INTNUM STRING { +insert_stmt: INSERT INTNUM STRING INTNUM INTNUM INTNUM { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); + ins->age = $4; + ins->dailySteps = $5; + ins->distanceFromHome = $6; $$ = (Node*)ins; } ; Yup, that's it. Just need to add a few extra INTNUM 's for the additional columns and store them in the InsertStmt node. Speaking of, we need to add those new properties to the insert struct. src/include/parser/parsetree.h +#include <stdint.h> *** code excluded for brevity *** typedef struct InsertStmt { NodeTag type; - int personId; + int32_t personId; char* name; + uint8_t age; + int16_t dailySteps; + int64_t distanceFromHome; } InsertStmt; While we're at it, I'm going to change personId 's type to more accurately reflect the data type we're storing on the page. And because these use the specialty int types, we need to #include <stdint.h> . Note: because our additional data types are all \"ints\", we do not need to add any logic for freeing their memory. We do, however, have one more small update to make in the parsetree file: src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); + printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); + printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); + printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } I just want our info prints to show what the parser parsed out for us. That ends the changes required in the parser. In the next section, we'll make updates to the actual insert functionality and finally be able to run some insert statements and test out our changes.","title":"Parser Refactor"},{"location":"06-data-types-ints/02-parser-refactor/#parser-refactor","text":"Next up, we need to update our parser grammar to include the new columns in our insert statement. Remember, the table we're working with now is defined as: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null ); Currently our insert statement syntax looks like this: bql > insert [person_id] '[name]'; We're going to change it to this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home]; And fortunately, this type of change is very easy to make.","title":"Parser Refactor"},{"location":"06-data-types-ints/02-parser-refactor/#grammar-update","text":"Our lexer already knows how to find numbers when it scans our input (except for a small edge case I'll go over later), so we only need to change the grammar file. src/parser/gram.y -insert_stmt: INSERT INTNUM STRING { +insert_stmt: INSERT INTNUM STRING INTNUM INTNUM INTNUM { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); + ins->age = $4; + ins->dailySteps = $5; + ins->distanceFromHome = $6; $$ = (Node*)ins; } ; Yup, that's it. Just need to add a few extra INTNUM 's for the additional columns and store them in the InsertStmt node. Speaking of, we need to add those new properties to the insert struct. src/include/parser/parsetree.h +#include <stdint.h> *** code excluded for brevity *** typedef struct InsertStmt { NodeTag type; - int personId; + int32_t personId; char* name; + uint8_t age; + int16_t dailySteps; + int64_t distanceFromHome; } InsertStmt; While we're at it, I'm going to change personId 's type to more accurately reflect the data type we're storing on the page. And because these use the specialty int types, we need to #include <stdint.h> . Note: because our additional data types are all \"ints\", we do not need to add any logic for freeing their memory. We do, however, have one more small update to make in the parsetree file: src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); + printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); + printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); + printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } I just want our info prints to show what the parser parsed out for us. That ends the changes required in the parser. In the next section, we'll make updates to the actual insert functionality and finally be able to run some insert statements and test out our changes.","title":"Grammar Update"},{"location":"06-data-types-ints/03-fill-and-defill/","text":"Fill and Defill In this section, we're going to update our code to actually insert our new data, and we'll finally be able to test it out by running some insert statements. Let's pretend we're an insert statement being shuffled along our program's insert code path and make the necessary changes as we find them. Starting at the top, our main function requests input from the user and sends it through the parser. Processing the parse tree is where we come in: src/main.c int main(int argc, char** argv) { *** code omitted for brevity break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; + uint8_t age = ((InsertStmt*)n)->age; + int16_t dailySteps = ((InsertStmt*)n)->dailySteps; + int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; - if (!insert_record(bp, person_id, name)) { + if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** } We need to extract the new fields from our InsertStmt node and pass them to the insert_record function. Next we follow the insert path until we get to the fill_record and fill_val functions. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { + case DT_TINYINT: + dataLen = 1; + uint8_t valTinyInt = datumGetUInt8(datum); + memcpy(data, &valTinyInt, dataLen); + break; + case DT_SMALLINT: + dataLen = 2; + int16_t valSmallInt = datumGetInt16(datum); + memcpy(data, &valSmallInt, dataLen); + break; case DT_INT: dataLen = 4; int32_t valInt = datumGetInt32(datum); memcpy(data, &valInt, dataLen); break; + case DT_BIGINT: + dataLen = 8; + int64_t valBigInt = datumGetInt64(datum); + memcpy(data, &valBigInt, dataLen); + break; case DT_CHAR: dataLen = col->len; char* str = strdup(datumGetString(datum)); int charLen = strlen(str); if (charLen > dataLen) charLen = dataLen; memcpy(data, str, charLen); free(str); break; } data += dataLen; *dataP = data; } We're simply adding more cases to the switch statement to account for the new data types. Make sure you set the dataLen to the correct byte size for the data type. Datum Conversions Since we've made several calls to the datum conversion functions for the new data types, we actually need to write them now. src/include/storage/datum.h +Datum uint8GetDatum(uint8_t i); +Datum int16GetDatum(int16_t i); Datum int32GetDatum(int32_t i); +Datum int64GetDatum(int64_t i); Datum charGetDatum(char* c); +uint8_t datumGetUInt8(Datum d); +int16_t datumGetInt16(Datum d); int32_t datumGetInt32(Datum d); +int64_t datumGetInt64(Datum d); char* datumGetString(Datum d); src/storage/datum.c +Datum uint8GetDatum(uint8_t i) { + return (Datum) i; +} + +Datum int16GetDatum(int16_t i) { + return (Datum) i; +} Datum int32GetDatum(int32_t i) { return (Datum) i; } +Datum int64GetDatum(int64_t i) { + return (Datum) i; +} Datum charGetDatum(char* c) { return (Datum) c; } +uint8_t datumGetUInt8(Datum d) { + return (uint8_t) d; +} + +int16_t datumGetInt16(Datum d) { + return (int16_t) d; +} int32_t datumGetInt32(Datum d) { return (int32_t) d; } +int64_t datumGetInt64(Datum d) { + return (int64_t) d; +} char* datumGetString(Datum d) { return (char*) d; } Testing It Out That's everything we need for insert operations, let's test it out. Make sure you delete the main.dbd file before compiling and running the program because any existing data was built on the old table definition. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 1234567890; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 1234567890 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... From the node print piece, you can see the values we parsed out of the insert statement. And \\quit makes sure the data is written to disk. Let's check it out: xxd db_files/main.dbd The orange and purple boxes are the page header and record header, respectively. Then the yellow and green boxes are the columns we're already used to: person_id and name . The new ones follow in the order we inserted them. age is the light blue box and represented in hex as 0x2d, which is the same as 45 in decimal. Check. daily_steps is the next two bytes. Remember, my machine is a little endian machine, so the bytes are stored in the opposite order you'd expect. So the actual hex representation of the number is 0x3039, which translates to 12,345 in decimal. Check. distance_from_home is the remaining 8 bytes in the record and is also stored in little endian. So the hex representation is 0x499602d2, which translates to 1,234,567,890. Everything checks out, right? Wrong. Let's revisit that edge case I mentioned a little while back. BigInt Overflow? You may remember our lexer tokenizes numbers using the atoi() function. This will only work for numbers that fit inside a 4-byte int. What happens if we try to insert a number that overflows a 4-byte int, e.g. is greater than 2.1 billion? Let's find out: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: -539222988 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... This time I inserted the same data, except for distance_from_home I added a zero to the end, making the total 12 billion. Turns out, our code wraps around to the negative regime of the 4-byte number space. Let's fix this. src/parser/scan.l --?[0-9]+ { yylval->intval = atoi(yytext); return INTVAL; } +-?[0-9]+ { yylval->numval = strtoll(yytext, &yytext, 10); return NUMBER; } We changed our converter function to use strtoll from the C standard, which can handle up to the long long data type. We also changed the names of the tokens defined by bison: src/parser/gram.y %union { char* str; - int intval; + long long numval; struct Node* node; struct ParseList* list; } %parse-param { struct Node** n } %param { void* scanner } %token <str> SYS_CMD STRING IDENT -%token <intval> INTNUM +%token <numval> NUMBER *** code omitted for brevity *** -insert_stmt: INSERT INTNUM STRING INTNUM INTNUM INTNUM { +insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); ins->age = $4; ins->dailySteps = $5; ins->distanceFromHome = $6; $$ = (Node*)ins; } ; Here we just changed INTNUM to NUMBER . Now if we run it and try to insert the same large number... $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 12345678900 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... We can see that the lexer/parser successfully handled the large number. Defill Now, let's refactor the code required for defilling the new data types. The pattern is pretty simple, we just need to add more switch/case branches for the new DT_* types. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { + case DT_TINYINT: + return record_get_tinyint(r, offset); + case DT_SMALLINT: + return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); + case DT_BIGINT: + return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } This function is called in a loop by the defill_record function. We simple add our new data types to the switch/case block. But we also need to create new record_get_* functions for each of the new data types. src/storage/record.c +static Datum record_get_tinyint(Record r, int* offset) { + uint8_t tinyintVal; + memcpy(&tinyintVal, r + *offset, 1); + *offset += 1; + return uint8GetDatum(tinyintVal); +} + +static Datum record_get_smallint(Record r, int* offset) { + int16_t smallintVal; + memcpy(&smallintVal, r + *offset, 2); + *offset += 2; + return int16GetDatum(smallintVal); +} static Datum record_get_int(Record r, int* offset) { int32_t intVal; memcpy(&intVal, r + *offset, 4); *offset += 4; return int32GetDatum(intVal); } +static Datum record_get_bigint(Record r, int* offset) { + int64_t bigintVal; + memcpy(&bigintVal, r + *offset, 8); + *offset += 8; + return int64GetDatum(bigintVal); +} We can see the pattern is the exact same as we already have for the Int type. The only differences are calling the correct datum conversion function and incrementing offset by the correct amount. We have one final thing to do before we can run select statements, but this page has gone on too long, so I'm going to move it to its own dedicated page.","title":"Fill and Defill"},{"location":"06-data-types-ints/03-fill-and-defill/#fill-and-defill","text":"In this section, we're going to update our code to actually insert our new data, and we'll finally be able to test it out by running some insert statements. Let's pretend we're an insert statement being shuffled along our program's insert code path and make the necessary changes as we find them. Starting at the top, our main function requests input from the user and sends it through the parser. Processing the parse tree is where we come in: src/main.c int main(int argc, char** argv) { *** code omitted for brevity break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; + uint8_t age = ((InsertStmt*)n)->age; + int16_t dailySteps = ((InsertStmt*)n)->dailySteps; + int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; - if (!insert_record(bp, person_id, name)) { + if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** } We need to extract the new fields from our InsertStmt node and pass them to the insert_record function. Next we follow the insert path until we get to the fill_record and fill_val functions. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { + case DT_TINYINT: + dataLen = 1; + uint8_t valTinyInt = datumGetUInt8(datum); + memcpy(data, &valTinyInt, dataLen); + break; + case DT_SMALLINT: + dataLen = 2; + int16_t valSmallInt = datumGetInt16(datum); + memcpy(data, &valSmallInt, dataLen); + break; case DT_INT: dataLen = 4; int32_t valInt = datumGetInt32(datum); memcpy(data, &valInt, dataLen); break; + case DT_BIGINT: + dataLen = 8; + int64_t valBigInt = datumGetInt64(datum); + memcpy(data, &valBigInt, dataLen); + break; case DT_CHAR: dataLen = col->len; char* str = strdup(datumGetString(datum)); int charLen = strlen(str); if (charLen > dataLen) charLen = dataLen; memcpy(data, str, charLen); free(str); break; } data += dataLen; *dataP = data; } We're simply adding more cases to the switch statement to account for the new data types. Make sure you set the dataLen to the correct byte size for the data type.","title":"Fill and Defill"},{"location":"06-data-types-ints/03-fill-and-defill/#datum-conversions","text":"Since we've made several calls to the datum conversion functions for the new data types, we actually need to write them now. src/include/storage/datum.h +Datum uint8GetDatum(uint8_t i); +Datum int16GetDatum(int16_t i); Datum int32GetDatum(int32_t i); +Datum int64GetDatum(int64_t i); Datum charGetDatum(char* c); +uint8_t datumGetUInt8(Datum d); +int16_t datumGetInt16(Datum d); int32_t datumGetInt32(Datum d); +int64_t datumGetInt64(Datum d); char* datumGetString(Datum d); src/storage/datum.c +Datum uint8GetDatum(uint8_t i) { + return (Datum) i; +} + +Datum int16GetDatum(int16_t i) { + return (Datum) i; +} Datum int32GetDatum(int32_t i) { return (Datum) i; } +Datum int64GetDatum(int64_t i) { + return (Datum) i; +} Datum charGetDatum(char* c) { return (Datum) c; } +uint8_t datumGetUInt8(Datum d) { + return (uint8_t) d; +} + +int16_t datumGetInt16(Datum d) { + return (int16_t) d; +} int32_t datumGetInt32(Datum d) { return (int32_t) d; } +int64_t datumGetInt64(Datum d) { + return (int64_t) d; +} char* datumGetString(Datum d) { return (char*) d; }","title":"Datum Conversions"},{"location":"06-data-types-ints/03-fill-and-defill/#testing-it-out","text":"That's everything we need for insert operations, let's test it out. Make sure you delete the main.dbd file before compiling and running the program because any existing data was built on the old table definition. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 1234567890; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 1234567890 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... From the node print piece, you can see the values we parsed out of the insert statement. And \\quit makes sure the data is written to disk. Let's check it out: xxd db_files/main.dbd The orange and purple boxes are the page header and record header, respectively. Then the yellow and green boxes are the columns we're already used to: person_id and name . The new ones follow in the order we inserted them. age is the light blue box and represented in hex as 0x2d, which is the same as 45 in decimal. Check. daily_steps is the next two bytes. Remember, my machine is a little endian machine, so the bytes are stored in the opposite order you'd expect. So the actual hex representation of the number is 0x3039, which translates to 12,345 in decimal. Check. distance_from_home is the remaining 8 bytes in the record and is also stored in little endian. So the hex representation is 0x499602d2, which translates to 1,234,567,890. Everything checks out, right? Wrong. Let's revisit that edge case I mentioned a little while back.","title":"Testing It Out"},{"location":"06-data-types-ints/03-fill-and-defill/#bigint-overflow","text":"You may remember our lexer tokenizes numbers using the atoi() function. This will only work for numbers that fit inside a 4-byte int. What happens if we try to insert a number that overflows a 4-byte int, e.g. is greater than 2.1 billion? Let's find out: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: -539222988 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... This time I inserted the same data, except for distance_from_home I added a zero to the end, making the total 12 billion. Turns out, our code wraps around to the negative regime of the 4-byte number space. Let's fix this. src/parser/scan.l --?[0-9]+ { yylval->intval = atoi(yytext); return INTVAL; } +-?[0-9]+ { yylval->numval = strtoll(yytext, &yytext, 10); return NUMBER; } We changed our converter function to use strtoll from the C standard, which can handle up to the long long data type. We also changed the names of the tokens defined by bison: src/parser/gram.y %union { char* str; - int intval; + long long numval; struct Node* node; struct ParseList* list; } %parse-param { struct Node** n } %param { void* scanner } %token <str> SYS_CMD STRING IDENT -%token <intval> INTNUM +%token <numval> NUMBER *** code omitted for brevity *** -insert_stmt: INSERT INTNUM STRING INTNUM INTNUM INTNUM { +insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); ins->age = $4; ins->dailySteps = $5; ins->distanceFromHome = $6; $$ = (Node*)ins; } ; Here we just changed INTNUM to NUMBER . Now if we run it and try to insert the same large number... $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 12345678900 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... We can see that the lexer/parser successfully handled the large number.","title":"BigInt Overflow?"},{"location":"06-data-types-ints/03-fill-and-defill/#defill","text":"Now, let's refactor the code required for defilling the new data types. The pattern is pretty simple, we just need to add more switch/case branches for the new DT_* types. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { + case DT_TINYINT: + return record_get_tinyint(r, offset); + case DT_SMALLINT: + return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); + case DT_BIGINT: + return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } This function is called in a loop by the defill_record function. We simple add our new data types to the switch/case block. But we also need to create new record_get_* functions for each of the new data types. src/storage/record.c +static Datum record_get_tinyint(Record r, int* offset) { + uint8_t tinyintVal; + memcpy(&tinyintVal, r + *offset, 1); + *offset += 1; + return uint8GetDatum(tinyintVal); +} + +static Datum record_get_smallint(Record r, int* offset) { + int16_t smallintVal; + memcpy(&smallintVal, r + *offset, 2); + *offset += 2; + return int16GetDatum(smallintVal); +} static Datum record_get_int(Record r, int* offset) { int32_t intVal; memcpy(&intVal, r + *offset, 4); *offset += 4; return int32GetDatum(intVal); } +static Datum record_get_bigint(Record r, int* offset) { + int64_t bigintVal; + memcpy(&bigintVal, r + *offset, 8); + *offset += 8; + return int64GetDatum(bigintVal); +} We can see the pattern is the exact same as we already have for the Int type. The only differences are calling the correct datum conversion function and incrementing offset by the correct amount. We have one final thing to do before we can run select statements, but this page has gone on too long, so I'm going to move it to its own dedicated page.","title":"Defill"},{"location":"06-data-types-ints/04-select-updates/","text":"Select Updates The last thing we need to do is refactor our resultset code to handle the new data types. These changes are very similar to the defill updates - simply adding swicth/cases for the three new Int types. We have three functions to update: src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { + case DT_TINYINT: + len = num_digits(datumGetUInt8(data->values[i])); + break; + case DT_SMALLINT: + len = num_digits(datumGetInt16(data->values[i])); + break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; + case DT_BIGINT: + len = num_digits(datumGetInt64(data->values[i])); + break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { + case DT_TINYINT: + numDigits = num_digits(datumGetUInt8(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%u\", datumGetUInt8(d)); + break; + case DT_SMALLINT: + numDigits = num_digits(datumGetInt16(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%d\", datumGetInt16(d)); + break; case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; + case DT_BIGINT: + numDigits = num_digits(datumGetInt64(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%ld\", datumGetInt64(d)); + break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { *** omitted for brevity *** while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { + case DT_TINYINT: + case DT_SMALLINT: case DT_INT: + case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Pretty basic here. We're just adding logic for handling each of the new data types. Now let's try to select some data: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name, age, daily_steps, distance_from_home; ====== Node ====== = Type: Select = Targets: = person_id = name = age = daily_steps = distance_from_home -------- *** Rows: 1 -------- |person_id |name |age |daily_steps |distance_from_home | --------------------------------------------------------------- | 69|Chris Burke | 45| 12345| 12345678900| (Rows: 1) bql > We can see our resultset code can now gracefully handle the new columns and is accurately reporting the same data we inserted earlier.","title":"Select Updates"},{"location":"06-data-types-ints/04-select-updates/#select-updates","text":"The last thing we need to do is refactor our resultset code to handle the new data types. These changes are very similar to the defill updates - simply adding swicth/cases for the three new Int types. We have three functions to update: src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { + case DT_TINYINT: + len = num_digits(datumGetUInt8(data->values[i])); + break; + case DT_SMALLINT: + len = num_digits(datumGetInt16(data->values[i])); + break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; + case DT_BIGINT: + len = num_digits(datumGetInt64(data->values[i])); + break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { + case DT_TINYINT: + numDigits = num_digits(datumGetUInt8(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%u\", datumGetUInt8(d)); + break; + case DT_SMALLINT: + numDigits = num_digits(datumGetInt16(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%d\", datumGetInt16(d)); + break; case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; + case DT_BIGINT: + numDigits = num_digits(datumGetInt64(d)); + cell = malloc(numDigits + 1); + sprintf(cell, \"%ld\", datumGetInt64(d)); + break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { *** omitted for brevity *** while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { + case DT_TINYINT: + case DT_SMALLINT: case DT_INT: + case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Pretty basic here. We're just adding logic for handling each of the new data types. Now let's try to select some data: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, name, age, daily_steps, distance_from_home; ====== Node ====== = Type: Select = Targets: = person_id = name = age = daily_steps = distance_from_home -------- *** Rows: 1 -------- |person_id |name |age |daily_steps |distance_from_home | --------------------------------------------------------------- | 69|Chris Burke | 45| 12345| 12345678900| (Rows: 1) bql > We can see our resultset code can now gracefully handle the new columns and is accurately reporting the same data we inserted earlier.","title":"Select Updates"},{"location":"07-data-types-bool/01-hard-coded-table-refactor/","text":"Hard-Coded Table Refactor I don't know about you, but I'm starting to get real tired of refactoring our temporary code due to introducing a new data type. Rest assured, relief is on the horizon, but we're not there just yet. Quick note: most of the text in this section will mostly be copy/pasted (with applicable changes) from the last section where we introduced the Int 's. That's because we're changing the code in all the same places, so you can skip to the code diff sections if you don't feel like reading the same walls of text again. In this section, we're introducing the Bool data type. Its storage footprint will fill one byte and we'll be using the uint8_t C type in our code. The Bool is essentially a TinyInt , but will only have two valid values: true and false. Using a full byte to store something that only needs a single bit may seem inefficient... and it absolutely is. However, there's all sorts of extra logical overhead that comes with packing multiple Bools in a single byte that I don't want to deal with. Some database engines, like Microsoft SQL Server, do indeed store boolean values in a single bit. In MSSQL's case, the data type is actually called a Bit . It still requires a full byte of storage space, but if a table contains more than one Bit column, the engine is able to stuff up to 8 of them in that single byte. I want to keep our code simple, so we're going to use a full byte for each Bool column we come across. Table DDL First thing's first, our new table definition. It's the same as last time, except we're adding a Bool column to the end: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); And for fun let's calculate the byte requirement of a new record: 12-byte header 4-byte person_id 20-byte name 1-byte age 2-byte daily_steps 8-byte distance_from_home 1-byte is_alive 4-byte slot pointer total : 52 bytes Since the page header consumes the first 20 bytes, we can just barely fit two records on our 128-byte data page. Refactoring Let's start adding support for the new data type. Beginning witht he obvious: the DataType enum. src/include/storage/record.h typedef enum DataType { DT_TINYINT, /* 1-byte, unsigned */ DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ DT_BIGINT, /* 8-bytes, signed */ + DT_BOOL, /* 1-byte, unsigned | similar to DT_TINYINT, but always evaluates to 1 or 0 */ DT_CHAR, /* Byte-size defined at table creation */ DT_UNKNOWN } DataType; Next, we can update all of our temporary functions in our main.c file: src/main.c -#define RECORD_LEN 47 +#define RECORD_LEN 48 -static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); data[2] = uint8GetDatum(age); data[3] = int16GetDatum(dailySteps); data[4] = int64GetDatum(distanceFromHome); + data[5] = uint8GetDatum(isAlive); } We need to update the RECORD_LEN macro to reflect the total footprint of our data records. Note it's 48 instead of 52 (from above) because this macro does not include the 4-byte slot pointer. We also piggyback on the existing uint8 datum conversion function because that's how we're representing the Bool data type. Next, we add parameters for each of the new columns in our table as well as calls to the datum conversion functions that we will need to write. static RecordDescriptor* construct_record_descriptor() { - RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (5 * sizeof(Column))); + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (6 * sizeof(Column))); - rd->ncols = 5; + rd->ncols = 6; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); construct_column_desc(&rd->cols[2], \"age\", DT_TINYINT, 2, 1); construct_column_desc(&rd->cols[3], \"daily_steps\", DT_SMALLINT, 3, 2); construct_column_desc(&rd->cols[4], \"distance_from_home\", DT_BIGINT, 4, 8); + construct_column_desc(&rd->cols[5], \"is_alive\", DT_BOOL, 5, 1); return rd; } We update our RecordDescriptor factory to create a RecordDescriptor with 6 columns now instead of 5, and add calls to construct_column_desc with the appropriate inputs for our new DT_BOOL column. -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { Datum* data = malloc(rd->ncols * sizeof(Datum)); - populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome); + populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome, isAlive); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Our serializer function just needs to include the new column as a parameter, and pass it to the populate_datum_array function. -static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); - serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome); + serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome, isAlive); bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Similar to the above, we just add the new parameter and pass it along. static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; if ( !( strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0 || strcasecmp(r->name, \"age\") == 0 || strcasecmp(r->name, \"daily_steps\") == 0 || - strcasecmp(r->name, \"distance_from_home\") == 0 + strcasecmp(r->name, \"distance_from_home\") == 0 || + strcasecmp(r->name, \"is_alive\") == 0 ) ) { return false; } } return true; } And the last of our temporary functions, the analyzer, just needs to check for the new column in the Select target list. That's it for refactoring the temporary code. Next up, we'll refactor the parser to make sure it looks for the new column in its insert statement grammar.","title":"Hard-Coded Table Refactor"},{"location":"07-data-types-bool/01-hard-coded-table-refactor/#hard-coded-table-refactor","text":"I don't know about you, but I'm starting to get real tired of refactoring our temporary code due to introducing a new data type. Rest assured, relief is on the horizon, but we're not there just yet. Quick note: most of the text in this section will mostly be copy/pasted (with applicable changes) from the last section where we introduced the Int 's. That's because we're changing the code in all the same places, so you can skip to the code diff sections if you don't feel like reading the same walls of text again. In this section, we're introducing the Bool data type. Its storage footprint will fill one byte and we'll be using the uint8_t C type in our code. The Bool is essentially a TinyInt , but will only have two valid values: true and false. Using a full byte to store something that only needs a single bit may seem inefficient... and it absolutely is. However, there's all sorts of extra logical overhead that comes with packing multiple Bools in a single byte that I don't want to deal with. Some database engines, like Microsoft SQL Server, do indeed store boolean values in a single bit. In MSSQL's case, the data type is actually called a Bit . It still requires a full byte of storage space, but if a table contains more than one Bit column, the engine is able to stuff up to 8 of them in that single byte. I want to keep our code simple, so we're going to use a full byte for each Bool column we come across.","title":"Hard-Coded Table Refactor"},{"location":"07-data-types-bool/01-hard-coded-table-refactor/#table-ddl","text":"First thing's first, our new table definition. It's the same as last time, except we're adding a Bool column to the end: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); And for fun let's calculate the byte requirement of a new record: 12-byte header 4-byte person_id 20-byte name 1-byte age 2-byte daily_steps 8-byte distance_from_home 1-byte is_alive 4-byte slot pointer total : 52 bytes Since the page header consumes the first 20 bytes, we can just barely fit two records on our 128-byte data page.","title":"Table DDL"},{"location":"07-data-types-bool/01-hard-coded-table-refactor/#refactoring","text":"Let's start adding support for the new data type. Beginning witht he obvious: the DataType enum. src/include/storage/record.h typedef enum DataType { DT_TINYINT, /* 1-byte, unsigned */ DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ DT_BIGINT, /* 8-bytes, signed */ + DT_BOOL, /* 1-byte, unsigned | similar to DT_TINYINT, but always evaluates to 1 or 0 */ DT_CHAR, /* Byte-size defined at table creation */ DT_UNKNOWN } DataType; Next, we can update all of our temporary functions in our main.c file: src/main.c -#define RECORD_LEN 47 +#define RECORD_LEN 48 -static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static void populate_datum_array(Datum* data, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { data[0] = int32GetDatum(person_id); data[1] = charGetDatum(name); data[2] = uint8GetDatum(age); data[3] = int16GetDatum(dailySteps); data[4] = int64GetDatum(distanceFromHome); + data[5] = uint8GetDatum(isAlive); } We need to update the RECORD_LEN macro to reflect the total footprint of our data records. Note it's 48 instead of 52 (from above) because this macro does not include the 4-byte slot pointer. We also piggyback on the existing uint8 datum conversion function because that's how we're representing the Bool data type. Next, we add parameters for each of the new columns in our table as well as calls to the datum conversion functions that we will need to write. static RecordDescriptor* construct_record_descriptor() { - RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (5 * sizeof(Column))); + RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (6 * sizeof(Column))); - rd->ncols = 5; + rd->ncols = 6; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"name\", DT_CHAR, 1, 20); construct_column_desc(&rd->cols[2], \"age\", DT_TINYINT, 2, 1); construct_column_desc(&rd->cols[3], \"daily_steps\", DT_SMALLINT, 3, 2); construct_column_desc(&rd->cols[4], \"distance_from_home\", DT_BIGINT, 4, 8); + construct_column_desc(&rd->cols[5], \"is_alive\", DT_BOOL, 5, 1); return rd; } We update our RecordDescriptor factory to create a RecordDescriptor with 6 columns now instead of 5, and add calls to construct_column_desc with the appropriate inputs for our new DT_BOOL column. -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { Datum* data = malloc(rd->ncols * sizeof(Datum)); - populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome); + populate_datum_array(data, person_id, name, age, dailySteps, distanceFromHome, isAlive); fill_record(rd, r + sizeof(RecordHeader), data); free(data); } Our serializer function just needs to include the new column as a parameter, and pass it to the populate_datum_array function. -static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome) { +static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); Record r = record_init(RECORD_LEN); - serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome); + serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome, isAlive); bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); free_record_desc(rd); free(r); return insertSuccessful; } Similar to the above, we just add the new parameter and pass it along. static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; if ( !( strcasecmp(r->name, \"person_id\") == 0 || strcasecmp(r->name, \"name\") == 0 || strcasecmp(r->name, \"age\") == 0 || strcasecmp(r->name, \"daily_steps\") == 0 || - strcasecmp(r->name, \"distance_from_home\") == 0 + strcasecmp(r->name, \"distance_from_home\") == 0 || + strcasecmp(r->name, \"is_alive\") == 0 ) ) { return false; } } return true; } And the last of our temporary functions, the analyzer, just needs to check for the new column in the Select target list. That's it for refactoring the temporary code. Next up, we'll refactor the parser to make sure it looks for the new column in its insert statement grammar.","title":"Refactoring"},{"location":"07-data-types-bool/02-parser-refactor/","text":"Parser Refactor Next up, we need to update our parser grammar to include the new columns in our insert statement. Remember, the table we're working with now is defined as: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); Currently our insert statement syntax looks like this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home]; We're going to change it to this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home] [is_alive]; And fortunately, this type of change is very easy to make. Lexer Update This is something we did not have to do last time. We need to update our lexer to match the strings True and False . That way our grammar knows to set the isAlive property of the insert node to either 1 or 0. Even though we use a uint8_t C type, we don't want to allow its value to be anything other than 1 or 0. src/parser/scan.l /* keywords */ +FALSE { return KW_FALSE; } INSERT { return INSERT; } SELECT { return SELECT; } +TRUE { return KW_TRUE; } We match the literal strings \"FALSE\" and \"TRUE\" and return their associated tokens to bison. Grammar Update src/parser/gram.y %union { char* str; long long numval; + int i; struct Node* node; struct ParseList* list; } I'm adding an int to the union because I don't want to use a long long to represent random numbers here and there. In this case, an int will convert to a uint8_t just fine because we're constraining the values to 1 and 0. /* reserved keywords in alphabetical order */ +%token KW_FALSE %token INSERT %token SELECT +%token KW_TRUE %type <node> cmd stmt sys_cmd select_stmt insert_stmt target %type <list> target_list +%type <i> bool %start query We add our new tokens and a new type for the new grammar rule we're introducing below. -insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER { +insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER bool { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); ins->age = $4; ins->dailySteps = $5; ins->distanceFromHome = $6; + ins->isAlive = $7; $$ = (Node*)ins; } ; +bool: KW_FALSE { + $$ = 0; + } + | KW_TRUE { + $$ = 1; + } + ; For the insert_stmt rule, we simply need to add the bool to the list the same way we added the ints last time. However, we define a new bool grammar rule that accepts either of the new tokens and sets their value to 1 or 0 accordingly. Parsetree Next, we need to update our parsetree code. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int32_t personId; char* name; uint8_t age; int16_t dailySteps; int64_t distanceFromHome; + uint8_t isAlive; } InsertStmt; src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); + printf(\"= is_alive: %u\\n\", ((InsertStmt*)n)->isAlive); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } Updating the node printer to show the parsed is_alive value. I'm printing it as %u because I don't want to write the extra code to translate it to TRUE or FALSE. That ends the changes required in the parser. In the next section, we'll make updates to the actual insert functionality and finally be able to run some insert statements and test out our changes.","title":"Parser Refactor"},{"location":"07-data-types-bool/02-parser-refactor/#parser-refactor","text":"Next up, we need to update our parser grammar to include the new columns in our insert statement. Remember, the table we're working with now is defined as: Create Table person ( person_id Int Not Null, name Char(20) Not Null, age TinyInt Not Null, daily_steps SmallInt Not Null, distance_from_home BigInt Not Null, is_alive Bool Not Null ); Currently our insert statement syntax looks like this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home]; We're going to change it to this: bql > insert [person_id] '[name]' [age] [daily_steps] [distance_from_home] [is_alive]; And fortunately, this type of change is very easy to make.","title":"Parser Refactor"},{"location":"07-data-types-bool/02-parser-refactor/#lexer-update","text":"This is something we did not have to do last time. We need to update our lexer to match the strings True and False . That way our grammar knows to set the isAlive property of the insert node to either 1 or 0. Even though we use a uint8_t C type, we don't want to allow its value to be anything other than 1 or 0. src/parser/scan.l /* keywords */ +FALSE { return KW_FALSE; } INSERT { return INSERT; } SELECT { return SELECT; } +TRUE { return KW_TRUE; } We match the literal strings \"FALSE\" and \"TRUE\" and return their associated tokens to bison.","title":"Lexer Update"},{"location":"07-data-types-bool/02-parser-refactor/#grammar-update","text":"src/parser/gram.y %union { char* str; long long numval; + int i; struct Node* node; struct ParseList* list; } I'm adding an int to the union because I don't want to use a long long to represent random numbers here and there. In this case, an int will convert to a uint8_t just fine because we're constraining the values to 1 and 0. /* reserved keywords in alphabetical order */ +%token KW_FALSE %token INSERT %token SELECT +%token KW_TRUE %type <node> cmd stmt sys_cmd select_stmt insert_stmt target %type <list> target_list +%type <i> bool %start query We add our new tokens and a new type for the new grammar rule we're introducing below. -insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER { +insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER bool { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; ins->name = str_strip_quotes($3); ins->age = $4; ins->dailySteps = $5; ins->distanceFromHome = $6; + ins->isAlive = $7; $$ = (Node*)ins; } ; +bool: KW_FALSE { + $$ = 0; + } + | KW_TRUE { + $$ = 1; + } + ; For the insert_stmt rule, we simply need to add the bool to the list the same way we added the ints last time. However, we define a new bool grammar rule that accepts either of the new tokens and sets their value to 1 or 0 accordingly.","title":"Grammar Update"},{"location":"07-data-types-bool/02-parser-refactor/#parsetree","text":"Next, we need to update our parsetree code. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int32_t personId; char* name; uint8_t age; int16_t dailySteps; int64_t distanceFromHome; + uint8_t isAlive; } InsertStmt; src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); + printf(\"= is_alive: %u\\n\", ((InsertStmt*)n)->isAlive); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } Updating the node printer to show the parsed is_alive value. I'm printing it as %u because I don't want to write the extra code to translate it to TRUE or FALSE. That ends the changes required in the parser. In the next section, we'll make updates to the actual insert functionality and finally be able to run some insert statements and test out our changes.","title":"Parsetree"},{"location":"07-data-types-bool/03-fill-and-defill/","text":"Fill and Defill In this section, we're going to update our code to actually insert our new data, and we'll finally be able to test it out by running some insert statements. Let's pretend we're an insert statement being shuffled along our program's insert code path and make the necessary changes as we find them. Starting at the top, our main function requests input from the user and sends it through the parser. Processing the parse tree is where we come in: src/main.c int main(int argc, char** argv) { *** code omitted for brevity *** break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; uint8_t age = ((InsertStmt*)n)->age; int16_t dailySteps = ((InsertStmt*)n)->dailySteps; int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; + uint8_t isAlive = ((InsertStmt*)n)->isAlive; - if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome)) { + if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome, isAlive)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** } We need to extract the new fields from our InsertStmt node and pass them to the insert_record function. Next we follow the insert path until we get to the fill_record and fill_val functions. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { + case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: dataLen = 1; uint8_t valTinyInt = datumGetUInt8(datum); memcpy(data, &valTinyInt, dataLen); break; case DT_SMALLINT: *** code omitted for brevity *** } data += dataLen; *dataP = data; } Here we can simply piggy-back off of the DT_TINYINT logic because they are the same C-type. Testing It Out That's everything we need for insert operations, let's test it out. Make sure you delete the main.dbd file before compiling and running the program because any existing data was built on the old table definition. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900 true; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 12345678900 = is_alive: 1 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... From the node print piece, you can see the values we parsed out of the insert statement. And \\quit makes sure the data is written to disk. Let's check it out: The orange box, once again, is the page header. Following it is our record data (same as before), and at the end of the record is our Bool column (red box). It's set to 0x01, which translates to True in our DB system. Defill Now, let's refactor the code required for defilling the new data types. The pattern is pretty simple, we just need to add more switch/case branches for the new DT_BOOL type. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { + case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: return record_get_tinyint(r, offset); case DT_SMALLINT: return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); case DT_BIGINT: return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } This is easy once again because we can piggy-back on the existing DT_TINYINT logic. Which is a bonus because we don't need to write a record_get_bool function, so... we're done! In the last section, we'll update the select logic so we're able to output the data to the console.","title":"Fill and Defill"},{"location":"07-data-types-bool/03-fill-and-defill/#fill-and-defill","text":"In this section, we're going to update our code to actually insert our new data, and we'll finally be able to test it out by running some insert statements. Let's pretend we're an insert statement being shuffled along our program's insert code path and make the necessary changes as we find them. Starting at the top, our main function requests input from the user and sends it through the parser. Processing the parse tree is where we come in: src/main.c int main(int argc, char** argv) { *** code omitted for brevity *** break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; char* name = ((InsertStmt*)n)->name; uint8_t age = ((InsertStmt*)n)->age; int16_t dailySteps = ((InsertStmt*)n)->dailySteps; int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; + uint8_t isAlive = ((InsertStmt*)n)->isAlive; - if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome)) { + if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome, isAlive)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** } We need to extract the new fields from our InsertStmt node and pass them to the insert_record function. Next we follow the insert path until we get to the fill_record and fill_val functions. src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { + case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: dataLen = 1; uint8_t valTinyInt = datumGetUInt8(datum); memcpy(data, &valTinyInt, dataLen); break; case DT_SMALLINT: *** code omitted for brevity *** } data += dataLen; *dataP = data; } Here we can simply piggy-back off of the DT_TINYINT logic because they are the same C-type.","title":"Fill and Defill"},{"location":"07-data-types-bool/03-fill-and-defill/#testing-it-out","text":"That's everything we need for insert operations, let's test it out. Make sure you delete the main.dbd file before compiling and running the program because any existing data was built on the old table definition. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 69 'Chris Burke' 45 12345 12345678900 true; ====== Node ====== = Type: Insert = person_id: 69 = name: Chris Burke = age: 45 = daily_steps: 12345 = distance_from_home: 12345678900 = is_alive: 1 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... From the node print piece, you can see the values we parsed out of the insert statement. And \\quit makes sure the data is written to disk. Let's check it out: The orange box, once again, is the page header. Following it is our record data (same as before), and at the end of the record is our Bool column (red box). It's set to 0x01, which translates to True in our DB system.","title":"Testing It Out"},{"location":"07-data-types-bool/03-fill-and-defill/#defill","text":"Now, let's refactor the code required for defilling the new data types. The pattern is pretty simple, we just need to add more switch/case branches for the new DT_BOOL type. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { + case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: return record_get_tinyint(r, offset); case DT_SMALLINT: return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); case DT_BIGINT: return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } This is easy once again because we can piggy-back on the existing DT_TINYINT logic. Which is a bonus because we don't need to write a record_get_bool function, so... we're done! In the last section, we'll update the select logic so we're able to output the data to the console.","title":"Defill"},{"location":"07-data-types-bool/04-select-updates/","text":"Select Updates These changes are extremely simple. We're just going to piggy-back off the DT_TINYINT logic like we've been doing this whole time. src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { + case DT_BOOL: case DT_TINYINT: len = num_digits(datumGetUInt8(data->values[i])); break; case DT_SMALLINT: len = num_digits(datumGetInt16(data->values[i])); break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_BIGINT: len = num_digits(datumGetInt64(data->values[i])); break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { + case DT_BOOL: case DT_TINYINT: numDigits = num_digits(datumGetUInt8(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%u\", datumGetUInt8(d)); break; case DT_SMALLINT: numDigits = num_digits(datumGetInt16(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt16(d)); break; case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; case DT_BIGINT: numDigits = num_digits(datumGetInt64(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%ld\", datumGetInt64(d)); break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { *** omitted for brevity *** while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { + case DT_BOOL: case DT_TINYINT: case DT_SMALLINT: case DT_INT: case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Now let's test it out: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select name, is_alive; ====== Node ====== = Type: Select = Targets: = name = is_alive -------- *** Rows: 1 -------- |name |is_alive | ------------------------ |Chris Burke | 1| (Rows: 1) bql > We're just selecting the name and is_alive columns, but our code is correctly displaying the Bool data type.","title":"Select Updates"},{"location":"07-data-types-bool/04-select-updates/#select-updates","text":"These changes are extremely simple. We're just going to piggy-back off the DT_TINYINT logic like we've been doing this whole time. src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { + case DT_BOOL: case DT_TINYINT: len = num_digits(datumGetUInt8(data->values[i])); break; case DT_SMALLINT: len = num_digits(datumGetInt16(data->values[i])); break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_BIGINT: len = num_digits(datumGetInt64(data->values[i])); break; case DT_CHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } static void print_cell_num(DataType dt, Datum d, int width) { int numDigits; char* cell; switch (dt) { + case DT_BOOL: case DT_TINYINT: numDigits = num_digits(datumGetUInt8(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%u\", datumGetUInt8(d)); break; case DT_SMALLINT: numDigits = num_digits(datumGetInt16(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt16(d)); break; case DT_INT: numDigits = num_digits(datumGetInt32(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%d\", datumGetInt32(d)); break; case DT_BIGINT: numDigits = num_digits(datumGetInt64(d)); cell = malloc(numDigits + 1); sprintf(cell, \"%ld\", datumGetInt64(d)); break; } cell[numDigits] = '\\0'; print_cell_with_padding(cell, width, true); if (cell != NULL) free(cell); } void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { *** omitted for brevity *** while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { + case DT_BOOL: case DT_TINYINT: case DT_SMALLINT: case DT_INT: case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } Now let's test it out: $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select name, is_alive; ====== Node ====== = Type: Select = Targets: = name = is_alive -------- *** Rows: 1 -------- |name |is_alive | ------------------------ |Chris Burke | 1| (Rows: 1) bql > We're just selecting the name and is_alive columns, but our code is correctly displaying the Bool data type.","title":"Select Updates"},{"location":"08-data-types-varchar/01-intro/","text":"Varchar Intro The last data type we're going to add (for now) to our database system is the variable-length Varchar type. Its purpose is to store text data, just like the Char type, but unlike the Char , our storage engine will only store as many bytes required to represent the data. For example, consider these two tables: Create Table person_char ( person_id Not Null, first_name Char(20) Not Null, last_name Char(20) Not Null ); Create Table person_varchar ( person_id Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null ); Let's say we want to store the following values in each table. How many bytes for the data do records in each table require (ignoring overhead)? person_id first_name last_name 1 Chris Burke Let's compare... column person_char person_varchar person_id 4-bytes 4-bytes first_name 20-bytes 5-bytes last_name 20-bytes 5-bytes TOTAL 44-bytes 14-bytes As you can see, Varchar columns have a humongous advantage over Char columns simply because they can adapt to the size of the data stored in them. Char 's are so archaic that if you're ever designing a data model, you should pretend Char doesn't exist - there's no reason to choose them over Varchar (or Text in postgres). While Varchar 's do offer a big storage savings over Char 's, it's important to note there is some overhead associated with storing Varchar data. Namely, we must store its length so that our code knows exactly where to stop reading from the data page. Different database systems implement the storage mechanism for variable-length columns differently, i.e. Postgres does it different from MS SQL Server does it different from MySQL. Our implementation will be somewhat similar to MS SQL Server. The unique thing about SQL Server is the storage engine will group all fixed-length columns at the beginning of the record, and all variable-length columns at the end. It doesn't matter what order we list the columns in our Create Table statement, the engine always places the variable-length columns at the end. Then we also store a 2-byte field prepended to the Varchar data that has the length of the data, including the 2-byte overhead. So if we revisit the Char vs Varchar comparison from above, but this time include the Varchar overhead, we'll get: column person_char person_varchar person_id 4-bytes 4-bytes first_name 20-bytes 7-bytes last_name 20-bytes 7-bytes TOTAL 44-bytes 18-bytes We need two additional bytes for each of the Varchar columns. Alright enough yappin', let's get to the code.","title":"Intro"},{"location":"08-data-types-varchar/01-intro/#varchar-intro","text":"The last data type we're going to add (for now) to our database system is the variable-length Varchar type. Its purpose is to store text data, just like the Char type, but unlike the Char , our storage engine will only store as many bytes required to represent the data. For example, consider these two tables: Create Table person_char ( person_id Not Null, first_name Char(20) Not Null, last_name Char(20) Not Null ); Create Table person_varchar ( person_id Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null ); Let's say we want to store the following values in each table. How many bytes for the data do records in each table require (ignoring overhead)? person_id first_name last_name 1 Chris Burke Let's compare... column person_char person_varchar person_id 4-bytes 4-bytes first_name 20-bytes 5-bytes last_name 20-bytes 5-bytes TOTAL 44-bytes 14-bytes As you can see, Varchar columns have a humongous advantage over Char columns simply because they can adapt to the size of the data stored in them. Char 's are so archaic that if you're ever designing a data model, you should pretend Char doesn't exist - there's no reason to choose them over Varchar (or Text in postgres). While Varchar 's do offer a big storage savings over Char 's, it's important to note there is some overhead associated with storing Varchar data. Namely, we must store its length so that our code knows exactly where to stop reading from the data page. Different database systems implement the storage mechanism for variable-length columns differently, i.e. Postgres does it different from MS SQL Server does it different from MySQL. Our implementation will be somewhat similar to MS SQL Server. The unique thing about SQL Server is the storage engine will group all fixed-length columns at the beginning of the record, and all variable-length columns at the end. It doesn't matter what order we list the columns in our Create Table statement, the engine always places the variable-length columns at the end. Then we also store a 2-byte field prepended to the Varchar data that has the length of the data, including the 2-byte overhead. So if we revisit the Char vs Varchar comparison from above, but this time include the Varchar overhead, we'll get: column person_char person_varchar person_id 4-bytes 4-bytes first_name 20-bytes 7-bytes last_name 20-bytes 7-bytes TOTAL 44-bytes 18-bytes We need two additional bytes for each of the Varchar columns. Alright enough yappin', let's get to the code.","title":"Varchar Intro"},{"location":"08-data-types-varchar/02-storage-and-fill/","text":"Storage and Fill As has been the case for each new data type, we need to change the definition of the table we're working with. For this Varchar section, our table looks like this: Create Table person ( person_id Int Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null, age Int Not Null ); The only purpose of that age column is to demonstrate how the storage engine physically stores fixed-length columns before variable-length columns. But more on that later. Fortunately, we don't have to refactor the code too much in order to support Varchars, but there is some nuance to the fill and defill logic that might take a bit to wrap your head around. We'll get to that later though. Storage Engine First up, we start with the simple changes: supporting a new data type. src/include/storage/record.h typedef enum DataType { DT_TINYINT, /* 1-byte, unsigned */ DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ DT_BIGINT, /* 8-bytes, signed */ DT_BOOL, /* 1-byte, unsigned | similar to DT_TINYINT, but always evaluates to 1 or 0 */ DT_CHAR, /* Byte-size defined at table creation */ + DT_VARCHAR, /* Variable length. A 2-byte \"header\" stores the length of the column + followed by the actual column bytes */ DT_UNKNOWN } DataType; Next, since our storage engine separates the column data into two sections: fixed-length and variable-length, we need to add some metadata to support that downstream logic. src/include/storage/record.h typedef struct RecordDescriptor { int ncols; /* number of columns (defined by the Create Table DDL) */ + int nfixed; /* number of fixed-length columns */ Column cols[]; } RecordDescriptor; We add a property to the RecordDescriptor to keep track of the number of fixed-length columns in the record. Since this struct is passed all over the place in the storage engine, we'll be able to use it to accurately read and write column data in the correct spot. src/include/storage/record.h -void fill_record(RecordDescriptor* rd, Record r, Datum* data); +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen); We also need to change the definition of the fill_record function to support the two different sections in the physical record structure. Since we need to write all fixed-length data first, then write the variable-length data, it'll be easier to work with them in separate variables. Fill Let's step through the fill_record function and refactor the code as needed. Since separating fixed-length and variable-length columns is such a fundamental change, the fill_record function is going to be a complete rewrite. So I'm not providing a diff. src/storage/record.c void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen) { // fill fixed-length columns for (int i = 0; i < rd->nfixed; i++) { Column* col = get_nth_col(rd, true, i); fill_val(col, &r, fixed[i]); } // fill varlen columns for (int i = 0; i < (rd->ncols - rd->nfixed); i++) { Column* col = get_nth_col(rd, false, i); fill_val(col, &r, varlen[i]); } } The logic inside each for loop is pretty much the same, but now we have two loops. First, we write the fixed-length columns to the record, then we write the variable-length columns. And because the columns in the RecordDescriptor can be in a different order than the storage engine writes them, we need a helper function get us the correct Column from the RecordDescriptor . src/storage/record.c static Column* get_nth_col(RecordDescriptor* rd, bool isFixed, int n) { int nCol = 0; for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].dataType == DT_VARCHAR) { // column is variable-length if (!isFixed && nCol == n) return &rd->cols[i]; if (!isFixed) nCol++; } else { // column is fixed length if (isFixed && nCol == n) return &rd->cols[i]; if (isFixed) nCol++; } } return NULL; } This function is fairly straightforward. We just loop through the RecordDescriptor 's Column array and return the n th column according to the isFixed parameter. Although pretty simple, this function is crucial to reconciling the different physical ordering of columns compared to the order in the Create Table statement. Next, let's step into fill_val . src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { *** code omitted for brevity *** free(str); break; + case DT_VARCHAR: + fill_varchar(col, data, &dataLen, datum); + break; } data += dataLen; *dataP = data; } We just need to add a new case to the switch/case block and call our new fill_varchar function (defined below). src/storage/record.c static void fill_varchar(Column* col, char* data, int16_t* dataLen, Datum value) { int16_t charLen = strlen(datumGetString(value)); if (charLen > col->len) charLen = col->len; charLen += 2; // account for the 2-byte length overhead // write the 2-byte length overhead memcpy(data, &charLen, 2); // write the actual data memcpy(data + 2, datumGetString(value), charLen - 2); *dataLen = charLen; } This logic is pretty similar to the DT_CHAR case, except we need to also write the 2-byte length overhead directly before the column data itself. And remember, the length overhead also includes the 2-byte length of itself, hence the charLen += 2; statement. And that does it for the \"fill\" part of the storage engine. Next, we'll muddle through the tedious task of updating our temporary code - we are so close to ditching the temp code for good!","title":"Storage and Fill"},{"location":"08-data-types-varchar/02-storage-and-fill/#storage-and-fill","text":"As has been the case for each new data type, we need to change the definition of the table we're working with. For this Varchar section, our table looks like this: Create Table person ( person_id Int Not Null, first_name Varchar(20) Not Null, last_name Varchar(20) Not Null, age Int Not Null ); The only purpose of that age column is to demonstrate how the storage engine physically stores fixed-length columns before variable-length columns. But more on that later. Fortunately, we don't have to refactor the code too much in order to support Varchars, but there is some nuance to the fill and defill logic that might take a bit to wrap your head around. We'll get to that later though.","title":"Storage and Fill"},{"location":"08-data-types-varchar/02-storage-and-fill/#storage-engine","text":"First up, we start with the simple changes: supporting a new data type. src/include/storage/record.h typedef enum DataType { DT_TINYINT, /* 1-byte, unsigned */ DT_SMALLINT, /* 2-bytes, signed */ DT_INT, /* 4-bytes, signed */ DT_BIGINT, /* 8-bytes, signed */ DT_BOOL, /* 1-byte, unsigned | similar to DT_TINYINT, but always evaluates to 1 or 0 */ DT_CHAR, /* Byte-size defined at table creation */ + DT_VARCHAR, /* Variable length. A 2-byte \"header\" stores the length of the column + followed by the actual column bytes */ DT_UNKNOWN } DataType; Next, since our storage engine separates the column data into two sections: fixed-length and variable-length, we need to add some metadata to support that downstream logic. src/include/storage/record.h typedef struct RecordDescriptor { int ncols; /* number of columns (defined by the Create Table DDL) */ + int nfixed; /* number of fixed-length columns */ Column cols[]; } RecordDescriptor; We add a property to the RecordDescriptor to keep track of the number of fixed-length columns in the record. Since this struct is passed all over the place in the storage engine, we'll be able to use it to accurately read and write column data in the correct spot. src/include/storage/record.h -void fill_record(RecordDescriptor* rd, Record r, Datum* data); +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen); We also need to change the definition of the fill_record function to support the two different sections in the physical record structure. Since we need to write all fixed-length data first, then write the variable-length data, it'll be easier to work with them in separate variables.","title":"Storage Engine"},{"location":"08-data-types-varchar/02-storage-and-fill/#fill","text":"Let's step through the fill_record function and refactor the code as needed. Since separating fixed-length and variable-length columns is such a fundamental change, the fill_record function is going to be a complete rewrite. So I'm not providing a diff. src/storage/record.c void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen) { // fill fixed-length columns for (int i = 0; i < rd->nfixed; i++) { Column* col = get_nth_col(rd, true, i); fill_val(col, &r, fixed[i]); } // fill varlen columns for (int i = 0; i < (rd->ncols - rd->nfixed); i++) { Column* col = get_nth_col(rd, false, i); fill_val(col, &r, varlen[i]); } } The logic inside each for loop is pretty much the same, but now we have two loops. First, we write the fixed-length columns to the record, then we write the variable-length columns. And because the columns in the RecordDescriptor can be in a different order than the storage engine writes them, we need a helper function get us the correct Column from the RecordDescriptor . src/storage/record.c static Column* get_nth_col(RecordDescriptor* rd, bool isFixed, int n) { int nCol = 0; for (int i = 0; i < rd->ncols; i++) { if (rd->cols[i].dataType == DT_VARCHAR) { // column is variable-length if (!isFixed && nCol == n) return &rd->cols[i]; if (!isFixed) nCol++; } else { // column is fixed length if (isFixed && nCol == n) return &rd->cols[i]; if (isFixed) nCol++; } } return NULL; } This function is fairly straightforward. We just loop through the RecordDescriptor 's Column array and return the n th column according to the isFixed parameter. Although pretty simple, this function is crucial to reconciling the different physical ordering of columns compared to the order in the Create Table statement. Next, let's step into fill_val . src/storage/record.c static void fill_val(Column* col, char** dataP, Datum datum) { int16_t dataLen; char* data = *dataP; switch (col->dataType) { *** code omitted for brevity *** free(str); break; + case DT_VARCHAR: + fill_varchar(col, data, &dataLen, datum); + break; } data += dataLen; *dataP = data; } We just need to add a new case to the switch/case block and call our new fill_varchar function (defined below). src/storage/record.c static void fill_varchar(Column* col, char* data, int16_t* dataLen, Datum value) { int16_t charLen = strlen(datumGetString(value)); if (charLen > col->len) charLen = col->len; charLen += 2; // account for the 2-byte length overhead // write the 2-byte length overhead memcpy(data, &charLen, 2); // write the actual data memcpy(data + 2, datumGetString(value), charLen - 2); *dataLen = charLen; } This logic is pretty similar to the DT_CHAR case, except we need to also write the 2-byte length overhead directly before the column data itself. And remember, the length overhead also includes the 2-byte length of itself, hence the charLen += 2; statement. And that does it for the \"fill\" part of the storage engine. Next, we'll muddle through the tedious task of updating our temporary code - we are so close to ditching the temp code for good!","title":"Fill"},{"location":"08-data-types-varchar/03-temporary-code-refactor/","text":"Temporary Code Refactor Ugh, I know. You hate it. I hate it. But we're getting closer to ditching it for good. If you look at the project roadmap, the \"System Catalog\" sections are when we'll drop the temp code. Anyways, let's not drag this out and just get it over with. All diffs are in the main.c file. -#define RECORD_LEN 48 #define BUFPOOL_SLOTS 1 We can get rid of the RECORD_LEN macro because we no longer need to use it (you'll see why below). static void populate_datum_array(Datum* fixed, Datum* varlen, int32_t person_id, char* firstName, char* lastName, int32_t age) { fixed[0] = int32GetDatum(person_id); fixed[1] = int32GetDatum(age); varlen[0] = charGetDatum(firstName); varlen[1] = charGetDatum(lastName); } This function is a full rewrite. Since we're switching from a single Datum array to two separate arrays for the fixed and variable length columns, we can just scrap the only function and use this new version. static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (4 * sizeof(Column))); rd->ncols = 4; rd->nfixed = 2; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20); construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20); construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4); return rd; } Same thing goes for the RecordDescriptor factory. So much of the function guts changed, providing a diff would be too noisy. Notice, we're now setting rd->nfixed = 2; to support the two column buckets. Also of note, this is where we set the colloquial ordering of the table's columns. You can see the age column is index 3 - the final column in the table. The \"fill\" code we wrote in the previous section ensures it will end up before the varchar columns despite being defined as after them. static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* firstName, char* lastName, int32_t age) { Datum* fixed = malloc(rd->nfixed * sizeof(Datum)); Datum* varlen = malloc((rd->ncols - rd->nfixed) * sizeof(Datum)); populate_datum_array(fixed, varlen, person_id, firstName, lastName, age); fill_record(rd, r + sizeof(RecordHeader), fixed, varlen); free(fixed); free(varlen); } Another function whose guts almost entirely changed due to the separate fixed/varlen Datum arrays. The logic is the same, however, so I don't really need to explain what's going on. static int compute_record_length(RecordDescriptor* rd, int32_t person_id, char* firstName, char* lastName, int32_t age) { int len = 12; // start with the 12-byte header len += 4; // person_id /* for the varlen columns, we default to their max length if the values we're trying to insert would overflow them (they'll get truncated later) */ if (strlen(firstName) > rd->cols[1].len) { len += (rd->cols[1].len + 2); } else { len += (strlen(firstName) + 2); } if (strlen(lastName) > rd->cols[2].len) { len += (rd->cols[2].len + 2); } else { len += (strlen(lastName) + 2); } len += 4; // age return len; } This is a brand new function and is the reason we no longer need the RECORD_LEN macro. Since Varchar columns can differ in length on a row-by-row basis, we need a way to calculate the byte-length of a row on the fly. This is very much a hard-coded implementation. For the fixed-length columns (and the header) we can just add their static lengths to the total length. But for the Varchar columns, we need to compute the strlen of the incoming values. If the strlen exceeds the column's max length (defined in the Column* struct in the RecordDescriptor ), then we set its length to the max length. -static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { +static bool insert_record(BufPool* bp, int32_t person_id, char* firstName, char* lastName, int32_t age) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); - Record r = record_init(RECORD_LEN); + int recordLength = compute_record_length(rd, person_id, firstName, lastName, age); + Record r = record_init(recordLength); + - serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome, isAlive); + serialize_data(rd, r, person_id, firstName, lastName, age); - bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); + bool insertSuccessful = page_insert(slot->pg, r, recordLength); free_record_desc(rd); free(r); return insertSuccessful; } static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; if ( !( strcasecmp(r->name, \"person_id\") == 0 || - strcasecmp(r->name, \"name\") == 0 || - strcasecmp(r->name, \"age\") == 0 || - strcasecmp(r->name, \"daily_steps\") == 0 || - strcasecmp(r->name, \"distance_from_home\") == 0 || - strcasecmp(r->name, \"is_alive\") == 0 + strcasecmp(r->name, \"first_name\") == 0 || + strcasecmp(r->name, \"last_name\") == 0 || + strcasecmp(r->name, \"age\") == 0 ) ) { return false; } } return true; } int main(int argc, char** argv) { *** code omitted for brevity *** break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; - char* name = ((InsertStmt*)n)->name; - uint8_t age = ((InsertStmt*)n)->age; - int16_t dailySteps = ((InsertStmt*)n)->dailySteps; - int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; - uint8_t isAlive = ((InsertStmt*)n)->isAlive; + char* firstName = ((InsertStmt*)n)->firstName; + char* lastName = ((InsertStmt*)n)->lastName; + int32_t age = ((InsertStmt*)n)->age; - if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome, isAlive)) { + if (!insert_record(bp, person_id, firstName, lastName, age)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** return EXIT_SUCCESS; } The rest of the changes are just accounting for the new columns in the table definition. All of the logic remains the same as before.","title":"Temporary Code Refactor"},{"location":"08-data-types-varchar/03-temporary-code-refactor/#temporary-code-refactor","text":"Ugh, I know. You hate it. I hate it. But we're getting closer to ditching it for good. If you look at the project roadmap, the \"System Catalog\" sections are when we'll drop the temp code. Anyways, let's not drag this out and just get it over with. All diffs are in the main.c file. -#define RECORD_LEN 48 #define BUFPOOL_SLOTS 1 We can get rid of the RECORD_LEN macro because we no longer need to use it (you'll see why below). static void populate_datum_array(Datum* fixed, Datum* varlen, int32_t person_id, char* firstName, char* lastName, int32_t age) { fixed[0] = int32GetDatum(person_id); fixed[1] = int32GetDatum(age); varlen[0] = charGetDatum(firstName); varlen[1] = charGetDatum(lastName); } This function is a full rewrite. Since we're switching from a single Datum array to two separate arrays for the fixed and variable length columns, we can just scrap the only function and use this new version. static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (4 * sizeof(Column))); rd->ncols = 4; rd->nfixed = 2; construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20); construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20); construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4); return rd; } Same thing goes for the RecordDescriptor factory. So much of the function guts changed, providing a diff would be too noisy. Notice, we're now setting rd->nfixed = 2; to support the two column buckets. Also of note, this is where we set the colloquial ordering of the table's columns. You can see the age column is index 3 - the final column in the table. The \"fill\" code we wrote in the previous section ensures it will end up before the varchar columns despite being defined as after them. static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* firstName, char* lastName, int32_t age) { Datum* fixed = malloc(rd->nfixed * sizeof(Datum)); Datum* varlen = malloc((rd->ncols - rd->nfixed) * sizeof(Datum)); populate_datum_array(fixed, varlen, person_id, firstName, lastName, age); fill_record(rd, r + sizeof(RecordHeader), fixed, varlen); free(fixed); free(varlen); } Another function whose guts almost entirely changed due to the separate fixed/varlen Datum arrays. The logic is the same, however, so I don't really need to explain what's going on. static int compute_record_length(RecordDescriptor* rd, int32_t person_id, char* firstName, char* lastName, int32_t age) { int len = 12; // start with the 12-byte header len += 4; // person_id /* for the varlen columns, we default to their max length if the values we're trying to insert would overflow them (they'll get truncated later) */ if (strlen(firstName) > rd->cols[1].len) { len += (rd->cols[1].len + 2); } else { len += (strlen(firstName) + 2); } if (strlen(lastName) > rd->cols[2].len) { len += (rd->cols[2].len + 2); } else { len += (strlen(lastName) + 2); } len += 4; // age return len; } This is a brand new function and is the reason we no longer need the RECORD_LEN macro. Since Varchar columns can differ in length on a row-by-row basis, we need a way to calculate the byte-length of a row on the fly. This is very much a hard-coded implementation. For the fixed-length columns (and the header) we can just add their static lengths to the total length. But for the Varchar columns, we need to compute the strlen of the incoming values. If the strlen exceeds the column's max length (defined in the Column* struct in the RecordDescriptor ), then we set its length to the max length. -static bool insert_record(BufPool* bp, int32_t person_id, char* name, uint8_t age, int16_t dailySteps, int64_t distanceFromHome, uint8_t isAlive) { +static bool insert_record(BufPool* bp, int32_t person_id, char* firstName, char* lastName, int32_t age) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); - Record r = record_init(RECORD_LEN); + int recordLength = compute_record_length(rd, person_id, firstName, lastName, age); + Record r = record_init(recordLength); + - serialize_data(rd, r, person_id, name, age, dailySteps, distanceFromHome, isAlive); + serialize_data(rd, r, person_id, firstName, lastName, age); - bool insertSuccessful = page_insert(slot->pg, r, RECORD_LEN); + bool insertSuccessful = page_insert(slot->pg, r, recordLength); free_record_desc(rd); free(r); return insertSuccessful; } static bool analyze_selectstmt(SelectStmt* s) { for (int i = 0; i < s->targetList->length; i++) { ResTarget* r = (ResTarget*)s->targetList->elements[i].ptr; if ( !( strcasecmp(r->name, \"person_id\") == 0 || - strcasecmp(r->name, \"name\") == 0 || - strcasecmp(r->name, \"age\") == 0 || - strcasecmp(r->name, \"daily_steps\") == 0 || - strcasecmp(r->name, \"distance_from_home\") == 0 || - strcasecmp(r->name, \"is_alive\") == 0 + strcasecmp(r->name, \"first_name\") == 0 || + strcasecmp(r->name, \"last_name\") == 0 || + strcasecmp(r->name, \"age\") == 0 ) ) { return false; } } return true; } int main(int argc, char** argv) { *** code omitted for brevity *** break; case T_InsertStmt: { int32_t person_id = ((InsertStmt*)n)->personId; - char* name = ((InsertStmt*)n)->name; - uint8_t age = ((InsertStmt*)n)->age; - int16_t dailySteps = ((InsertStmt*)n)->dailySteps; - int64_t distanceFromHome = ((InsertStmt*)n)->distanceFromHome; - uint8_t isAlive = ((InsertStmt*)n)->isAlive; + char* firstName = ((InsertStmt*)n)->firstName; + char* lastName = ((InsertStmt*)n)->lastName; + int32_t age = ((InsertStmt*)n)->age; - if (!insert_record(bp, person_id, name, age, dailySteps, distanceFromHome, isAlive)) { + if (!insert_record(bp, person_id, firstName, lastName, age)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: *** code omitted for brevity *** return EXIT_SUCCESS; } The rest of the changes are just accounting for the new columns in the table definition. All of the logic remains the same as before.","title":"Temporary Code Refactor"},{"location":"08-data-types-varchar/04-inserting-data/","text":"Inserting Data Before we can run the program and insert data, we need to make a couple small changes to the parser. Worry not, the changes are pretty small. Parser Refactor src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int32_t personId; - char* name; - uint8_t age; - int16_t dailySteps; - int64_t distanceFromHome; - uint8_t isAlive; + char* firstName; + char* lastName; + int32_t age; } InsertStmt; src/parser/parsetree.c static void free_insert_stmt(InsertStmt* ins) { if (ins == NULL) return; - if (ins->name != NULL) free(ins->name); + free(ins->firstName); + free(ins->lastName); } src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); - printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); - printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); - printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); - printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); - printf(\"= is_alive: %u\\n\", ((InsertStmt*)n)->isAlive); + printf(\"= first_name: %s\\n\", ((InsertStmt*)n)->firstName); + printf(\"= last_name: %s\\n\", ((InsertStmt*)n)->lastName); + printf(\"= age: %d\\n\", ((InsertStmt*)n)->age); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } src/parser/gram.y -insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER bool { +insert_stmt: INSERT NUMBER STRING STRING NUMBER { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; - ins->name = str_strip_quotes($3); - ins->age = $4; - ins->dailySteps = $5; - ins->distanceFromHome = $6; - ins->isAlive = $7; + ins->firstName = str_strip_quotes($3); + ins->lastName = str_strip_quotes($4); + ins->age = $5; $$ = (Node*)ins; } ; All of the above changes are simply there to reflect the new table definition we're working with. Insert Data We're now at the point where we can insert data and inspect the contents of the data page. Let's dive right in (remember to delete your old data file first): $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 1 'Chris' 'Burke' 33; ====== Node ====== = Type: Insert = person_id: 1 = first_name: Chris = last_name: Burke = age: 33 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... Here I insert: | person_id | first_name | last_name | age | | 1 | Chris | Burke | 33 | Let's check out the contents of the page: Using the command xxd db_files/main.dbd Since we already know about the page header, record header, and slot pointers I am not highlighting those sections. Instead, let's focus on the interesting stuff. The red box is our person_id column - set to 1 as expected. The orange box is our age column - set to 33 as expected (21 in hex translates to 33 in decimal). We can see our storage engine successfully stored the age data before the varchar columns despite age being the last column in the table definition. Next we have the two varchar columns. The yellow boxes are the 2-byte length fields - both set to 7. And following that are the column data themselves (green boxes). Let's insert another record. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 2 'I am longer than twenty characters' 'was it truncated?' 69; ====== Node ====== = Type: Insert = person_id: 2 = first_name: I am longer than twenty characters = last_name: was it truncated? = age: 69 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... This time, I tried to insert text that exceeds the 20-character limit of the Varchar column. Let's see what our storage engine wrote to disk: Just highlighting our varchar columns, you can see the text representation on the right side and that yes our long column was indeed truncated.","title":"Inserting Data"},{"location":"08-data-types-varchar/04-inserting-data/#inserting-data","text":"Before we can run the program and insert data, we need to make a couple small changes to the parser. Worry not, the changes are pretty small.","title":"Inserting Data"},{"location":"08-data-types-varchar/04-inserting-data/#parser-refactor","text":"src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; int32_t personId; - char* name; - uint8_t age; - int16_t dailySteps; - int64_t distanceFromHome; - uint8_t isAlive; + char* firstName; + char* lastName; + int32_t age; } InsertStmt; src/parser/parsetree.c static void free_insert_stmt(InsertStmt* ins) { if (ins == NULL) return; - if (ins->name != NULL) free(ins->name); + free(ins->firstName); + free(ins->lastName); } src/parser/parsetree.c void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: printf(\"= Type: Insert\\n\"); printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); - printf(\"= name: %s\\n\", ((InsertStmt*)n)->name); - printf(\"= age: %u\\n\", ((InsertStmt*)n)->age); - printf(\"= daily_steps: %d\\n\", ((InsertStmt*)n)->dailySteps); - printf(\"= distance_from_home: %ld\\n\", ((InsertStmt*)n)->distanceFromHome); - printf(\"= is_alive: %u\\n\", ((InsertStmt*)n)->isAlive); + printf(\"= first_name: %s\\n\", ((InsertStmt*)n)->firstName); + printf(\"= last_name: %s\\n\", ((InsertStmt*)n)->lastName); + printf(\"= age: %d\\n\", ((InsertStmt*)n)->age); break; case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } src/parser/gram.y -insert_stmt: INSERT NUMBER STRING NUMBER NUMBER NUMBER bool { +insert_stmt: INSERT NUMBER STRING STRING NUMBER { InsertStmt* ins = create_node(InsertStmt); ins->personId = $2; - ins->name = str_strip_quotes($3); - ins->age = $4; - ins->dailySteps = $5; - ins->distanceFromHome = $6; - ins->isAlive = $7; + ins->firstName = str_strip_quotes($3); + ins->lastName = str_strip_quotes($4); + ins->age = $5; $$ = (Node*)ins; } ; All of the above changes are simply there to reflect the new table definition we're working with.","title":"Parser Refactor"},{"location":"08-data-types-varchar/04-inserting-data/#insert-data","text":"We're now at the point where we can insert data and inspect the contents of the data page. Let's dive right in (remember to delete your old data file first): $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 1 'Chris' 'Burke' 33; ====== Node ====== = Type: Insert = person_id: 1 = first_name: Chris = last_name: Burke = age: 33 Bytes read: 0 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... Here I insert: | person_id | first_name | last_name | age | | 1 | Chris | Burke | 33 | Let's check out the contents of the page: Using the command xxd db_files/main.dbd Since we already know about the page header, record header, and slot pointers I am not highlighting those sections. Instead, let's focus on the interesting stuff. The red box is our person_id column - set to 1 as expected. The orange box is our age column - set to 33 as expected (21 in hex translates to 33 in decimal). We can see our storage engine successfully stored the age data before the varchar columns despite age being the last column in the table definition. Next we have the two varchar columns. The yellow boxes are the 2-byte length fields - both set to 7. And following that are the column data themselves (green boxes). Let's insert another record. $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 2 'I am longer than twenty characters' 'was it truncated?' 69; ====== Node ====== = Type: Insert = person_id: 2 = first_name: I am longer than twenty characters = last_name: was it truncated? = age: 69 bql > \\quit ====== Node ====== = Type: SysCmd = Cmd: quit Shutting down... This time, I tried to insert text that exceeds the 20-character limit of the Varchar column. Let's see what our storage engine wrote to disk: Just highlighting our varchar columns, you can see the text representation on the right side and that yes our long column was indeed truncated.","title":"Insert Data"},{"location":"08-data-types-varchar/05-defill-and-display/","text":"Defill and Display The final piece to our Varchar journey is updating our defill and display code so we can run select statements. Defill src/storage/record.c void defill_record(RecordDescriptor* rd, Record r, Datum* values) { int offset = sizeof(RecordHeader); + Column* col; for (int i = 0; i < rd->ncols; i++) { - Column* col = &rd->cols[i]; - values[i] = record_get_col_value(col, r, &offset); + if (i < rd->nfixed) { + col = get_nth_col(rd, true, i); + } else { + col = get_nth_col(rd, false, i - rd->nfixed); + } + + values[col->colnum] = record_get_col_value(col, r, &offset); } } We're essentially fully rewriting the logic to gracefully work with the fixed/varlen column separation on disk. We loop through each column and utilize our get_nth_col function to make sure we're grabbing the correct information. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: return record_get_tinyint(r, offset); case DT_SMALLINT: return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); case DT_BIGINT: return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); + case DT_VARCHAR: + return record_get_varchar(r, offset); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } Add a new case statement for the Varchar type. src/storage/record.c static Datum record_get_varchar(Record r, int* offset) { int16_t len; memcpy(&len, r + *offset, 2); /* The `len - 1` expression is a combination of two steps: We subtract 2 bytes because we don't need memory for the 2-byte length overhead. Then we add 1 byte to account for the null terminator we need to append on the end of the string. */ char* pChar = malloc(len - 1); memcpy(pChar, r + *offset + 2, len - 2); pChar[len - 2] = '\\0'; *offset += len; return charGetDatum(pChar); } Create a new function to extract the varchar data. Display src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { case DT_BOOL: case DT_TINYINT: len = num_digits(datumGetUInt8(data->values[i])); break; case DT_SMALLINT: len = num_digits(datumGetInt16(data->values[i])); break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_BIGINT: len = num_digits(datumGetInt64(data->values[i])); break; case DT_CHAR: + case DT_VARCHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } src/resultset/resultset_print.c void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { printf(\"--------\\n\"); printf(\"*** Rows: %d\\n\", rs->rows->numItems); printf(\"--------\\n\"); if (rs->rows->numItems == 0) return; int* widths = malloc(sizeof(int) * rd->ncols); compute_column_widths(rd, rs, widths); print_column_headers(rd, targets, widths); ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_BOOL: case DT_TINYINT: case DT_SMALLINT: case DT_INT: case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: + case DT_VARCHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } No new logic necessary, we just need to add DT_VARCHAR to the switch/case blocks and piggy back off of the DT_CHAR logic. src/resultset/recordset.c static void free_recordset_row_columns(RecordSetRow* row, RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { - if (rd->cols[i].dataType == DT_CHAR) { + if (rd->cols[i].dataType == DT_CHAR || rd->cols[i].dataType == DT_VARCHAR) { free((void*)row->values[i]); } } } Lastly, we just need to make sure we free the memory associated with DT_VARCHAR values. Running Select Statements Now we can test our changes by running select statements: $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, first_name, last_name, age; ====== Node ====== = Type: Select = Targets: = person_id = first_name = last_name = age -------- *** Rows: 2 -------- |person_id |first_name |last_name |age | ---------------------------------------------------------- | 1|Chris |Burke | 33| | 2|I am longer than twe |was it truncated? | 69| (Rows: 2) bql > Success! Our resultset display is functioning properly. In the next section, we're going to introduce Null columns to our database. Don't worry though, the columns in our table will stay the same so the temporary code refactor will be minimal.","title":"Defill and Display"},{"location":"08-data-types-varchar/05-defill-and-display/#defill-and-display","text":"The final piece to our Varchar journey is updating our defill and display code so we can run select statements.","title":"Defill and Display"},{"location":"08-data-types-varchar/05-defill-and-display/#defill","text":"src/storage/record.c void defill_record(RecordDescriptor* rd, Record r, Datum* values) { int offset = sizeof(RecordHeader); + Column* col; for (int i = 0; i < rd->ncols; i++) { - Column* col = &rd->cols[i]; - values[i] = record_get_col_value(col, r, &offset); + if (i < rd->nfixed) { + col = get_nth_col(rd, true, i); + } else { + col = get_nth_col(rd, false, i - rd->nfixed); + } + + values[col->colnum] = record_get_col_value(col, r, &offset); } } We're essentially fully rewriting the logic to gracefully work with the fixed/varlen column separation on disk. We loop through each column and utilize our get_nth_col function to make sure we're grabbing the correct information. src/storage/record.c static Datum record_get_col_value(Column* col, Record r, int* offset) { switch (col->dataType) { case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: return record_get_tinyint(r, offset); case DT_SMALLINT: return record_get_smallint(r, offset); case DT_INT: return record_get_int(r, offset); case DT_BIGINT: return record_get_bigint(r, offset); case DT_CHAR: return record_get_char(r, offset, col->len); + case DT_VARCHAR: + return record_get_varchar(r, offset); default: printf(\"record_get_col_value() | Unknown data type!\\n\"); return (Datum)NULL; } } Add a new case statement for the Varchar type. src/storage/record.c static Datum record_get_varchar(Record r, int* offset) { int16_t len; memcpy(&len, r + *offset, 2); /* The `len - 1` expression is a combination of two steps: We subtract 2 bytes because we don't need memory for the 2-byte length overhead. Then we add 1 byte to account for the null terminator we need to append on the end of the string. */ char* pChar = malloc(len - 1); memcpy(pChar, r + *offset + 2, len - 2); pChar[len - 2] = '\\0'; *offset += len; return charGetDatum(pChar); } Create a new function to extract the varchar data.","title":"Defill"},{"location":"08-data-types-varchar/05-defill-and-display/#display","text":"src/resultset/resultset_print.c static void compute_column_widths(RecordDescriptor* rd, RecordSet* rs, int* widths) { int maxLen; for (int i = 0; i < rd->ncols; i++) { maxLen = strlen(rd->cols[i].colname); Column* col = &rd->cols[i]; ListItem* row = rs->rows->head; while (row != NULL) { int len; RecordSetRow* data = (RecordSetRow*)row->ptr; switch (col->dataType) { case DT_BOOL: case DT_TINYINT: len = num_digits(datumGetUInt8(data->values[i])); break; case DT_SMALLINT: len = num_digits(datumGetInt16(data->values[i])); break; case DT_INT: len = num_digits(datumGetInt32(data->values[i])); break; case DT_BIGINT: len = num_digits(datumGetInt64(data->values[i])); break; case DT_CHAR: + case DT_VARCHAR: len = strlen(datumGetString(data->values[i])); break; default: printf(\"compute_column_widths() | Unknown data type\\n\"); } if (len > maxLen) maxLen = len; row = row->next; } widths[i] = maxLen + 1; } } src/resultset/resultset_print.c void resultset_print(RecordDescriptor* rd, RecordSet* rs, RecordDescriptor* targets) { printf(\"--------\\n\"); printf(\"*** Rows: %d\\n\", rs->rows->numItems); printf(\"--------\\n\"); if (rs->rows->numItems == 0) return; int* widths = malloc(sizeof(int) * rd->ncols); compute_column_widths(rd, rs, widths); print_column_headers(rd, targets, widths); ListItem* row = rs->rows->head; while (row != NULL) { printf(\"|\"); Datum* values = (Datum*)(((RecordSetRow*)row->ptr)->values); for (int i = 0; i < targets->ncols; i++) { int colIndex = get_col_index(rd, targets->cols[i].colname); Column* col = &rd->cols[colIndex]; switch (col->dataType) { case DT_BOOL: case DT_TINYINT: case DT_SMALLINT: case DT_INT: case DT_BIGINT: print_cell_num(col->dataType, values[colIndex], widths[colIndex]); break; case DT_CHAR: + case DT_VARCHAR: print_cell_with_padding(datumGetString(values[colIndex]), widths[colIndex], false); break; default: printf(\"resultset_print() | Unknown data type\\n\"); } } printf(\"\\n\"); row = row->next; } printf(\"(Rows: %d)\\n\\n\", rs->rows->numItems); free(widths); } No new logic necessary, we just need to add DT_VARCHAR to the switch/case blocks and piggy back off of the DT_CHAR logic. src/resultset/recordset.c static void free_recordset_row_columns(RecordSetRow* row, RecordDescriptor* rd) { for (int i = 0; i < rd->ncols; i++) { - if (rd->cols[i].dataType == DT_CHAR) { + if (rd->cols[i].dataType == DT_CHAR || rd->cols[i].dataType == DT_VARCHAR) { free((void*)row->values[i]); } } } Lastly, we just need to make sure we free the memory associated with DT_VARCHAR values.","title":"Display"},{"location":"08-data-types-varchar/05-defill-and-display/#running-select-statements","text":"Now we can test our changes by running select statements: $ ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > select person_id, first_name, last_name, age; ====== Node ====== = Type: Select = Targets: = person_id = first_name = last_name = age -------- *** Rows: 2 -------- |person_id |first_name |last_name |age | ---------------------------------------------------------- | 1|Chris |Burke | 33| | 2|I am longer than twe |was it truncated? | 69| (Rows: 2) bql > Success! Our resultset display is functioning properly. In the next section, we're going to introduce Null columns to our database. Don't worry though, the columns in our table will stay the same so the temporary code refactor will be minimal.","title":"Running Select Statements"},{"location":"09-data-constraint-null/01-intro/","text":"Null Constraints Note: in this section I we talk about Null data in the database. This is not to be consued with the NULL C-type. So in these next few sections, and probably for the entirety of this writeup, I will use proper-case Null to refer to the database null, and all-caps NULL to refer to the C null pointer. Null data is an extremely important feature of relational databases. Sometimes an entity doesn't have any data to store for a given field. For example, you might have a table that stores data for sporting events. And in that table there would be columns for start_date and end_date that store the exact times the event started and ended. While the event is in progress, the end_date field would necessarily be Null . Since Null literally means \"nothing\", we will need a mechanism to store \"nothing\" in a way that doesn't interfere with the storage engine's ability to read data from a record. In our current implementation, we take advantage of strictly-defined data lengths while stepping through a record and reading data. If any given field were missing, it would throw off all subsequent reads in that record. So we need the ability to signify to the storage engine that a certain field exists or not. Enter: the null bitmap. Null Bitmap The Null bitmap is exactly that, a bitmap indicating whether a given column is Null or not. It lives in the boundary between the fixed-length and variable-length columns, BUT it is only present when the table has nullable columns. If a table is defined such that all columns are Not Null , then the storage engine does not waste space on a Null bitmap because it would be redundant. Moreover, the order of bits in the null bitmap correspond to the physical order of columns stored in the record. Meaning, all fixed-length columns are represented first, followed by all variable-length columns. Now let's consider the table we're going to work with and look at the physical structure of a record in that table. Our table is defined as: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); We have two fixed-length columns and two variable-length columns; one column of each is nullable. The physical structure will look like: We have the ever-present 12-byte record header to start us off, then our fixed-length columns person_id and age . Followed by our 1-byte null bitmap, then our variable-length columns first_name and last_name .","title":"Intro"},{"location":"09-data-constraint-null/01-intro/#null-constraints","text":"Note: in this section I we talk about Null data in the database. This is not to be consued with the NULL C-type. So in these next few sections, and probably for the entirety of this writeup, I will use proper-case Null to refer to the database null, and all-caps NULL to refer to the C null pointer. Null data is an extremely important feature of relational databases. Sometimes an entity doesn't have any data to store for a given field. For example, you might have a table that stores data for sporting events. And in that table there would be columns for start_date and end_date that store the exact times the event started and ended. While the event is in progress, the end_date field would necessarily be Null . Since Null literally means \"nothing\", we will need a mechanism to store \"nothing\" in a way that doesn't interfere with the storage engine's ability to read data from a record. In our current implementation, we take advantage of strictly-defined data lengths while stepping through a record and reading data. If any given field were missing, it would throw off all subsequent reads in that record. So we need the ability to signify to the storage engine that a certain field exists or not. Enter: the null bitmap.","title":"Null Constraints"},{"location":"09-data-constraint-null/01-intro/#null-bitmap","text":"The Null bitmap is exactly that, a bitmap indicating whether a given column is Null or not. It lives in the boundary between the fixed-length and variable-length columns, BUT it is only present when the table has nullable columns. If a table is defined such that all columns are Not Null , then the storage engine does not waste space on a Null bitmap because it would be redundant. Moreover, the order of bits in the null bitmap correspond to the physical order of columns stored in the record. Meaning, all fixed-length columns are represented first, followed by all variable-length columns. Now let's consider the table we're going to work with and look at the physical structure of a record in that table. Our table is defined as: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); We have two fixed-length columns and two variable-length columns; one column of each is nullable. The physical structure will look like: We have the ever-present 12-byte record header to start us off, then our fixed-length columns person_id and age . Followed by our 1-byte null bitmap, then our variable-length columns first_name and last_name .","title":"Null Bitmap"},{"location":"09-data-constraint-null/02-parser-refactor/","text":"Parser Refactor This is the last time we'll have to refactor code that we know is going to be thrown out in the future. Hurray! But I must begin with an apology. The amount of refactoring we need to do is quite significant, and it's made worse knowing we're going to toss it all out when we implement system catalogs soon. But this is the last time we'll have to write a bunch of throwaway code, so that's something! You're probably wondering why the refactor will be so large since our table is pretty much the same. Well, in order to support Null 's, we need to restructure our parser's insert statement grammar, which means we need to redo the InsertStmt struct, which means all of our temporary code that works with that struct needs to change as well. It's a real house of cards/trail of dominos situation. Let's just dive right in, starting with the parser. Scanner As a reminder, our table DDL looks like this: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); We have two columns that can store Null values. And in order to consume Null 's from the client, we need to update our scanner and grammar to be able to parse them. From the client's perspective, we want to consume Null 's like this: bql > insert 1 Null 'Burke' Null; The insert command still expects four inputs, but in order to indicate you want a Null in a given field, you just pass in the literal keyword: \"Null\". Fortunately, the scanner update is quite simple: src/parser/scan.l INSERT { return INSERT; } +NULL { return KW_NULL; } SELECT { return SELECT; } We just add a new \"NULL\" keyword. Grammar The grammar, on the other hand, is a lot more complex by comparison. src/parser/gram.y %token INSERT +%token KW_NULL %token SELECT %token KW_TRUE -%type <node> cmd stmt sys_cmd select_stmt insert_stmt target +%type <node> cmd stmt sys_cmd select_stmt insert_stmt target literal -%type <list> target_list +%type <list> target_list literal_values_list %start query We have to add a new token for KW_NULL to match what the new scanner token. Then we add two new rules, a <node> type called literal , and a <list> type called literal_values_list . As you can probably guess, the latter is just a ParseList of the former. More on these below. src/parser/gram.y -insert_stmt: INSERT NUMBER STRING STRING NUMBER { +insert_stmt: INSERT literal_values_list { InsertStmt* ins = create_node(InsertStmt); - ins->personId = $2; - ins->firstName = str_strip_quotes($3); - ins->lastName = str_strip_quotes($4); - ins->age = $5; + ins->values = $2; $$ = (Node*)ins; } ; +literal_values_list: literal { + $$ = create_parselist($1); + } + | literal_values_list literal { + $$ = parselist_append($1, $2); + } + ; The big change in the insert grammar is instead of explicitly defined input rules for each of the four inputs, we now consume a list. And that list pattern is almost exactly the same pattern we use for the target_list in the select statement grammar. Notice that instead of individual fields hard-coded in the InsertStmt node, we have a new field called values . This is the big change that necessitates a bunch of downstream refactoring in our temporary code. But more on that later. What the heck is a literal ? src/parser/gram.y -bool: KW_FALSE { - $$ = 0; - } - | KW_TRUE { - $$ = 1; - } - ; +literal: NUMBER { + Literal* l = create_node(Literal); + l->str = NULL; + l->intVal = $1; + l->isNull = false; + + $$ = (Node*)l; + } + | STRING { + Literal* l = create_node(Literal); + l->str = str_strip_quotes($1); + l->isNull = false; + + $$ = (Node*)l; + } + | KW_NULL { + Literal* l = create_node(Literal); + l->str = NULL; + l->isNull = true; + + $$ = (Node*)l; + } + | KW_FALSE { + Literal* l = create_node(Literal); + l->str = NULL; + l->boolVal = false; + l->isNull = false; + + $$ = (Node*)l; + } + | KW_TRUE { + Literal* l = create_node(Literal); + l->str = NULL; + l->boolVal = true; + l->isNull = false; + + $$ = (Node*)l; + } + ; First, we toss out the bool grammar rule because we don't need it anymore. Then we define our literal . Glancing at the five different ways a literal is defined, you can see they just correspond to the different data types. Starting from the top, our first literal corresponds to the NUMBER token and is just an int . You'll notice we're creating a new node type, called Literal , which we'll need to define. But suffice to say, the Literal struct has a field for each type of data we support. NUMBER 's get assigned to the intVal field, STRING 's are assigned to the str field, and so on. Two noteworthy things to mention. (1) the isNull field is set in all cases. And (2) we set str to NULL for all non- STRING data types. The first, isNull , is super important for downstream processing. Our fill_val function needs to know if the incoming data is null or not so it can either write data to the record or not. The second, str , is important simply because it is a pointer. All unused pointers must be explicitly set to NULL otherwise any access to them, even to attempt freeing the memory, is undefined behavior. So we MUST set it to NULL if we're not using it. Parsetree Refactor Now that our scanner and grammar refactor is complete, let's update our parsetree structs and code. src/include/parser/parsetree.h #ifndef PARSETREE_H #define PARSETREE_H #include <stdint.h> +#include <stdbool.h> typedef enum NodeTag { T_SysCmd, T_InsertStmt, T_SelectStmt, T_ParseList, T_ResTarget, + T_Literal } NodeTag; We need to include a new standard library to support using the bool primitive in our new Literal struct below. And we also add the T_Literal node tag to the enum. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; - int32_t personId; - char* firstName; - char* lastName; - int32_t age; + ParseList* values; } InsertStmt; typedef struct ResTarget { NodeTag type; char* name; } ResTarget; typedef struct SelectStmt { NodeTag type; ParseList* targetList; } SelectStmt; +typedef struct Literal { + NodeTag type; + bool isNull; + int64_t intVal; + char* str; + bool boolVal; +} Literal; Next we delete the hard-coded fields in the InsertStmt struct and replace them with a more flexible ParseList* field. This is how the grammar can stash its literal_values_list grammar rule into the InsertStmt node. Next, we define the Literal struct. It has a field for each of the data type we support, bool , int 's, and char* , as well as a meta isNull field. The int is an 8-byte integer because it's the largest number type our database supports and upsize conversions are lossless. If the client tries to input a number that would overflow the data type.. well that's a problem for the analyzer - which we will not implement yet. Anyways, moving on to the parsetree code. src/parser/parsetree.c static void free_insert_stmt(InsertStmt* ins) { if (ins == NULL) return; - free(ins->firstName); - free(ins->lastName); + free_parselist(ins->values); + free(ins->values); } +static void free_literal(Literal* l) { + if (l->str != NULL) { + free(l->str); + } +} void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; case T_InsertStmt: free_insert_stmt((InsertStmt*)n); break; case T_SelectStmt: free_selectstmt((SelectStmt*)n); break; case T_ParseList: free_parselist((ParseList*)n); break; case T_ResTarget: free_restarget((ResTarget*)n); break; + case T_Literal: + free_literal((Literal*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } First, we update code for freeing the memory consumed by our refactored InsertStmt node and the new Literal node. Pretty straightforward. src/parser/parsetree.c #include \"parser/parsetree.h\" +#include \"storage/record.h\" *** omitted for brevity *** +// Probably a temporary function +static void print_insertstmt_literal(Literal* l, char* colname, DataType dt) { + int padLen = 20 - strlen(colname); + + if (l->isNull) { + printf(\"= %s%*sNULL\\n\", colname, padLen, \" \"); + } else { + switch (dt) { + case DT_VARCHAR: + printf(\"= %s%*s%s\\n\", colname, padLen, \" \", l->str); + break; + case DT_INT: + printf(\"= %s%*s%d\\n\", colname, padLen, \" \", (int32_t)l->intVal); + break; + } + } +} void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: { + InsertStmt* i = (InsertStmt*)n; printf(\"= Type: Insert\\n\"); - printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); - printf(\"= first_name: %s\\n\", ((InsertStmt*)n)->firstName); - printf(\"= last_name: %s\\n\", ((InsertStmt*)n)->lastName); - printf(\"= age: %d\\n\", ((InsertStmt*)n)->age); + print_insertstmt_literal((Literal*)i->values->elements[0].ptr, \"person_id\", DT_INT); + print_insertstmt_literal((Literal*)i->values->elements[1].ptr, \"first_name\", DT_VARCHAR); + print_insertstmt_literal((Literal*)i->values->elements[2].ptr, \"last_name\", DT_VARCHAR); + print_insertstmt_literal((Literal*)i->values->elements[3].ptr, \"age\", DT_INT); break; } case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } Next, we restructure how our print_node function works. Instead of explicitly printing the hard-coded fields of the InsertStmt node, we create a helper function (probably temporary, ugh!), that prints what we want. The reason we're using a temporary function instead of drilling into the Literal struct in a printf call is because we need to check if the value is Null first. If the value is Null , then we just want to print the string \"NULL\", otherwise we print its value. Since our table only has Varchar and Int types, I'm only including DT_VARCHAR and DT_INT in the switch/case block of the helper function. If we had more than just those columns, we would need to voer those types. That concludes our parser refactor. In the next section we're going to refactor how our storage engine writes data to a record. And it gets interesting because we're going to delve into the wonderful world of bitfield manipulation.","title":"Parser Refactor"},{"location":"09-data-constraint-null/02-parser-refactor/#parser-refactor","text":"This is the last time we'll have to refactor code that we know is going to be thrown out in the future. Hurray! But I must begin with an apology. The amount of refactoring we need to do is quite significant, and it's made worse knowing we're going to toss it all out when we implement system catalogs soon. But this is the last time we'll have to write a bunch of throwaway code, so that's something! You're probably wondering why the refactor will be so large since our table is pretty much the same. Well, in order to support Null 's, we need to restructure our parser's insert statement grammar, which means we need to redo the InsertStmt struct, which means all of our temporary code that works with that struct needs to change as well. It's a real house of cards/trail of dominos situation. Let's just dive right in, starting with the parser.","title":"Parser Refactor"},{"location":"09-data-constraint-null/02-parser-refactor/#scanner","text":"As a reminder, our table DDL looks like this: Create Table person ( person_id Int Not Null, first_name Varchar(20) Null, last_name Varchar(20) Not Null, age Int Null ); We have two columns that can store Null values. And in order to consume Null 's from the client, we need to update our scanner and grammar to be able to parse them. From the client's perspective, we want to consume Null 's like this: bql > insert 1 Null 'Burke' Null; The insert command still expects four inputs, but in order to indicate you want a Null in a given field, you just pass in the literal keyword: \"Null\". Fortunately, the scanner update is quite simple: src/parser/scan.l INSERT { return INSERT; } +NULL { return KW_NULL; } SELECT { return SELECT; } We just add a new \"NULL\" keyword.","title":"Scanner"},{"location":"09-data-constraint-null/02-parser-refactor/#grammar","text":"The grammar, on the other hand, is a lot more complex by comparison. src/parser/gram.y %token INSERT +%token KW_NULL %token SELECT %token KW_TRUE -%type <node> cmd stmt sys_cmd select_stmt insert_stmt target +%type <node> cmd stmt sys_cmd select_stmt insert_stmt target literal -%type <list> target_list +%type <list> target_list literal_values_list %start query We have to add a new token for KW_NULL to match what the new scanner token. Then we add two new rules, a <node> type called literal , and a <list> type called literal_values_list . As you can probably guess, the latter is just a ParseList of the former. More on these below. src/parser/gram.y -insert_stmt: INSERT NUMBER STRING STRING NUMBER { +insert_stmt: INSERT literal_values_list { InsertStmt* ins = create_node(InsertStmt); - ins->personId = $2; - ins->firstName = str_strip_quotes($3); - ins->lastName = str_strip_quotes($4); - ins->age = $5; + ins->values = $2; $$ = (Node*)ins; } ; +literal_values_list: literal { + $$ = create_parselist($1); + } + | literal_values_list literal { + $$ = parselist_append($1, $2); + } + ; The big change in the insert grammar is instead of explicitly defined input rules for each of the four inputs, we now consume a list. And that list pattern is almost exactly the same pattern we use for the target_list in the select statement grammar. Notice that instead of individual fields hard-coded in the InsertStmt node, we have a new field called values . This is the big change that necessitates a bunch of downstream refactoring in our temporary code. But more on that later. What the heck is a literal ? src/parser/gram.y -bool: KW_FALSE { - $$ = 0; - } - | KW_TRUE { - $$ = 1; - } - ; +literal: NUMBER { + Literal* l = create_node(Literal); + l->str = NULL; + l->intVal = $1; + l->isNull = false; + + $$ = (Node*)l; + } + | STRING { + Literal* l = create_node(Literal); + l->str = str_strip_quotes($1); + l->isNull = false; + + $$ = (Node*)l; + } + | KW_NULL { + Literal* l = create_node(Literal); + l->str = NULL; + l->isNull = true; + + $$ = (Node*)l; + } + | KW_FALSE { + Literal* l = create_node(Literal); + l->str = NULL; + l->boolVal = false; + l->isNull = false; + + $$ = (Node*)l; + } + | KW_TRUE { + Literal* l = create_node(Literal); + l->str = NULL; + l->boolVal = true; + l->isNull = false; + + $$ = (Node*)l; + } + ; First, we toss out the bool grammar rule because we don't need it anymore. Then we define our literal . Glancing at the five different ways a literal is defined, you can see they just correspond to the different data types. Starting from the top, our first literal corresponds to the NUMBER token and is just an int . You'll notice we're creating a new node type, called Literal , which we'll need to define. But suffice to say, the Literal struct has a field for each type of data we support. NUMBER 's get assigned to the intVal field, STRING 's are assigned to the str field, and so on. Two noteworthy things to mention. (1) the isNull field is set in all cases. And (2) we set str to NULL for all non- STRING data types. The first, isNull , is super important for downstream processing. Our fill_val function needs to know if the incoming data is null or not so it can either write data to the record or not. The second, str , is important simply because it is a pointer. All unused pointers must be explicitly set to NULL otherwise any access to them, even to attempt freeing the memory, is undefined behavior. So we MUST set it to NULL if we're not using it.","title":"Grammar"},{"location":"09-data-constraint-null/02-parser-refactor/#parsetree-refactor","text":"Now that our scanner and grammar refactor is complete, let's update our parsetree structs and code. src/include/parser/parsetree.h #ifndef PARSETREE_H #define PARSETREE_H #include <stdint.h> +#include <stdbool.h> typedef enum NodeTag { T_SysCmd, T_InsertStmt, T_SelectStmt, T_ParseList, T_ResTarget, + T_Literal } NodeTag; We need to include a new standard library to support using the bool primitive in our new Literal struct below. And we also add the T_Literal node tag to the enum. src/include/parser/parsetree.h typedef struct InsertStmt { NodeTag type; - int32_t personId; - char* firstName; - char* lastName; - int32_t age; + ParseList* values; } InsertStmt; typedef struct ResTarget { NodeTag type; char* name; } ResTarget; typedef struct SelectStmt { NodeTag type; ParseList* targetList; } SelectStmt; +typedef struct Literal { + NodeTag type; + bool isNull; + int64_t intVal; + char* str; + bool boolVal; +} Literal; Next we delete the hard-coded fields in the InsertStmt struct and replace them with a more flexible ParseList* field. This is how the grammar can stash its literal_values_list grammar rule into the InsertStmt node. Next, we define the Literal struct. It has a field for each of the data type we support, bool , int 's, and char* , as well as a meta isNull field. The int is an 8-byte integer because it's the largest number type our database supports and upsize conversions are lossless. If the client tries to input a number that would overflow the data type.. well that's a problem for the analyzer - which we will not implement yet. Anyways, moving on to the parsetree code. src/parser/parsetree.c static void free_insert_stmt(InsertStmt* ins) { if (ins == NULL) return; - free(ins->firstName); - free(ins->lastName); + free_parselist(ins->values); + free(ins->values); } +static void free_literal(Literal* l) { + if (l->str != NULL) { + free(l->str); + } +} void free_node(Node* n) { if (n == NULL) return; switch (n->type) { case T_SysCmd: free_syscmd((SysCmd*)n); break; case T_InsertStmt: free_insert_stmt((InsertStmt*)n); break; case T_SelectStmt: free_selectstmt((SelectStmt*)n); break; case T_ParseList: free_parselist((ParseList*)n); break; case T_ResTarget: free_restarget((ResTarget*)n); break; + case T_Literal: + free_literal((Literal*)n); + break; default: printf(\"Unknown node type\\n\"); } free(n); } First, we update code for freeing the memory consumed by our refactored InsertStmt node and the new Literal node. Pretty straightforward. src/parser/parsetree.c #include \"parser/parsetree.h\" +#include \"storage/record.h\" *** omitted for brevity *** +// Probably a temporary function +static void print_insertstmt_literal(Literal* l, char* colname, DataType dt) { + int padLen = 20 - strlen(colname); + + if (l->isNull) { + printf(\"= %s%*sNULL\\n\", colname, padLen, \" \"); + } else { + switch (dt) { + case DT_VARCHAR: + printf(\"= %s%*s%s\\n\", colname, padLen, \" \", l->str); + break; + case DT_INT: + printf(\"= %s%*s%d\\n\", colname, padLen, \" \", (int32_t)l->intVal); + break; + } + } +} void print_node(Node* n) { if (n == NULL) { printf(\"print_node() | Node is NULL\\n\"); return; } printf(\"====== Node ======\\n\"); switch (n->type) { case T_SysCmd: printf(\"= Type: SysCmd\\n\"); printf(\"= Cmd: %s\\n\", ((SysCmd*)n)->cmd); break; case T_InsertStmt: { + InsertStmt* i = (InsertStmt*)n; printf(\"= Type: Insert\\n\"); - printf(\"= person_id: %d\\n\", ((InsertStmt*)n)->personId); - printf(\"= first_name: %s\\n\", ((InsertStmt*)n)->firstName); - printf(\"= last_name: %s\\n\", ((InsertStmt*)n)->lastName); - printf(\"= age: %d\\n\", ((InsertStmt*)n)->age); + print_insertstmt_literal((Literal*)i->values->elements[0].ptr, \"person_id\", DT_INT); + print_insertstmt_literal((Literal*)i->values->elements[1].ptr, \"first_name\", DT_VARCHAR); + print_insertstmt_literal((Literal*)i->values->elements[2].ptr, \"last_name\", DT_VARCHAR); + print_insertstmt_literal((Literal*)i->values->elements[3].ptr, \"age\", DT_INT); break; } case T_SelectStmt: print_selectstmt((SelectStmt*)n); break; default: printf(\"print_node() | unknown node type\\n\"); } } Next, we restructure how our print_node function works. Instead of explicitly printing the hard-coded fields of the InsertStmt node, we create a helper function (probably temporary, ugh!), that prints what we want. The reason we're using a temporary function instead of drilling into the Literal struct in a printf call is because we need to check if the value is Null first. If the value is Null , then we just want to print the string \"NULL\", otherwise we print its value. Since our table only has Varchar and Int types, I'm only including DT_VARCHAR and DT_INT in the switch/case block of the helper function. If we had more than just those columns, we would need to voer those types. That concludes our parser refactor. In the next section we're going to refactor how our storage engine writes data to a record. And it gets interesting because we're going to delve into the wonderful world of bitfield manipulation.","title":"Parsetree Refactor"},{"location":"09-data-constraint-null/03-writing-data/","text":"Writing Data Now that our parser can handle the new way we parse insert statements, we can write the code that actually handles Null data and how it gets persisted to disk. main.c File Like we've done before, we're going to pretend we're an insert statement and walk through the code path we'd follow, updating things accordingly. First up is our main function: src/main.c break; case T_InsertStmt: { - int32_t person_id = ((InsertStmt*)n)->personId; - char* firstName = ((InsertStmt*)n)->firstName; - char* lastName = ((InsertStmt*)n)->lastName; - int32_t age = ((InsertStmt*)n)->age; + InsertStmt* i = (InsertStmt*)n; - if (!insert_record(bp, person_id, firstName, lastName, age)) { + if (!insert_record(bp, i->values)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: if (!analyze_node(n)) { First, we get rid of references to the hard-coded fields in the old InsertStmt struct. Then in our call to insert_record , we just pass in the values . Which means we need to go refactor the insert_record function: src/main.c -static bool insert_record(BufPool* bp, int32_t person_id, char* firstName, char* lastName, int32_t age) { +static bool insert_record(BufPool* bp, ParseList* values) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); - int recordLength = compute_record_length(rd, person_id, firstName, lastName, age); + int recordLength = compute_record_length(rd, values); Record r = record_init(recordLength); - serialize_data(rd, r, person_id, firstName, lastName, age); + serialize_data(rd, r, values); bool insertSuccessful = page_insert(slot->pg, r, recordLength); free_record_desc(rd); free(r); return insertSuccessful; } The logic is exactly the same, but we need to replace the hard-coded input parameters for a single ParseList* values param. Then we replace any function calls that consumed those hard-coded parameters with a call that only passes in the values list. Now we need to go refactor compute_record_length and serialize_data . src/main.c static int compute_record_length(RecordDescriptor* rd, ParseList* values) { int len = 12; // start with the 12-byte header len += compute_null_bitmap_length(rd); len += 4; // person_id Literal* firstName = (Literal*)values->elements[1].ptr; Literal* lastName = (Literal*)values->elements[2].ptr; Literal* age = (Literal*)values->elements[3].ptr; /* for the varlen columns, we default to their max length if the values we're trying to insert would overflow them (they'll get truncated later) */ if (firstName->isNull) { len += 0; // Nulls do not consume any space } else if (strlen(firstName->str) > rd->cols[1].len) { len += (rd->cols[1].len + 2); } else { len += (strlen(firstName->str) + 2); } // We don't need a null check because this column is constrained to be Not Null if (strlen(lastName->str) > rd->cols[2].len) { len += (rd->cols[2].len + 2); } else { len += (strlen(lastName->str) + 2); } if (age->isNull) { len += 0; // Nulls do not consume any space } else { len += 4; // age } return len; } So much of this function changed, that I'm just going to call it a full rewrite. This is definitely a temporary function because it only works due to the assumptions we use. Starting at the top, we have 12 bytes for the record header - that's fine, no way around it. Next, we add the length of the null bitmap. And for that we create a new function to calculate its size - covered below. Next, we add 4 bytes for the person_id column. Since we know it cannot be Null (assumption), we know it ALWAYS consumes 4 bytes. Next, we parse out the Literal 's of the remaining three columns. For firstName , we check if the value passed in by the user is Null or not. If so, we don't add anything to the record's length because Null 's are absent in the physical record. If it's Not Null , then we perform the same logic as before: if the value's length exceeds the column's max length, we add max length + 2 to the record's length. For the lastName field, we can skip the null check because we know it cannot be Null (assumption), and perform the same logic we do for the firstName field. Lastly, we have a null check on the age column. If Null , the length is 0. If Not Null , the length is 4. How do we determine the size of the null bitmap? Simple, each column corresponds to a single bit in the null bitmap. For table with 1 to 8 columns, the null bitmap consumes 1 byte of space. If the table has 9 to 16 columns, the null bitmap consumes 2 bytes. And so on. Record Data src/include/storage/record.h +int compute_null_bitmap_length(RecordDescriptor* rd); src/storage/record.c +/** + * returns the number of bytes consumed by the null bitmap + * + * Every 8 columns requires an additional byte for the null bitmap +*/ +int compute_null_bitmap_length(RecordDescriptor* rd) { + if (!rd->hasNullableColumns) return 0; + + return (rd->ncols) / 8 + 1; +} int compute_record_fixed_length(RecordDescriptor* rd, bool* fixedNull) { This is actually going to be a permanent function, so we get to write it in the record.c file. Basically, the null bitmap consumes 1 byte for every 8 columns in the table. Okay, back to the insert_record function. The next refactoring target on our path is the serialize_data function: src/main.c -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* firstName, char* lastName, int32_t age) { static void serialize_data(RecordDescriptor* rd, Record r, ParseList* values) { Datum* fixed = malloc(rd->nfixed * sizeof(Datum)); Datum* varlen = malloc((rd->ncols - rd->nfixed) * sizeof(Datum)); + bool* fixedNull = malloc(rd->nfixed * sizeof(bool)); + bool* varlenNull = malloc((rd->ncols - rd->nfixed) * sizeof(bool)); - populate_datum_array(fixed, varlen, person_id, firstName, lastName, age); populate_datum_array(fixed, varlen, fixedNull, varlenNull, values); + int nullOffset = sizeof(RecordHeader) + compute_record_fixed_length(rd, fixedNull); + ((RecordHeader*)r)->nullOffset = nullOffset; + uint8_t* nullBitmap = r + nullOffset; - fill_record(rd, r + sizeof(RecordHeader), fixed, varlen); + fill_record(rd, r + sizeof(RecordHeader), fixed, varlen, fixedNull, varlenNull, nullBitmap); free(fixed); free(varlen); + free(fixedNull); + free(varlenNull); } First, we do the expected replacement of the hard-coded column parameters for the ParseList* values param. Then we need to create a pair of bool arrays to match the Datum arrays for the fixed and variable-length fields. The bool arrays are used to indicate whether or not the corresponding value is Null or not. Next, we make a call to the refactored populate_datum_array function. The refactored version (covered below) will populate values for our two Datum arrays and our two bool arrays based on the user-supplied values data. Following that, we have our very first usage of a RecordHeader field. We need to set the nullOffset field to represent the position of the null bitmap in the Record data. Remember, the null bitmap lives between the fixed-length columns and the variable-length columns. So its location can be calculated by (record header size) + (fixed-length size) . And in order to compute the fixed-length size, we need a new (permanent) helper function. Next, we have a refactored version of the fill_record function. We need to pass it the two new bool arrays, and a pointer to the null bitmap. Finally, we need to free the memory consumed by the two new bool arrays. As for the refactored populate_datum_array function, it is now going to populate the two null arrays alongside the values arrays. And the function is changing so much, we're just going to scrap it and write a new one from scratch. src/main.c static void populate_datum_array(Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, ParseList* values) { Literal* personId = (Literal*)values->elements[0].ptr; Literal* firstName = (Literal*)values->elements[1].ptr; Literal* lastName = (Literal*)values->elements[2].ptr; Literal* age = (Literal*)values->elements[3].ptr; fixed[0] = int32GetDatum(personId->intVal); fixedNull[0] = false; if (age->isNull) { fixed[1] = (Datum)NULL; fixedNull[1] = true; } else { fixed[1] = int32GetDatum(age->intVal); fixedNull[1] = false; } if (firstName->isNull) { varlen[0] = (Datum)NULL; varlenNull[0] = true; } else { varlen[0] = charGetDatum(firstName->str); varlenNull[0] = false; } varlen[1] = charGetDatum(lastName->str); varlenNull[1] = false; } First we parse out the Literal 's for each of our four hard-coded columns. Then we start filling in the value and null arrays. Since person_id is Not Null , we don't need a null check and can assign the values directly. age , on the other hand, can be Null , so we perform a null check before populating the arrays. Notice for the null case we cast NULL to a Datum . Why? Because our array expects a Datum type and the compiler will yell at us otherwise. We follow similar logic for the variable-length columns. first_name is Null able, so we need a null check, whereas last_name is not. Jumping back up to the serialize_data function, the next change we come across is fill_record . We have three additional parameters to pass in to the new version of the function: both null arrays and a pointer to the null bitmap. Let's update the header file first, then get to the code. src/include/storage.record.h -void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen); +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, uint8_t* bitmap); src/storage/record.c -void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen) { +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, uint8_t* nullBitmap) { + uint8_t* bitP = &nullBitmap[-1]; + int bitmask = 0x80; // fill fixed-length columns for (int i = 0; i < rd->nfixed; i++) { Column* col = get_nth_col(rd, true, i); - fill_val(col, &r, fixed[i]); + fill_val( + col, + &bitP, + &bitmask, + &r, + fixed[i], + fixedNull[i] + ); } + // jump past the null bitmap + r += compute_null_bitmap_length(rd); // fill varlen columns for (int i = 0; i < (rd->ncols - rd->nfixed); i++) { Column* col = get_nth_col(rd, false, i); - fill_val(col, &r, varlen[i]); + fill_val( + col, + &bitP, + &bitmask, + &r, + varlen[i], + varlenNull[i] + ); } } Since we're introducing the null bitmap, we'll need to start writing some bitwise operations. That's where these first two lines of the function come in. The actual bitwise operations will happen in fill_val , but we need to pass in a pointer to the null bitmap that it can operate on, as well as a bitmask to use for populating bits in the null bitmap. Why do we assign bitP to the -1 index? Is that legal? Although it looks like an invalid address, it technically is because the null bitmap is inside the Record chunk of memory. The -1 index just means it's pointing to the byte immediately preceeding the null bitmap. We'll go over the \"why\" below when we walk through an example. src/storage/record.c -static void fill_val(Column* col, char** dataP, Datum datum) { +static void fill_val(Column* col, uint8_t** bit, int* bitmask, char** dataP, Datum datum, bool isNull) { int16_t dataLen; char* data = *dataP; + if (*bitmask != 0x80) { + *bitmask <<= 1; + } else { + *bit += 1; + **bit = 0x0; + *bitmask = 1; + } + + // column is null, nothing more to do + if (isNull) return; + + **bit |= *bitmask; switch (col->dataType) { case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: *** omitted for brevity *** } Working with the actual data remains the same as before. The only difference is we added some logic to populate the bitfields in the null bitmap and short-circuit the function if the value to insert is Null . So how and why does this work? Step-by-Step Example Let's walk through an example line by line. Pretend we're the computer and we're trying to insert the following record: person_id first_name last_name age 1 Null Burke Null Ignoring the row header, how many bytes do we need to store this record? In physical order of the data: 4-byte person_id 0-byte age (Null) 1-byte null bitmap 0-byte first_name (Null) 7-byte last_name (2-byte overhead plus 5-bytes for data) Total : 12 bytes We start with a blank chunk of memory that represents the Record we're going to fill. Again, I'm ignoring the header portion in this example. Here's what we're working with: person_id | bitmap | last_name 0000 0000 | 00000000 | 0000 0000 0000 00 IMPORTANT : I'm representing person_id and last_name in hex like the xxd command shows, but I'm showing all 8 individual bits for the null bitmap portion. fill_record Starting at the top, we initialize two variables: uint8_t* bitP: points to the last byte of the person_id data int bitmask: 10000000 (binary) only showing the first byte Note: although bitmask is a 4-byte int, I'm only going to show the first byte throughout this example because that's the only part of it that's relevant. Next, we go into the loop where we fill_val all of the fixed-length columns. Our table has two fixed-length columns, so we start with person_id and pass in the appropriate parameters. Relevant to this example, we supply &bitP - a pointer to the pointer that currently points to the byte preceeding the null bitmap. And &bitmask - a pointer to the bitmask variable. fill_val - person_id Since we already know how this function writes data to our Record , I'm just going to focus on the new null bitmap operations. At the top, we're immediately confronted with this if/else statement: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } If bitmask is not equal to 0x80 ( 10000000 ), then we left shift bitmask by one. If you remember in fill_record , we explicitly initialized bitmask to 0x80 , so we can proceed to the else block. *bit += 1; Here, we advance the bitmap pointer by one. Remember how we started out by pointing to the byte preceeding the null bitmap? This is where we course-correct by pointing to the actual beginning of the null bitmap. **bit = 0x0; Next, we set the entire byte of the null bitmap to 0s. It doesn't matter how large the null bitmap is, this instruction will only set a single byte to 0s. *bitmask = 1; Reset the bitmask to 1 ( 00000001 ). Here's the current state of our variables: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0000 0000 | 00000000 | 0000 0000 0000 00 Why the convoluted mess with the if/else block and bitwise operations? Its purpose is to reset the bitmask back to 1 each time we hit a byte boundary. That situation won't come into play during this example because we only have four columns to worry about. Since each byte in the null bitmap can cover 8 columns, that logic is only important for tables with more than 8 columns. Now, why do we need to reset the bitmask at byte boundaries? Because the bitmap pointer ( bitP ) always points to the byte we're currently working with. We have a bitwise OR operation coming up between the bitmap byte and the bitmask. In order for this to work properly, we need to make sure we're only using the first byte of the bitmask variable. Final question: why do we need to advance the bitP pointer? Can't we just keep it pointing to the first byte in the null bitmap so that left-shifting the bitmask after 8 columns will still work? Short answer: no. What if we have a table with more than 32 columns? Left-shifting the bitmask after that would move it outside of its 4-byte range and the bitwise OR operation would no longer work. Moving on, we have two more lines before we write data to Record : // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; In this example person_id has a value, so we do not need to return early. Now, we bitwise OR the current null bitmap byte with the first byte of bitmask , then assign the result to the null bitmap byte. It looks like this: 00000000 <-- bitmap 00000001 <-- bitmask -------- 00000001 <-- result Then we continue to write person_id to the Record . Now, our variables look like this: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 Remember, we're dealing with a little endian machine, so the first byte of person_id is what's populated. And the first bit in the null bitmap signifies that a value is present for the first column. 1 means the column has a value, 0 means the column is Null . fill_val - age Now let's see how we handle a Null value. Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 And our if/else block of code: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } Since bitmask is not equal to 0x80 (i.e. we're not at a byte boundary), we get to perform the left-shift operation this time. uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000010 (binary) <------ THIS CHANGED person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 The value of bitmask went from 1 to 2 as a result of the left-shift. Next, we have: // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; Since age is Null, we exit the function and proceed to the variable-length columns. fill_val - first_name Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000010 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 Since first_name is also Null, we perform the exact same actions we did for age : left-shift bitmask then return . fill_val - last_name Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000100 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 First up, we have our if/else block: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } Our current state satisfies the condition check, so we left-shift bitmask . New state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00001000 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; And because last_name has a value, we perform the bitwise OR then proceed to writing data to Record . The bitwise OR operation looks like this: 00000001 <-- bitmap 00001000 <-- bitmask -------- 00001001 <-- result This results in the final state of our record looking like: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00001000 (binary) person_id | bitmap | last_name 0100 0000 | 00001001 | 0700 4275 726b 65 ^ B u r k e | |---- 2-byte variable-length header To recap, we have a null bitmap that looks like 00001001 . Reading right to left, it tells us the first and fourth columns are Not Null, while the 2nd and 3rd columns are Null. Finishing Up The last two changes I need to highlight are all the way back in the serialize_data function. We need to free the memory for used by our two new fixedNull and varlenNull arrays. Now we're ready to run some inserts and inspect the data file. Remember to delete your existing data file before running the program with our new code. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 1 Null 'Burke' 33; ====== Node ====== = Type: Insert = person_id 1 = first_name NULL = last_name Burke = age 33 Bytes read: 0 bql > I run insert 1 Null 'Burke' 33; to insert some data with the first_name column being Null. Let's take a look at the data file. $ xxd db_files/main.dbd 00000000: 0100 0000 0000 0000 0000 0000 0000 0100 ................ 00000010: 4c00 4c00 0000 0000 0000 0000 0000 1400 L.L............. 00000020: 0100 0000 2100 0000 0b07 0042 7572 6b65 ....!......Burke 00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000070: 0000 0000 0000 0000 0000 0000 1400 1c00 ................ Not particularly interesting since we can't see the bits in the null bitmap. Let's look at the 1s and 0s by using the -b flag: $ xxd -b db_files/main.dbd 00000000: 00000001 00000000 00000000 00000000 00000000 00000000 ...... 00000006: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000000c: 00000000 00000000 00000001 00000000 01001100 00000000 ....L. 00000012: 01001100 00000000 00000000 00000000 00000000 00000000 L..... 00000018: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000001e: 00010100 00000000 00000001 00000000 00000000 00000000 ...... 00000024: 00100001 00000000 00000000 00000000 00001011 00000111 !..... <-- Null bitmap is second to last byte 0000002a: 00000000 01000010 01110101 01110010 01101011 01100101 .Burke 00000030: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000036: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000003c: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000042: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000048: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000004e: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000054: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000005a: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000060: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000066: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000006c: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000072: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000078: 00000000 00000000 00000000 00000000 00010100 00000000 ...... 0000007e: 00011100 00000000 .. I'm feeling lazy and don't want to draw colorful boxes around a screenshot this time, so I'll describe it. The interesting piece here is the null bitmap. It's on the line that starts with 00000024: and is the second to last byte on the row. Its bitfields are 00001011 . Reading right to left, it tells us the 1st, 3rd, and 4th columns are Not Null, and the 2nd column is Null. And that's all there is to incorporating the null bitmap in our \"write data\" operations. In the next section we'll go through the \"read data\" code.","title":"Writing Data"},{"location":"09-data-constraint-null/03-writing-data/#writing-data","text":"Now that our parser can handle the new way we parse insert statements, we can write the code that actually handles Null data and how it gets persisted to disk.","title":"Writing Data"},{"location":"09-data-constraint-null/03-writing-data/#mainc-file","text":"Like we've done before, we're going to pretend we're an insert statement and walk through the code path we'd follow, updating things accordingly. First up is our main function: src/main.c break; case T_InsertStmt: { - int32_t person_id = ((InsertStmt*)n)->personId; - char* firstName = ((InsertStmt*)n)->firstName; - char* lastName = ((InsertStmt*)n)->lastName; - int32_t age = ((InsertStmt*)n)->age; + InsertStmt* i = (InsertStmt*)n; - if (!insert_record(bp, person_id, firstName, lastName, age)) { + if (!insert_record(bp, i->values)) { printf(\"Unable to insert record\\n\"); } break; } case T_SelectStmt: if (!analyze_node(n)) { First, we get rid of references to the hard-coded fields in the old InsertStmt struct. Then in our call to insert_record , we just pass in the values . Which means we need to go refactor the insert_record function: src/main.c -static bool insert_record(BufPool* bp, int32_t person_id, char* firstName, char* lastName, int32_t age) { +static bool insert_record(BufPool* bp, ParseList* values) { BufPoolSlot* slot = bufpool_read_page(bp, 1); if (slot == NULL) slot = bufpool_new_page(bp); RecordDescriptor* rd = construct_record_descriptor(); - int recordLength = compute_record_length(rd, person_id, firstName, lastName, age); + int recordLength = compute_record_length(rd, values); Record r = record_init(recordLength); - serialize_data(rd, r, person_id, firstName, lastName, age); + serialize_data(rd, r, values); bool insertSuccessful = page_insert(slot->pg, r, recordLength); free_record_desc(rd); free(r); return insertSuccessful; } The logic is exactly the same, but we need to replace the hard-coded input parameters for a single ParseList* values param. Then we replace any function calls that consumed those hard-coded parameters with a call that only passes in the values list. Now we need to go refactor compute_record_length and serialize_data . src/main.c static int compute_record_length(RecordDescriptor* rd, ParseList* values) { int len = 12; // start with the 12-byte header len += compute_null_bitmap_length(rd); len += 4; // person_id Literal* firstName = (Literal*)values->elements[1].ptr; Literal* lastName = (Literal*)values->elements[2].ptr; Literal* age = (Literal*)values->elements[3].ptr; /* for the varlen columns, we default to their max length if the values we're trying to insert would overflow them (they'll get truncated later) */ if (firstName->isNull) { len += 0; // Nulls do not consume any space } else if (strlen(firstName->str) > rd->cols[1].len) { len += (rd->cols[1].len + 2); } else { len += (strlen(firstName->str) + 2); } // We don't need a null check because this column is constrained to be Not Null if (strlen(lastName->str) > rd->cols[2].len) { len += (rd->cols[2].len + 2); } else { len += (strlen(lastName->str) + 2); } if (age->isNull) { len += 0; // Nulls do not consume any space } else { len += 4; // age } return len; } So much of this function changed, that I'm just going to call it a full rewrite. This is definitely a temporary function because it only works due to the assumptions we use. Starting at the top, we have 12 bytes for the record header - that's fine, no way around it. Next, we add the length of the null bitmap. And for that we create a new function to calculate its size - covered below. Next, we add 4 bytes for the person_id column. Since we know it cannot be Null (assumption), we know it ALWAYS consumes 4 bytes. Next, we parse out the Literal 's of the remaining three columns. For firstName , we check if the value passed in by the user is Null or not. If so, we don't add anything to the record's length because Null 's are absent in the physical record. If it's Not Null , then we perform the same logic as before: if the value's length exceeds the column's max length, we add max length + 2 to the record's length. For the lastName field, we can skip the null check because we know it cannot be Null (assumption), and perform the same logic we do for the firstName field. Lastly, we have a null check on the age column. If Null , the length is 0. If Not Null , the length is 4. How do we determine the size of the null bitmap? Simple, each column corresponds to a single bit in the null bitmap. For table with 1 to 8 columns, the null bitmap consumes 1 byte of space. If the table has 9 to 16 columns, the null bitmap consumes 2 bytes. And so on.","title":"main.c File"},{"location":"09-data-constraint-null/03-writing-data/#record-data","text":"src/include/storage/record.h +int compute_null_bitmap_length(RecordDescriptor* rd); src/storage/record.c +/** + * returns the number of bytes consumed by the null bitmap + * + * Every 8 columns requires an additional byte for the null bitmap +*/ +int compute_null_bitmap_length(RecordDescriptor* rd) { + if (!rd->hasNullableColumns) return 0; + + return (rd->ncols) / 8 + 1; +} int compute_record_fixed_length(RecordDescriptor* rd, bool* fixedNull) { This is actually going to be a permanent function, so we get to write it in the record.c file. Basically, the null bitmap consumes 1 byte for every 8 columns in the table. Okay, back to the insert_record function. The next refactoring target on our path is the serialize_data function: src/main.c -static void serialize_data(RecordDescriptor* rd, Record r, int32_t person_id, char* firstName, char* lastName, int32_t age) { static void serialize_data(RecordDescriptor* rd, Record r, ParseList* values) { Datum* fixed = malloc(rd->nfixed * sizeof(Datum)); Datum* varlen = malloc((rd->ncols - rd->nfixed) * sizeof(Datum)); + bool* fixedNull = malloc(rd->nfixed * sizeof(bool)); + bool* varlenNull = malloc((rd->ncols - rd->nfixed) * sizeof(bool)); - populate_datum_array(fixed, varlen, person_id, firstName, lastName, age); populate_datum_array(fixed, varlen, fixedNull, varlenNull, values); + int nullOffset = sizeof(RecordHeader) + compute_record_fixed_length(rd, fixedNull); + ((RecordHeader*)r)->nullOffset = nullOffset; + uint8_t* nullBitmap = r + nullOffset; - fill_record(rd, r + sizeof(RecordHeader), fixed, varlen); + fill_record(rd, r + sizeof(RecordHeader), fixed, varlen, fixedNull, varlenNull, nullBitmap); free(fixed); free(varlen); + free(fixedNull); + free(varlenNull); } First, we do the expected replacement of the hard-coded column parameters for the ParseList* values param. Then we need to create a pair of bool arrays to match the Datum arrays for the fixed and variable-length fields. The bool arrays are used to indicate whether or not the corresponding value is Null or not. Next, we make a call to the refactored populate_datum_array function. The refactored version (covered below) will populate values for our two Datum arrays and our two bool arrays based on the user-supplied values data. Following that, we have our very first usage of a RecordHeader field. We need to set the nullOffset field to represent the position of the null bitmap in the Record data. Remember, the null bitmap lives between the fixed-length columns and the variable-length columns. So its location can be calculated by (record header size) + (fixed-length size) . And in order to compute the fixed-length size, we need a new (permanent) helper function. Next, we have a refactored version of the fill_record function. We need to pass it the two new bool arrays, and a pointer to the null bitmap. Finally, we need to free the memory consumed by the two new bool arrays. As for the refactored populate_datum_array function, it is now going to populate the two null arrays alongside the values arrays. And the function is changing so much, we're just going to scrap it and write a new one from scratch. src/main.c static void populate_datum_array(Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, ParseList* values) { Literal* personId = (Literal*)values->elements[0].ptr; Literal* firstName = (Literal*)values->elements[1].ptr; Literal* lastName = (Literal*)values->elements[2].ptr; Literal* age = (Literal*)values->elements[3].ptr; fixed[0] = int32GetDatum(personId->intVal); fixedNull[0] = false; if (age->isNull) { fixed[1] = (Datum)NULL; fixedNull[1] = true; } else { fixed[1] = int32GetDatum(age->intVal); fixedNull[1] = false; } if (firstName->isNull) { varlen[0] = (Datum)NULL; varlenNull[0] = true; } else { varlen[0] = charGetDatum(firstName->str); varlenNull[0] = false; } varlen[1] = charGetDatum(lastName->str); varlenNull[1] = false; } First we parse out the Literal 's for each of our four hard-coded columns. Then we start filling in the value and null arrays. Since person_id is Not Null , we don't need a null check and can assign the values directly. age , on the other hand, can be Null , so we perform a null check before populating the arrays. Notice for the null case we cast NULL to a Datum . Why? Because our array expects a Datum type and the compiler will yell at us otherwise. We follow similar logic for the variable-length columns. first_name is Null able, so we need a null check, whereas last_name is not. Jumping back up to the serialize_data function, the next change we come across is fill_record . We have three additional parameters to pass in to the new version of the function: both null arrays and a pointer to the null bitmap. Let's update the header file first, then get to the code. src/include/storage.record.h -void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen); +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, uint8_t* bitmap); src/storage/record.c -void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen) { +void fill_record(RecordDescriptor* rd, Record r, Datum* fixed, Datum* varlen, bool* fixedNull, bool* varlenNull, uint8_t* nullBitmap) { + uint8_t* bitP = &nullBitmap[-1]; + int bitmask = 0x80; // fill fixed-length columns for (int i = 0; i < rd->nfixed; i++) { Column* col = get_nth_col(rd, true, i); - fill_val(col, &r, fixed[i]); + fill_val( + col, + &bitP, + &bitmask, + &r, + fixed[i], + fixedNull[i] + ); } + // jump past the null bitmap + r += compute_null_bitmap_length(rd); // fill varlen columns for (int i = 0; i < (rd->ncols - rd->nfixed); i++) { Column* col = get_nth_col(rd, false, i); - fill_val(col, &r, varlen[i]); + fill_val( + col, + &bitP, + &bitmask, + &r, + varlen[i], + varlenNull[i] + ); } } Since we're introducing the null bitmap, we'll need to start writing some bitwise operations. That's where these first two lines of the function come in. The actual bitwise operations will happen in fill_val , but we need to pass in a pointer to the null bitmap that it can operate on, as well as a bitmask to use for populating bits in the null bitmap. Why do we assign bitP to the -1 index? Is that legal? Although it looks like an invalid address, it technically is because the null bitmap is inside the Record chunk of memory. The -1 index just means it's pointing to the byte immediately preceeding the null bitmap. We'll go over the \"why\" below when we walk through an example. src/storage/record.c -static void fill_val(Column* col, char** dataP, Datum datum) { +static void fill_val(Column* col, uint8_t** bit, int* bitmask, char** dataP, Datum datum, bool isNull) { int16_t dataLen; char* data = *dataP; + if (*bitmask != 0x80) { + *bitmask <<= 1; + } else { + *bit += 1; + **bit = 0x0; + *bitmask = 1; + } + + // column is null, nothing more to do + if (isNull) return; + + **bit |= *bitmask; switch (col->dataType) { case DT_BOOL: // Bools and TinyInts are the same C-type case DT_TINYINT: *** omitted for brevity *** } Working with the actual data remains the same as before. The only difference is we added some logic to populate the bitfields in the null bitmap and short-circuit the function if the value to insert is Null . So how and why does this work?","title":"Record Data"},{"location":"09-data-constraint-null/03-writing-data/#step-by-step-example","text":"Let's walk through an example line by line. Pretend we're the computer and we're trying to insert the following record: person_id first_name last_name age 1 Null Burke Null Ignoring the row header, how many bytes do we need to store this record? In physical order of the data: 4-byte person_id 0-byte age (Null) 1-byte null bitmap 0-byte first_name (Null) 7-byte last_name (2-byte overhead plus 5-bytes for data) Total : 12 bytes We start with a blank chunk of memory that represents the Record we're going to fill. Again, I'm ignoring the header portion in this example. Here's what we're working with: person_id | bitmap | last_name 0000 0000 | 00000000 | 0000 0000 0000 00 IMPORTANT : I'm representing person_id and last_name in hex like the xxd command shows, but I'm showing all 8 individual bits for the null bitmap portion.","title":"Step-by-Step Example"},{"location":"09-data-constraint-null/03-writing-data/#fill_record","text":"Starting at the top, we initialize two variables: uint8_t* bitP: points to the last byte of the person_id data int bitmask: 10000000 (binary) only showing the first byte Note: although bitmask is a 4-byte int, I'm only going to show the first byte throughout this example because that's the only part of it that's relevant. Next, we go into the loop where we fill_val all of the fixed-length columns. Our table has two fixed-length columns, so we start with person_id and pass in the appropriate parameters. Relevant to this example, we supply &bitP - a pointer to the pointer that currently points to the byte preceeding the null bitmap. And &bitmask - a pointer to the bitmask variable.","title":"fill_record"},{"location":"09-data-constraint-null/03-writing-data/#fill_val-person_id","text":"Since we already know how this function writes data to our Record , I'm just going to focus on the new null bitmap operations. At the top, we're immediately confronted with this if/else statement: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } If bitmask is not equal to 0x80 ( 10000000 ), then we left shift bitmask by one. If you remember in fill_record , we explicitly initialized bitmask to 0x80 , so we can proceed to the else block. *bit += 1; Here, we advance the bitmap pointer by one. Remember how we started out by pointing to the byte preceeding the null bitmap? This is where we course-correct by pointing to the actual beginning of the null bitmap. **bit = 0x0; Next, we set the entire byte of the null bitmap to 0s. It doesn't matter how large the null bitmap is, this instruction will only set a single byte to 0s. *bitmask = 1; Reset the bitmask to 1 ( 00000001 ). Here's the current state of our variables: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0000 0000 | 00000000 | 0000 0000 0000 00 Why the convoluted mess with the if/else block and bitwise operations? Its purpose is to reset the bitmask back to 1 each time we hit a byte boundary. That situation won't come into play during this example because we only have four columns to worry about. Since each byte in the null bitmap can cover 8 columns, that logic is only important for tables with more than 8 columns. Now, why do we need to reset the bitmask at byte boundaries? Because the bitmap pointer ( bitP ) always points to the byte we're currently working with. We have a bitwise OR operation coming up between the bitmap byte and the bitmask. In order for this to work properly, we need to make sure we're only using the first byte of the bitmask variable. Final question: why do we need to advance the bitP pointer? Can't we just keep it pointing to the first byte in the null bitmap so that left-shifting the bitmask after 8 columns will still work? Short answer: no. What if we have a table with more than 32 columns? Left-shifting the bitmask after that would move it outside of its 4-byte range and the bitwise OR operation would no longer work. Moving on, we have two more lines before we write data to Record : // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; In this example person_id has a value, so we do not need to return early. Now, we bitwise OR the current null bitmap byte with the first byte of bitmask , then assign the result to the null bitmap byte. It looks like this: 00000000 <-- bitmap 00000001 <-- bitmask -------- 00000001 <-- result Then we continue to write person_id to the Record . Now, our variables look like this: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 Remember, we're dealing with a little endian machine, so the first byte of person_id is what's populated. And the first bit in the null bitmap signifies that a value is present for the first column. 1 means the column has a value, 0 means the column is Null .","title":"fill_val - person_id"},{"location":"09-data-constraint-null/03-writing-data/#fill_val-age","text":"Now let's see how we handle a Null value. Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000001 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 And our if/else block of code: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } Since bitmask is not equal to 0x80 (i.e. we're not at a byte boundary), we get to perform the left-shift operation this time. uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000010 (binary) <------ THIS CHANGED person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 The value of bitmask went from 1 to 2 as a result of the left-shift. Next, we have: // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; Since age is Null, we exit the function and proceed to the variable-length columns.","title":"fill_val - age"},{"location":"09-data-constraint-null/03-writing-data/#fill_val-first_name","text":"Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000010 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 Since first_name is also Null, we perform the exact same actions we did for age : left-shift bitmask then return .","title":"fill_val - first_name"},{"location":"09-data-constraint-null/03-writing-data/#fill_val-last_name","text":"Current state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00000100 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 First up, we have our if/else block: if (*bitmask != 0x80) { *bitmask <<= 1; } else { *bit += 1; **bit = 0x0; *bitmask = 1; } Our current state satisfies the condition check, so we left-shift bitmask . New state: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00001000 (binary) person_id | bitmap | last_name 0100 0000 | 00000001 | 0000 0000 0000 00 // column is null, nothing more to do if (isNull) return; **bit |= *bitmask; And because last_name has a value, we perform the bitwise OR then proceed to writing data to Record . The bitwise OR operation looks like this: 00000001 <-- bitmap 00001000 <-- bitmask -------- 00001001 <-- result This results in the final state of our record looking like: uint8_t* bitP: pointing to the beginning of the null bitmap int bitmask: 00001000 (binary) person_id | bitmap | last_name 0100 0000 | 00001001 | 0700 4275 726b 65 ^ B u r k e | |---- 2-byte variable-length header To recap, we have a null bitmap that looks like 00001001 . Reading right to left, it tells us the first and fourth columns are Not Null, while the 2nd and 3rd columns are Null.","title":"fill_val - last_name"},{"location":"09-data-constraint-null/03-writing-data/#finishing-up","text":"The last two changes I need to highlight are all the way back in the serialize_data function. We need to free the memory for used by our two new fixedNull and varlenNull arrays. Now we're ready to run some inserts and inspect the data file. Remember to delete your existing data file before running the program with our new code. $ rm -f db_files/main.dbd $ make clean && make && ./burkeql ====== BurkeQL Config ====== = DATA_FILE: /home/burke/source_control/burkeql-db/db_files/main.dbd = PAGE_SIZE: 128 bql > insert 1 Null 'Burke' 33; ====== Node ====== = Type: Insert = person_id 1 = first_name NULL = last_name Burke = age 33 Bytes read: 0 bql > I run insert 1 Null 'Burke' 33; to insert some data with the first_name column being Null. Let's take a look at the data file. $ xxd db_files/main.dbd 00000000: 0100 0000 0000 0000 0000 0000 0000 0100 ................ 00000010: 4c00 4c00 0000 0000 0000 0000 0000 1400 L.L............. 00000020: 0100 0000 2100 0000 0b07 0042 7572 6b65 ....!......Burke 00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 00000070: 0000 0000 0000 0000 0000 0000 1400 1c00 ................ Not particularly interesting since we can't see the bits in the null bitmap. Let's look at the 1s and 0s by using the -b flag: $ xxd -b db_files/main.dbd 00000000: 00000001 00000000 00000000 00000000 00000000 00000000 ...... 00000006: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000000c: 00000000 00000000 00000001 00000000 01001100 00000000 ....L. 00000012: 01001100 00000000 00000000 00000000 00000000 00000000 L..... 00000018: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000001e: 00010100 00000000 00000001 00000000 00000000 00000000 ...... 00000024: 00100001 00000000 00000000 00000000 00001011 00000111 !..... <-- Null bitmap is second to last byte 0000002a: 00000000 01000010 01110101 01110010 01101011 01100101 .Burke 00000030: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000036: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000003c: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000042: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000048: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000004e: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000054: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000005a: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000060: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000066: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 0000006c: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000072: 00000000 00000000 00000000 00000000 00000000 00000000 ...... 00000078: 00000000 00000000 00000000 00000000 00010100 00000000 ...... 0000007e: 00011100 00000000 .. I'm feeling lazy and don't want to draw colorful boxes around a screenshot this time, so I'll describe it. The interesting piece here is the null bitmap. It's on the line that starts with 00000024: and is the second to last byte on the row. Its bitfields are 00001011 . Reading right to left, it tells us the 1st, 3rd, and 4th columns are Not Null, and the 2nd column is Null. And that's all there is to incorporating the null bitmap in our \"write data\" operations. In the next section we'll go through the \"read data\" code.","title":"Finishing Up"},{"location":"09-data-constraint-null/04-reading-data/","text":"Reading Data For the reading data changes, we again follow the code from the top and make updates as needed. Starting at the entry point for a Select statement, here's the top-level code block we're going to follow: case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); } else { TableDesc* td = new_tabledesc(\"person\"); td->rd = construct_record_descriptor(); RecordSet* rs = new_recordset(); rs->rows = new_linkedlist(); RecordDescriptor* targets = construct_record_descriptor_from_target_list(((SelectStmt*)n)->targetList); tableam_fullscan(bp, td, rs->rows); resultset_print(td->rd, rs, targets); free_recordset(rs, td->rd); free_tabledesc(td); free_record_desc(targets); } break; RecordDescriptor The line: td->rd = construct_record_descriptor(); is where our first change happens. We need to add nullability flags to the RecordDescriptor and Column structs. These flags will allow our output code to skip a bunch of logic checks for cases when the column value is Not Null . src/include/storage/record.h #include <stdint.h> +#include <stdbool.h> #include \"storage/datum.h\" *** omitted for brevity *** #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct Column { char* colname; DataType dataType; int colnum; /* 0-based col index */ int len; + bool isNotNull; } Column; *** omitted for brevity *** typedef struct RecordDescriptor { int ncols; /* number of columns (defined by the Create Table DDL) */ int nfixed; /* number of fixed-length columns */ + bool hasNullableColumns; Column cols[]; } RecordDescriptor; Record record_init(uint16_t recordLen); void free_record(Record r); void free_record_desc(RecordDescriptor* rd); -void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); +void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len, bool isNotNull); Two simple changes going on here. We add nullability flags to both the Column and RecordDescriptor structs, and we add a new parameter to the column constructor function. src/storage/record.c -void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len) { +void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len, bool isNotNull) { col->colname = strdup(colname); col->dataType = type; col->colnum = colnum; col->len = len; + col->isNotNull = isNotNull; } We just need to add a new line to set the isNotNull property of the Column struct. src/main.c static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (4 * sizeof(Column))); rd->ncols = 4; rd->nfixed = 2; - construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); - construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20); - construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20); - construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4); + construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4, true); + construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20, false); + construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20, true); + construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4, false); + + rd->hasNullableColumns = true; return rd; } static RecordDescriptor* construct_record_descriptor_from_target_list(ParseList* targetList) { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (targetList->length * sizeof (Column))); rd->ncols = targetList->length; for (int i = 0; i < rd->ncols; i++) { ResTarget* t = (ResTarget*)targetList->elements[i].ptr; - // we don't care about the data type or length here + // we don't care about the data type, length, or nullability here - construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0); + construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0, true); } return rd; } And here, we make sure to pass in a value to the new isNotNull parameter. RecordSet Back to our top-level code, the next line we dive into is: RecordSet* rs = new_recordset(); We have similar changes to the RecordSet regime as we had to RecordDescriptor . The changes are primarily focused on the RecordSetRow struct. Similar to how it keeps an array of column values, we also need to keep a bool array to identify if the column is null or not. This will make it easier for our output code to identify when a column's value is NULL or not. src/include/resultset/recordset.h typedef struct RecordSetRow { Datum* values; + bool* isnull; } RecordSetRow; src/resultset/recordset.c RecordSetRow* new_recordset_row(int ncols) { RecordSetRow* row = malloc(sizeof(RecordSetRow)); row->values = malloc(ncols * sizeof(Datum)); + row->isnull = malloc(ncols * sizeof(bool)); return row; } void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd) { if (row->values != NULL) { free_recordset_row_columns(row, rd); free(row->values); + free(row->isnull); } free(row); } These basic changes simply allocate and free the memory reserved for the new bool array in a RecordSetRow . defill_record Continuing along our top-level code block, the next line we dive into: tableam_fullscan(bp, td, rs->rows); This function makes a call to defill_record . Since we made several changes to fill_record , it only makes sense we have to refactor its opposite. src/access/tableam.c void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows) { BufPoolSlot* slot = bufpool_read_page(bp, 1); while (slot != NULL) { PageHeader* pgHdr = (PageHeader*)slot->pg; int numRecords = pgHdr->numRecords; for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); - defill_record(td->rd, slot->pg + sp->offset, row->values); + defill_record(td->rd, slot->pg + sp->offset, row->values, row->isnull); linkedlist_append(rows, row); } slot = bufpool_read_page(bp, pgHdr->nextPageId); } } We simply need to pass it a reference to ResultSetRow 's array of nulls. The magic happens below.. src/include/storage/record.h -void defill_record(RecordDescriptor* rd, Record r, Datum* values); +void defill_record(RecordDescriptor* rd, Record r, Datum* values, bool* isnull); src/storage/record.c -void defill_record(RecordDescriptor* rd, Record r, Datum* values) { +void defill_record(RecordDescriptor* rd, Record r, Datum* values, bool* isnull) { int offset = sizeof(RecordHeader); + uint16_t nullOffset = ((RecordHeader*)r)->nullOffset; + uint8_t* nullBitmap = r + nullOffset; Column* col; for (int i = 0; i < rd->ncols; i++) { + // we've passed the fixed-length, so we skip over the null bitmap + if (i == rd->nfixed) offset += compute_null_bitmap_length(rd); if (i < rd->nfixed) { col = get_nth_col(rd, true, i); } else { col = get_nth_col(rd, false, i - rd->nfixed); } - values[col->colnum] = record_get_col_value(col, r, &offset); + if (col_isnull(i, nullBitmap)) { + values[col->colnum] = (Datum)NULL; + isnull[col->colnum] = true; + } else { + values[col->colnum] = record_get_col_value(col, r, &offset); + isnull[col->colnum] = false; } } } Because we're now dealing with Null columns, we need to find the null bitmap. Fortunately the RecordHeader stores a pointer to the location of the null bitmap, which makes it trivial to find the beginning of the null bitmap. When we're reading data from a record, we read left to right. And remember fixed-length columns are physically stored on the left side of a record, while variable-length columns are stored on the right - separated by the null bitmap. So at the beginning of the loop, we need to check if we've read through all of the fixed-length columns. If so, we need to adjust the read pointer ( offset ) to skip past the null bitmap and start reading the variable-length columns. THe last set of changes involves logic to set the isnull array appropriately. The check utilizes a new function that finds the corresponding bit in the null bitmap for the current column we're trying to read: src/include/storage/record.h +bool col_isnull(int colnum, uint8_t* nullBitmap); src/storage/record.c +bool col_isnull(int colnum, uint8_t* nullBitmap) { + return !(nullBitmap[colnum >> 3] & (1 << (colnum & 0x07))); +} This function takes two parameters: the column number in the table (counting from left to right as the data are physically stored), and a pointer to the null bitmap. It returns true if the column corresponding to colnum is Null , false if there is data stored in the column. At first glance this function may look a little funky due to the weird-looking bitwise operations, but it will make a lot of sense as we work through some examples. Recall the bitwise logic in the fill_val function from the previous section. The entire purpose of the bit-shift and bitwise OR operations were to reset the the bitmask to 1 each time we hit a byte boundary. The bitwise operations in this function serve a very similar purpose. Let's run through some examples to see how it works. col_isnull examples Let's say we have a table with 16 columns; meaning we'll need a 2-byte null bitmap. Bitmap: 0010 1110 1101 1100 col_isnull - colnum = 3 colnum: 0000 0010 Remember colnum is a zero-based index, so colnum = 3 is referring to the fourth column in this record. Reading right to left, the fourth column is a binary 1 , which means the column is not null. Let's evaluate the function step-by-step. colnum >> 3 0000 0010 >> 3 = 0000 0000 Left-shifting colnum by 3 bits results in a decimal value of 0. Using that, we access the nullBitmap as if it were an array; meaning we want the 0th uint8_t block in the array - the first byte of the null bitmap. nullBitmap[0] = 1101 1100 Now on the right side, we left-shift decimal 1 by the result of colnum & 0x07 colnum & 0x07 = 3 & 0x07 0000 0010 <-- 3 0000 0111 <-- 0x07 --------- & 0000 0010 <-- bitwise & result = 3 So we left-shift decimal 1 by 3 bits. 0000 0001 << 3 = 0000 1000 Finally, we bitwise & the left side with the right side 1101 1100 <-- 0th byte of the null bitmap 0000 1000 <-- result of the right side --------- & 0000 1000 = 8 So the function will return !(8) . In C, false is defined as exactly 0; true is defined as \"not false\". Since 8 is not zero, it evaluates to true and we return the NOT of true . So our function will return false , meaning our column has data. col_isnull - colnum = 8 This time, we'll pick a colnum representing a column in the first bit of the second byte in the null bitmap. This example will really show the purpose of these bitwise operations. colnum: 0000 1000 colnum >> 3 0000 1000 >> 3 = 0000 0001 = 1 Meaning we want the 1st (0-based) uint8_t block in the nullBitmap array. nullBitmap[1] = 0010 1110 On the right side we evaluate the following: colnum & 0x07 = 8 & 0x07 0000 1000 <-- 8 0000 0111 <-- 0x07 --------- & 0000 0000 = 0 Left-shift decimal 1 by 0 bits yeilds decimal 1. So our final operation: 0010 1110 <-- 1st (0-based) byte of the null bitmap 0000 0000 <-- result of the right side --------- & 0000 0000 = 0 We return the opposite of false , which is true . Our column is Null . Hopefully the above made it clear how these operations keep our nullBitmap inspection coordinated. I.e. the purpose of the bit-shift in nullBitmap[colnum >> 3] is to access the next block of 8-bits for every 8 values of colnum . The purpose of colnum & 0x07 is to effectively normalize colnum to a value between 0 and 7. Then the 1 << (...) piece maps that normalized 0-7 value to the correct bit.","title":"Reading Data"},{"location":"09-data-constraint-null/04-reading-data/#reading-data","text":"For the reading data changes, we again follow the code from the top and make updates as needed. Starting at the entry point for a Select statement, here's the top-level code block we're going to follow: case T_SelectStmt: if (!analyze_node(n)) { printf(\"Semantic analysis failed\\n\"); } else { TableDesc* td = new_tabledesc(\"person\"); td->rd = construct_record_descriptor(); RecordSet* rs = new_recordset(); rs->rows = new_linkedlist(); RecordDescriptor* targets = construct_record_descriptor_from_target_list(((SelectStmt*)n)->targetList); tableam_fullscan(bp, td, rs->rows); resultset_print(td->rd, rs, targets); free_recordset(rs, td->rd); free_tabledesc(td); free_record_desc(targets); } break;","title":"Reading Data"},{"location":"09-data-constraint-null/04-reading-data/#recorddescriptor","text":"The line: td->rd = construct_record_descriptor(); is where our first change happens. We need to add nullability flags to the RecordDescriptor and Column structs. These flags will allow our output code to skip a bunch of logic checks for cases when the column value is Not Null . src/include/storage/record.h #include <stdint.h> +#include <stdbool.h> #include \"storage/datum.h\" *** omitted for brevity *** #pragma pack(push, 1) /* disabling memory alignment because I don't want to deal with it */ typedef struct Column { char* colname; DataType dataType; int colnum; /* 0-based col index */ int len; + bool isNotNull; } Column; *** omitted for brevity *** typedef struct RecordDescriptor { int ncols; /* number of columns (defined by the Create Table DDL) */ int nfixed; /* number of fixed-length columns */ + bool hasNullableColumns; Column cols[]; } RecordDescriptor; Record record_init(uint16_t recordLen); void free_record(Record r); void free_record_desc(RecordDescriptor* rd); -void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len); +void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len, bool isNotNull); Two simple changes going on here. We add nullability flags to both the Column and RecordDescriptor structs, and we add a new parameter to the column constructor function. src/storage/record.c -void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len) { +void construct_column_desc(Column* col, char* colname, DataType type, int colnum, int len, bool isNotNull) { col->colname = strdup(colname); col->dataType = type; col->colnum = colnum; col->len = len; + col->isNotNull = isNotNull; } We just need to add a new line to set the isNotNull property of the Column struct. src/main.c static RecordDescriptor* construct_record_descriptor() { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (4 * sizeof(Column))); rd->ncols = 4; rd->nfixed = 2; - construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4); - construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20); - construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20); - construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4); + construct_column_desc(&rd->cols[0], \"person_id\", DT_INT, 0, 4, true); + construct_column_desc(&rd->cols[1], \"first_name\", DT_VARCHAR, 1, 20, false); + construct_column_desc(&rd->cols[2], \"last_name\", DT_VARCHAR, 2, 20, true); + construct_column_desc(&rd->cols[3], \"age\", DT_INT, 3, 4, false); + + rd->hasNullableColumns = true; return rd; } static RecordDescriptor* construct_record_descriptor_from_target_list(ParseList* targetList) { RecordDescriptor* rd = malloc(sizeof(RecordDescriptor) + (targetList->length * sizeof (Column))); rd->ncols = targetList->length; for (int i = 0; i < rd->ncols; i++) { ResTarget* t = (ResTarget*)targetList->elements[i].ptr; - // we don't care about the data type or length here + // we don't care about the data type, length, or nullability here - construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0); + construct_column_desc(&rd->cols[i], t->name, DT_UNKNOWN, i, 0, true); } return rd; } And here, we make sure to pass in a value to the new isNotNull parameter.","title":"RecordDescriptor"},{"location":"09-data-constraint-null/04-reading-data/#recordset","text":"Back to our top-level code, the next line we dive into is: RecordSet* rs = new_recordset(); We have similar changes to the RecordSet regime as we had to RecordDescriptor . The changes are primarily focused on the RecordSetRow struct. Similar to how it keeps an array of column values, we also need to keep a bool array to identify if the column is null or not. This will make it easier for our output code to identify when a column's value is NULL or not. src/include/resultset/recordset.h typedef struct RecordSetRow { Datum* values; + bool* isnull; } RecordSetRow; src/resultset/recordset.c RecordSetRow* new_recordset_row(int ncols) { RecordSetRow* row = malloc(sizeof(RecordSetRow)); row->values = malloc(ncols * sizeof(Datum)); + row->isnull = malloc(ncols * sizeof(bool)); return row; } void free_recordset_row(RecordSetRow* row, RecordDescriptor* rd) { if (row->values != NULL) { free_recordset_row_columns(row, rd); free(row->values); + free(row->isnull); } free(row); } These basic changes simply allocate and free the memory reserved for the new bool array in a RecordSetRow .","title":"RecordSet"},{"location":"09-data-constraint-null/04-reading-data/#defill_record","text":"Continuing along our top-level code block, the next line we dive into: tableam_fullscan(bp, td, rs->rows); This function makes a call to defill_record . Since we made several changes to fill_record , it only makes sense we have to refactor its opposite. src/access/tableam.c void tableam_fullscan(BufPool* bp, TableDesc* td, LinkedList* rows) { BufPoolSlot* slot = bufpool_read_page(bp, 1); while (slot != NULL) { PageHeader* pgHdr = (PageHeader*)slot->pg; int numRecords = pgHdr->numRecords; for (int i = 0; i < numRecords; i++) { RecordSetRow* row = new_recordset_row(td->rd->ncols); int slotPointerOffset = conf->pageSize - (sizeof(SlotPointer) * (i + 1)); SlotPointer* sp = (SlotPointer*)(slot->pg + slotPointerOffset); - defill_record(td->rd, slot->pg + sp->offset, row->values); + defill_record(td->rd, slot->pg + sp->offset, row->values, row->isnull); linkedlist_append(rows, row); } slot = bufpool_read_page(bp, pgHdr->nextPageId); } } We simply need to pass it a reference to ResultSetRow 's array of nulls. The magic happens below.. src/include/storage/record.h -void defill_record(RecordDescriptor* rd, Record r, Datum* values); +void defill_record(RecordDescriptor* rd, Record r, Datum* values, bool* isnull); src/storage/record.c -void defill_record(RecordDescriptor* rd, Record r, Datum* values) { +void defill_record(RecordDescriptor* rd, Record r, Datum* values, bool* isnull) { int offset = sizeof(RecordHeader); + uint16_t nullOffset = ((RecordHeader*)r)->nullOffset; + uint8_t* nullBitmap = r + nullOffset; Column* col; for (int i = 0; i < rd->ncols; i++) { + // we've passed the fixed-length, so we skip over the null bitmap + if (i == rd->nfixed) offset += compute_null_bitmap_length(rd); if (i < rd->nfixed) { col = get_nth_col(rd, true, i); } else { col = get_nth_col(rd, false, i - rd->nfixed); } - values[col->colnum] = record_get_col_value(col, r, &offset); + if (col_isnull(i, nullBitmap)) { + values[col->colnum] = (Datum)NULL; + isnull[col->colnum] = true; + } else { + values[col->colnum] = record_get_col_value(col, r, &offset); + isnull[col->colnum] = false; } } } Because we're now dealing with Null columns, we need to find the null bitmap. Fortunately the RecordHeader stores a pointer to the location of the null bitmap, which makes it trivial to find the beginning of the null bitmap. When we're reading data from a record, we read left to right. And remember fixed-length columns are physically stored on the left side of a record, while variable-length columns are stored on the right - separated by the null bitmap. So at the beginning of the loop, we need to check if we've read through all of the fixed-length columns. If so, we need to adjust the read pointer ( offset ) to skip past the null bitmap and start reading the variable-length columns. THe last set of changes involves logic to set the isnull array appropriately. The check utilizes a new function that finds the corresponding bit in the null bitmap for the current column we're trying to read: src/include/storage/record.h +bool col_isnull(int colnum, uint8_t* nullBitmap); src/storage/record.c +bool col_isnull(int colnum, uint8_t* nullBitmap) { + return !(nullBitmap[colnum >> 3] & (1 << (colnum & 0x07))); +} This function takes two parameters: the column number in the table (counting from left to right as the data are physically stored), and a pointer to the null bitmap. It returns true if the column corresponding to colnum is Null , false if there is data stored in the column. At first glance this function may look a little funky due to the weird-looking bitwise operations, but it will make a lot of sense as we work through some examples. Recall the bitwise logic in the fill_val function from the previous section. The entire purpose of the bit-shift and bitwise OR operations were to reset the the bitmask to 1 each time we hit a byte boundary. The bitwise operations in this function serve a very similar purpose. Let's run through some examples to see how it works.","title":"defill_record"},{"location":"09-data-constraint-null/04-reading-data/#col_isnull-examples","text":"Let's say we have a table with 16 columns; meaning we'll need a 2-byte null bitmap. Bitmap: 0010 1110 1101 1100","title":"col_isnull examples"},{"location":"09-data-constraint-null/04-reading-data/#col_isnull-colnum-3","text":"colnum: 0000 0010 Remember colnum is a zero-based index, so colnum = 3 is referring to the fourth column in this record. Reading right to left, the fourth column is a binary 1 , which means the column is not null. Let's evaluate the function step-by-step. colnum >> 3 0000 0010 >> 3 = 0000 0000 Left-shifting colnum by 3 bits results in a decimal value of 0. Using that, we access the nullBitmap as if it were an array; meaning we want the 0th uint8_t block in the array - the first byte of the null bitmap. nullBitmap[0] = 1101 1100 Now on the right side, we left-shift decimal 1 by the result of colnum & 0x07 colnum & 0x07 = 3 & 0x07 0000 0010 <-- 3 0000 0111 <-- 0x07 --------- & 0000 0010 <-- bitwise & result = 3 So we left-shift decimal 1 by 3 bits. 0000 0001 << 3 = 0000 1000 Finally, we bitwise & the left side with the right side 1101 1100 <-- 0th byte of the null bitmap 0000 1000 <-- result of the right side --------- & 0000 1000 = 8 So the function will return !(8) . In C, false is defined as exactly 0; true is defined as \"not false\". Since 8 is not zero, it evaluates to true and we return the NOT of true . So our function will return false , meaning our column has data.","title":"col_isnull - colnum = 3"},{"location":"09-data-constraint-null/04-reading-data/#col_isnull-colnum-8","text":"This time, we'll pick a colnum representing a column in the first bit of the second byte in the null bitmap. This example will really show the purpose of these bitwise operations. colnum: 0000 1000 colnum >> 3 0000 1000 >> 3 = 0000 0001 = 1 Meaning we want the 1st (0-based) uint8_t block in the nullBitmap array. nullBitmap[1] = 0010 1110 On the right side we evaluate the following: colnum & 0x07 = 8 & 0x07 0000 1000 <-- 8 0000 0111 <-- 0x07 --------- & 0000 0000 = 0 Left-shift decimal 1 by 0 bits yeilds decimal 1. So our final operation: 0010 1110 <-- 1st (0-based) byte of the null bitmap 0000 0000 <-- result of the right side --------- & 0000 0000 = 0 We return the opposite of false , which is true . Our column is Null . Hopefully the above made it clear how these operations keep our nullBitmap inspection coordinated. I.e. the purpose of the bit-shift in nullBitmap[colnum >> 3] is to access the next block of 8-bits for every 8 values of colnum . The purpose of colnum & 0x07 is to effectively normalize colnum to a value between 0 and 7. Then the 1 << (...) piece maps that normalized 0-7 value to the correct bit.","title":"col_isnull - colnum = 8"}]}